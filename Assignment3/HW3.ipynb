{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# ECE 720 Assignment Hands-on Project 3\n",
    "\n",
    "In this assignment, we will explore how to quantitatively analyze an RNN model.\n",
    "\n",
    "Before working on the assignment, it is highly recommended reading the paper: [DeepStellar: model-based quantitative analysis of stateful deep learning systems](https://dl.acm.org/doi/10.1145/3338906.3338954) . Our materials will be highly relevant to this paper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Content:\n",
    "\n",
    "1. Implement state abstraction---a key component in DeepStellar. (7pt)\n",
    "\n",
    "   1.1 Build state abstraction and transition model based on training data. (4pt)\n",
    "   \n",
    "   1.2 Based on your implementation, which state is most frequently visited? (3pt)\n",
    "   \n",
    "2. Implement a function to obtain the corresponding trace* (state transition) in the abstracted model of a given text. (4pt)\n",
    "\n",
    "3. Implement the metrics for measuring **state-based trace similarity** and **transition-based trace similarity**. (4pt, 2pt for each metric)\n",
    "\n",
    "4. Use DeepStellar to analyze adversarial attack (4pt)\n",
    "\n",
    "    4.1 Output traces of original data and attacked data. (1pt)\n",
    "    \n",
    "    4.2 Draw a figure to visualize each trace. (1pt)\n",
    "    \n",
    "    4.3 Calculate their **state-based trace similarity** and **transition-based trace similarity** based on the defined functions in 3. (1pt)\n",
    "    \n",
    "    4.4 Analyze the difference between original data and attacked data: give a brief explanation on why the model's prediction result is incorrect on the attacked data. (1pt)\n",
    "    \n",
    "5. Brief discussion on the open question: how to further improve the state abstraction method? (1pt)\n",
    "\n",
    "**trace: a sequence of RNN state vectors*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "In this assignment, let's consider the following task: **multi-class text classification**.\n",
    "\n",
    "The data used is AGNews dataset [\\[1\\]](http://groups.di.unipi.it/~gulli/AG_corpus_of_news_articles.html),[\\[2\\]](https://pytorch.org/text/stable/datasets.html#ag-news). In this dataset, each news corpus is labeled as one of four classes:\n",
    "\n",
    "- 0-World\n",
    "- 1-Sports\n",
    "- 2-Business\n",
    "- 3-Sci/Tech"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, let's consider using this RNN model, specifically, a simple one-layer GRU model, the hidden-state size is 256.\n",
    "\n",
    "![RNN model](https://raw.githubusercontent.com/momentum-openspace/momentum-openspace.github.io/new-template/files/asg3/rnn-many-to-one-ltr.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "In this RNN model, we used a word-based tokenizer to tokenize a sentence and the vocab was built with top-30000 frequent words in the training data. The embedding layer has a size of 200. The final hidden states will go into two linear layers to generate the classification result. Since this is a multi-class classification problem, we utilize the `nn.CrossEntropyLoss` for training.\n",
    "\n",
    "We have done the training for you, therefore you will have a pre-trained RNN model for this assigment. The training accuracy is around 91.5%.\n",
    "\n",
    "The detailed model architecture is listed as below:\n",
    "\n",
    "```\n",
    "SimpleGRUMultiClassification(\n",
    "  (embedding): Embedding(30002, 200)\n",
    "  (dropout): Dropout(p=0.3, inplace=False)\n",
    "  (lstm): GRU(200, 256, num_layers=2, batch_first=True, dropout=0.3)\n",
    "  (softmax): Softmax(dim=-1)\n",
    "  (dense): Sequential(\n",
    "    (0): Linear(in_features=256, out_features=64, bias=True)\n",
    "    (1): ReLU()\n",
    "    (2): Linear(in_features=64, out_features=4, bias=True)\n",
    "  )\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now let's try to load the pretrained model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "! wget --no-check-certificate \"https://onedrive.live.com/download?cid=9F719B6A4F197D9C&resid=9F719B6A4F197D9C%214304&authkey=ABp_zbZfV0lDbBI\" -O asg3.zip\n",
    "! unzip asg3.zip\n",
    "\n",
    "# install some libraries if you're working with your own PC.\n",
    "# pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter-amirhossein/projects/MLSE/Assignment3/venv/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import imp\n",
    "import platform\n",
    "\n",
    "py_version = platform.python_version().split('.')[1]\n",
    "if py_version == '7':\n",
    "    util = imp.load_compiled(\"util\", \"./util.cpython-37.pyc\")\n",
    "elif py_version == '8':\n",
    "    util = imp.load_compiled(\"util\", \"./util.cpython-38.pyc\")\n",
    "elif py_version == '9':\n",
    "    util = imp.load_compiled(\"util\", \"./util.cpython-39.pyc\")\n",
    "else:\n",
    "    raise NotImplementedError('Only supports Python>=3.7')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import joblib\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from util import load_tokenizer, SimpleGRUMultiClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pre-trained model and tokenizer\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "ckpt = torch.load('./file/agnews_ckpt_best.pth', map_location=device)\n",
    "model = SimpleGRUMultiClassification(*ckpt['model_args'])\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "model.load_state_dict(ckpt['model'])\n",
    "\n",
    "_, tokenizer, _, _ = load_tokenizer('./file/tokenizer.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Here we provide a simple example on how to use this pretrained model to predict a new corpus. We use the `model.proifle` function to get both the hidden states and the prediction results on each hidden state. The result from final hidden state is used as the model's prediction result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The predicted label is: Sci/Tech\n"
     ]
    }
   ],
   "source": [
    "example_text = \"Yahoo introducing new search engine tools\\\",\\\"Internet giant Yahoo is introducing new search engine tools that will enable users to create personal folders and share their favorite links with others.\"\n",
    "\n",
    "label2text = {0: \"World\", 1: \"Sports\", 2: \"Business\", 3: \"Sci/Tech\"}\n",
    "\n",
    "# Convert the text corpus into tokens, and transform the token list into a torch tensor.\n",
    "tokens = torch.tensor(tokenizer(example_text)).to(device).long()\n",
    "\n",
    "# add a dimension to the tensor, to let it become a tensor with a size of (1, n), where 1 represent the batch size, and n is the number of tokens.\n",
    "input_tensor = tokens.view(1, -1)\n",
    "\n",
    "# use ``model.profile`` to predict on the input tensor.\n",
    "hidden_states, pred_tensor = model.profile(input_tensor)\n",
    "\n",
    "# recall that the output is a 4-dim array processed by softmax function, therefore we utilize the ``argmax`` function to get the prediction result.\n",
    "pred = np.argmax(pred_tensor[0].cpu().detach().numpy()[-1])\n",
    "print('The predicted label is: %s' % label2text[pred])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now, we can profile on the training data, to get all the hidden states and prediction results. Since this process is time-consuming, we've done it for you. Besides, in DeepStellar, the high-dimension hidden states also need to be processed by PCA to get low-dimension ones. We've also done the PCA process for you. And you can simply load the processed data and the pca model, which will be used in the remaining parts of this assignment. In this assignment, we take top-3 principal components in PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pre-processed data and pca model\n",
    "\n",
    "(pca_data, embedding, text, _, label, pred, pred_pro) = joblib.load('./file/pca_data.prt')\n",
    "pca_model = joblib.load('./file/pca_model.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Data Structure\n",
    "- `pca_data` is a list. Each element in this list is a numpy array of size `(n,3)`, representing the pca-processed hidden states of this training text corpus.\n",
    "- `embedding` is a list. Each element in this list is a numpy array of size `(n,)`, representing the embeddings of this training text corpus.\n",
    "- `text` is a list. Each element in this list is a numpy array of size `(n,)`, representing the tokenized words of this training text corpus.\n",
    "- `label` is a list. Each element in this list is an integer, representing the ground-truth label of this training text corpus.\n",
    "- `pred` is a list. Each element in this list is an integer, representing the prediction result of this training text corpus.\n",
    "- `pred_pro` is a list. Each element in this list is a numpy array of size `(n,4)`, representing all the intermediate prediction probabilities of this training text corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "`pca_model` is a Python object of PCA, which is trained on the training data, and we provide the API reference about how to use this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### API Reference (all relevant APIs included)\n",
    "\n",
    "#### pca_model.do_reduction(data_list)\n",
    "\n",
    "**Parameter:**\n",
    "- **data_list** (list) - A list of numpy array where each array stores a sequence of hidden states\n",
    "\n",
    "**Returns:**\n",
    "- **pca_data** (list) - A list of numpy array where each array stores a sequence of concrete states processed by pca\n",
    "\n",
    "#### Examples\n",
    "```\n",
    ">>> data_list = [np.random.random((200, 256))]\n",
    ">>> pca_data = pca_model.do_reduction(data_list)\n",
    ">>> pca_data[0].shape\n",
    "'(200, 3)'\n",
    "```\n",
    "---\n",
    "\n",
    "#### tokenizer(text)\n",
    "\n",
    "**Parameter:**\n",
    "- **text** (str) - A string of text corpus.\n",
    "\n",
    "**Returns:**\n",
    "- **tokens** (list) - A list of word tokens.\n",
    "\n",
    "#### Examples\n",
    "```\n",
    ">>> tokens = tokenizer(\"hello world\")\n",
    ">>> tokens\n",
    "'[12225, 47]'\n",
    "```\n",
    "---\n",
    "\n",
    "#### model.profile(input_tensor)\n",
    "\n",
    "**Parameter:**\n",
    "- **input_tensor** (torch.Tensor) - The input tensor should be a torch Long tensor of shape `(B,N)`, `B`\n",
    "is the batch size, and `N` is the number of tokens.\n",
    "\n",
    "**Returns:**\n",
    "- **hidden_states** (torch.Tensor) - The hidden states of shape `(B,N,H)`, `B` is the batch size, and `N` is\n",
    "the number of tokens, `H` is the dimension of hidden states.\n",
    "- **pred_tensor** (torch.Tensor) - The prediction result of shape `(B,N,C)`, `B` is the batch size, and `N` is\n",
    "the number of tokens, `C` is the number of class (classification target).\n",
    "\n",
    "#### Examples\n",
    "```\n",
    ">>> tokens = tokenizer(\"hello world\")\n",
    ">>> tokens\n",
    "'[12225, 47]'\n",
    ">>> hidden_states, pred_tensor = model.profile(torch.tensor(tokens).long().view(1, -1))\n",
    ">>> hidden_states.size()\n",
    "'torch.Size([1, 2, 256])'\n",
    ">>> pred_tensor.size()\n",
    "'torch.Size([1, 2, 4])'\n",
    "```\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "You can spend some time on trying these functions and APIs. Once you feel good about using them, let's turn to the assignment quesions ;-)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 1. Implement DeepStellar. (7pt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### The following `class DeepStellar` is the basic class of DeepStellar model, you will need to finish the todo blocks in order to:\n",
    "\n",
    "1. Build state transition model based on training data. (4pt)\n",
    "\n",
    "2. Based on your implementation, which state is most frequently visited? (3pt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class DeepStellar(object):\n",
    "    r\"\"\"The class of DeepStellar model.\n",
    "    Arguments\n",
    "    ---------\n",
    "    rnn : torch.nn.Module\n",
    "        A PyTorch based RNN model that would be analyzed in DeepStellar.\n",
    "    tokenizer : tokenizer_pipeline object\n",
    "        The tokenizer used when training the RNN model. This is used to split\n",
    "        text corpus and convert them into word-based tokens.\n",
    "    pca_model : PCAReduction object\n",
    "        The PCA model. This is to transform a high-dimension data into low-dimension\n",
    "        one. Note that in this assignment, we keep top-3 dimension in PCA.\n",
    "    pca_data : list\n",
    "        This is a list of numpy array, which should be the pca-processed hidden states.\n",
    "        The shape of each numpy array should be ``(n, 3)``, where n is the length of\n",
    "        each sequence.\n",
    "    m_split : int\n",
    "        m_split decides how many intervals to be split on each dimension.\n",
    "    Example\n",
    "    -------\n",
    "    >>> deep_stellar_model = DeepStellar(rnn, tokenizer, pca_model, pca_data, 10)\n",
    "    >>> state_distribution = deep_stellar_model.get_state_distribution()\n",
    "    \"\"\"\n",
    "    def __init__(self, rnn, tokenizer, pca_model, pca_data, m_split=10):\n",
    "        self.rnn = rnn\n",
    "        self.tokenizer = tokenizer\n",
    "        self.pca = pca_model\n",
    "        self.m_split = m_split\n",
    "        self.pca_data = pca_data\n",
    "        self.traces = self._build_abstraction_model()\n",
    "        \n",
    "    def _build_abstraction_model(self):\n",
    "        \"\"\"Iterate all the training data (``self.pca_data``) to build DeepStellar\n",
    "        Returns\n",
    "        -------\n",
    "        traces : list\n",
    "            A list of numpy array which should be the abstracted states. Each numpy\n",
    "            array should look like\n",
    "            ``[[1, 2, 3], [4, 5, 6], ...]``,\n",
    "            and each element represents which interval it belongs to.\n",
    "            For example, suppose the lower bound of first dimension is -10, the upper\n",
    "            bound is 10, we split the grid into 10 intervals, then each interval\n",
    "            has a size of 2. Hence, 4.67 should belong to the grid 7 because\n",
    "            ``(4.67 - (-10)) / 2 = 7.335`` (We use 0 to denote the first interval\n",
    "            therefore 10th interval will be 9)\n",
    "        \"\"\"\n",
    "\n",
    "        '''\n",
    "        Hint: you might want to first iterative through all arrays in\n",
    "        ``self.pca_data`` to find the min value and max value of each PCA dimension.\n",
    "        Then, you can calculate the size of each interval, and transform the concrete\n",
    "        states in ``self.pca_data`` into abstracted states.\n",
    "        \n",
    "        And you might want to add some attributes to this class for future accessing \n",
    "        your state abstraction model.\n",
    "\n",
    "        Your final output should be a list of state trace as addressed in the above\n",
    "        comments.\n",
    "        '''\n",
    "\n",
    "        # ======= Below is assignment TODO ======= #\n",
    "        import math\n",
    "\n",
    "        NUM_DIMS = 3\n",
    "        self.min_values = [np.inf] * NUM_DIMS\n",
    "        self.max_values = [-np.inf] * NUM_DIMS\n",
    "        self.interval_length = [0] * NUM_DIMS\n",
    "        traces = []\n",
    "\n",
    "        for text in pca_data:\n",
    "            for hidden_state in text:\n",
    "                for idx, hs in enumerate(hidden_state):\n",
    "                    if hs > self.max_values[idx]:\n",
    "                        self.max_values[idx] = hs\n",
    "                    elif hs < self.min_values[idx]:\n",
    "                        self.min_values[idx] = hs\n",
    "\n",
    "\n",
    "        for i in range(NUM_DIMS):\n",
    "            self.interval_length[i] = (self.max_values[i] - self.min_values[i])/self.m_split\n",
    "\n",
    "        for text in pca_data:\n",
    "            t = []\n",
    "            for hidden_state in text:\n",
    "                h = []\n",
    "                for idx, hs in enumerate(hidden_state):\n",
    "                    s = math.floor((hs-self.min_values[idx])/self.interval_length[idx])\n",
    "                    if s == self.m_split:  s = self.m_split-1\n",
    "                    h.append(s)\n",
    "                t.append(h)\n",
    "            traces.append(t)\n",
    "        # ======= Above is assignment TODO ======= #\n",
    "\n",
    "        return traces\n",
    "\n",
    "    def get_state_distribution(self):\n",
    "        \"\"\"Iterate all the traces to calculate the state distribution.\n",
    "        Returns\n",
    "        -------\n",
    "        states : numpy.array()\n",
    "            A numpy array which has a size of ``(m_split, m_split, m_split)``, and the\n",
    "            value of represents the visit time of this abstract state.\n",
    "        \"\"\"\n",
    "        states = np.zeros((self.m_split, self.m_split, self.m_split))\n",
    "        for trace in self.traces:\n",
    "            for state in trace:\n",
    "                states[state[0], state[1], state[2]] += 1\n",
    "        return states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now let's build the DeepStellar model. Note that `m_split` will be set as **10** in this assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "deep_stellar = DeepStellar(model, tokenizer, pca_model, pca_data, 10)\n",
    "states = deep_stellar.get_state_distribution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Then, you can try to find the most frequently visited state by iterate through `states`. Please write your code\n",
    "in the following cell and paste your results in the project report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most frequently visited state is [5, 4, 5] which is visited 380580 times\n"
     ]
    }
   ],
   "source": [
    "# ======= Below is assignment TODO ======= #\n",
    "most_visited = None\n",
    "max_visits = 0\n",
    "m_split = len(states)\n",
    "\n",
    "for state_0 in range(m_split):\n",
    "    for state_1 in range(m_split):\n",
    "        for state_2 in range(m_split):\n",
    "            if states[state_0, state_1, state_2] > max_visits:\n",
    "                max_visits = states[state_0, state_1, state_2]\n",
    "                most_visited = [state_0, state_1, state_2]\n",
    "\n",
    "print(f'Most frequently visited state is {most_visited} which is visited {int(max_visits)} times')\n",
    "# ======= Above is assignment TODO ======= #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 2. Implement a function to obtain trace of a given text. (4pt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Since we have build the DeepStellar model, now we'd like to think about how to extract trace on a new text corpus.\n",
    "\n",
    "You will need to finish the following function, and please following the comments on the returns types. :-)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_trace(deep_stellar, text):\n",
    "    \"\"\"Get the state trace of a new text corpus.\n",
    "    Arguments\n",
    "    ---------\n",
    "    deep_stellar : DeepStellar object\n",
    "    text : str\n",
    "    Returns\n",
    "    -------\n",
    "    trace : numpy.array()\n",
    "        A numpy array to store the state trace, and this numpy array should look like\n",
    "        ``[[1, 2, 3], [4, 5, 6], ...]``,\n",
    "        and each element represents which interval it belongs to.\n",
    "        If the value is smaller than the lower bound, then we denote it as -1, if it's\n",
    "        larger than the higher bound, then we denote it as 10 (recall that we denote 10\n",
    "        intervals from 0 to 9).\n",
    "    pred : int\n",
    "        The prediction result of this text.\n",
    "    \"\"\"\n",
    "\n",
    "    # ======= Below is assignment TODO ======= #\n",
    "    import math\n",
    "\n",
    "    tokens = torch.tensor(tokenizer(text)).to(device).long()\n",
    "    input_tensor = tokens.view(1, -1)\n",
    "    hidden_states, pred_tensor = model.profile(input_tensor)\n",
    "    pca_data = pca_model.do_reduction([torch.squeeze(hidden_states).cpu().detach().numpy()])[0]\n",
    "    trace = []\n",
    "\n",
    "    for hidden_state in pca_data:\n",
    "        h = []\n",
    "        for idx, hs in enumerate(hidden_state):\n",
    "            s = math.floor((hs-deep_stellar.min_values[idx])/deep_stellar.interval_length[idx])\n",
    "            if s == deep_stellar.m_split:  s = deep_stellar.m_split-1\n",
    "            elif s > deep_stellar.m_split:  s = deep_stellar.m_split\n",
    "            elif s < 0:  s = -1\n",
    "            h.append(s)\n",
    "        trace.append(h)\n",
    "\n",
    "    pred = np.argmax(pred_tensor[0].cpu().detach().numpy()[-1])\n",
    "    # ======= Above is assignment TODO ======= #\n",
    "\n",
    "    return trace, pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now let's test this function using a minimal example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "trace, pred = get_trace(deep_stellar, \"hello world\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Implement **state-based trace similarity** and **transition-based trace similarity**. (4pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def state_based_trace_similarity(tr1, tr2):\n",
    "    \"\"\"Calculate the state-based trace similarity\n",
    "    Arguments\n",
    "    ---------\n",
    "    tr1 : numpy.array()\n",
    "        A numpy array to store the state trace, and this numpy array should look like\n",
    "        ``[[1, 2, 3], [4, 5, 6], ...]``\n",
    "    tr2 : numpy.array()\n",
    "        A numpy array to store the state trace, and this numpy array should look like\n",
    "        ``[[1, 2, 3], [4, 5, 6], ...]``\n",
    "    Returns\n",
    "    -------\n",
    "    similarity : float\n",
    "    \"\"\"\n",
    "    # ======= Below is assignment TODO ======= #\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    # ======= Above is assignment TODO ======= #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def transition_based_trace_similarity(tr1, tr2):\n",
    "    \"\"\"Calculate the transition-based trace similarity\n",
    "    Arguments\n",
    "    ---------\n",
    "    tr1 : numpy.array()\n",
    "        A numpy array to store the state trace, and this numpy array should look like\n",
    "        ``[[1, 2, 3], [4, 5, 6], ...]``\n",
    "    tr2 : numpy.array()\n",
    "        A numpy array to store the state trace, and this numpy array should look like\n",
    "        ``[[1, 2, 3], [4, 5, 6], ...]``\n",
    "    Returns\n",
    "    -------\n",
    "    similarity : float\n",
    "    \"\"\"\n",
    "    # ======= Below is assignment TODO ======= #\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    # ======= Above is assignment TODO ======= #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 4. Use DeepStellar to analyze adversarial attack (4pt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now consider the following two paragraphs of text.\n",
    "\n",
    "> House defeats FDA proposal, House lawmakers yesterday killed a Senate proposal to give the Food and Drug Administration authority over the manufacture and marketing of tobacco.\n",
    "\n",
    "> House kills FDA proposal, House lawmakers yesterday killed a Senate proposal to give the Food and Drug Administration authority over the manufacture and marketing of tobacco."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Let's first see their prediction results. Try to use `get_trace(deep_stellar, text)` to print their traces and prediction results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "text1 = \"House defeats FDA proposal, House lawmakers yesterday killed a Senate proposal to give the Food and Drug Administration authority over the manufacture and marketing of tobacco.\"\n",
    "text2 = \"House kills FDA proposal, House lawmakers yesterday killed a Senate proposal to give the Food and Drug Administration authority over the manufacture and marketing of tobacco.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# ======= Below is assignment TODO ======= #\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ======= Above is assignment TODO ======= #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now, try to use some graphic lib to plot these two traces. You might want to take a look on this (https://matplotlib.org/stable/gallery/mplot3d/scatter3d.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ======= Below is assignment TODO ======= #\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ======= Above is assignment TODO ======= #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "What is the state-based trace similarity and transition-based trace similarity of them?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print('State-based trace similarity: %.2f' % state_based_trace_similarity(tr1, tr2))\n",
    "print('Transition-based trace similarity: %.2f' % transition_based_trace_similarity(tr1, tr2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "## Remark\n",
    "\n",
    "At this point (after finishing these three assignments), you should be ready with basis of machine learning testing techniques!;-) Now you can go further to explore and contribute to this research area!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
