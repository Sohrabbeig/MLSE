{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "eval.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "%env PYTHONPATH=\n",
        "!echo $PYTHONPATH"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ie26w8D29ewo",
        "outputId": "a521d428-f934-45d2-e297-16370f8c3612"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "env: PYTHONPATH=\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "MINICONDA_INSTALLER_SCRIPT=Miniconda3-4.5.4-Linux-x86_64.sh\n",
        "MINICONDA_PREFIX=/usr/local\n",
        "wget https://repo.continuum.io/miniconda/$MINICONDA_INSTALLER_SCRIPT\n",
        "chmod +x $MINICONDA_INSTALLER_SCRIPT\n",
        "./$MINICONDA_INSTALLER_SCRIPT -b -f -p $MINICONDA_PREFIX"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6jhI4Q209_3x",
        "outputId": "2543d1b2-42da-487a-f604-3701300abc87"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PREFIX=/usr/local\n",
            "installing: python-3.6.5-hc3d631a_2 ...\n",
            "installing: ca-certificates-2018.03.07-0 ...\n",
            "installing: conda-env-2.6.0-h36134e3_1 ...\n",
            "installing: libgcc-ng-7.2.0-hdf63c60_3 ...\n",
            "installing: libstdcxx-ng-7.2.0-hdf63c60_3 ...\n",
            "installing: libffi-3.2.1-hd88cf55_4 ...\n",
            "installing: ncurses-6.1-hf484d3e_0 ...\n",
            "installing: openssl-1.0.2o-h20670df_0 ...\n",
            "installing: tk-8.6.7-hc745277_3 ...\n",
            "installing: xz-5.2.4-h14c3975_4 ...\n",
            "installing: yaml-0.1.7-had09818_2 ...\n",
            "installing: zlib-1.2.11-ha838bed_2 ...\n",
            "installing: libedit-3.1.20170329-h6b74fdf_2 ...\n",
            "installing: readline-7.0-ha6073c6_4 ...\n",
            "installing: sqlite-3.23.1-he433501_0 ...\n",
            "installing: asn1crypto-0.24.0-py36_0 ...\n",
            "installing: certifi-2018.4.16-py36_0 ...\n",
            "installing: chardet-3.0.4-py36h0f667ec_1 ...\n",
            "installing: idna-2.6-py36h82fb2a8_1 ...\n",
            "installing: pycosat-0.6.3-py36h0a5515d_0 ...\n",
            "installing: pycparser-2.18-py36hf9f622e_1 ...\n",
            "installing: pysocks-1.6.8-py36_0 ...\n",
            "installing: ruamel_yaml-0.15.37-py36h14c3975_2 ...\n",
            "installing: six-1.11.0-py36h372c433_1 ...\n",
            "installing: cffi-1.11.5-py36h9745a5d_0 ...\n",
            "installing: setuptools-39.2.0-py36_0 ...\n",
            "installing: cryptography-2.2.2-py36h14c3975_0 ...\n",
            "installing: wheel-0.31.1-py36_0 ...\n",
            "installing: pip-10.0.1-py36_0 ...\n",
            "installing: pyopenssl-18.0.0-py36_0 ...\n",
            "installing: urllib3-1.22-py36hbe7ace6_0 ...\n",
            "installing: requests-2.18.4-py36he2e5f8d_1 ...\n",
            "installing: conda-4.5.4-py36_0 ...\n",
            "installation finished.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "--2022-04-16 22:22:24--  https://repo.continuum.io/miniconda/Miniconda3-4.5.4-Linux-x86_64.sh\n",
            "Resolving repo.continuum.io (repo.continuum.io)... 104.18.200.79, 104.18.201.79, 2606:4700::6812:c94f, ...\n",
            "Connecting to repo.continuum.io (repo.continuum.io)|104.18.200.79|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://repo.anaconda.com/miniconda/Miniconda3-4.5.4-Linux-x86_64.sh [following]\n",
            "--2022-04-16 22:22:24--  https://repo.anaconda.com/miniconda/Miniconda3-4.5.4-Linux-x86_64.sh\n",
            "Resolving repo.anaconda.com (repo.anaconda.com)... 104.16.131.3, 104.16.130.3, 2606:4700::6810:8203, ...\n",
            "Connecting to repo.anaconda.com (repo.anaconda.com)|104.16.131.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 58468498 (56M) [application/x-sh]\n",
            "Saving to: ‘Miniconda3-4.5.4-Linux-x86_64.sh’\n",
            "\n",
            "     0K .......... .......... .......... .......... ..........  0% 82.1M 1s\n",
            "    50K .......... .......... .......... .......... ..........  0% 6.75M 4s\n",
            "   100K .......... .......... .......... .......... ..........  0% 7.95M 5s\n",
            "   150K .......... .......... .......... .......... ..........  0% 44.9M 4s\n",
            "   200K .......... .......... .......... .......... ..........  0%  196M 3s\n",
            "   250K .......... .......... .......... .......... ..........  0%  232M 3s\n",
            "   300K .......... .......... .......... .......... ..........  0% 8.90M 3s\n",
            "   350K .......... .......... .......... .......... ..........  0%  200M 3s\n",
            "   400K .......... .......... .......... .......... ..........  0%  135M 3s\n",
            "   450K .......... .......... .......... .......... ..........  0% 12.0M 3s\n",
            "   500K .......... .......... .......... .......... ..........  0%  158M 3s\n",
            "   550K .......... .......... .......... .......... ..........  1%  138M 2s\n",
            "   600K .......... .......... .......... .......... ..........  1%  136M 2s\n",
            "   650K .......... .......... .......... .......... ..........  1%  158M 2s\n",
            "   700K .......... .......... .......... .......... ..........  1%  125M 2s\n",
            "   750K .......... .......... .......... .......... ..........  1%  162M 2s\n",
            "   800K .......... .......... .......... .......... ..........  1%  140M 2s\n",
            "   850K .......... .......... .......... .......... ..........  1%  195M 2s\n",
            "   900K .......... .......... .......... .......... ..........  1%  132M 2s\n",
            "   950K .......... .......... .......... .......... ..........  1%  140M 2s\n",
            "  1000K .......... .......... .......... .......... ..........  1%  104M 2s\n",
            "  1050K .......... .......... .......... .......... ..........  1%  147M 2s\n",
            "  1100K .......... .......... .......... .......... ..........  2%  254M 1s\n",
            "  1150K .......... .......... .......... .......... ..........  2% 18.7M 2s\n",
            "  1200K .......... .......... .......... .......... ..........  2%  138M 1s\n",
            "  1250K .......... .......... .......... .......... ..........  2%  230M 1s\n",
            "  1300K .......... .......... .......... .......... ..........  2%  140M 1s\n",
            "  1350K .......... .......... .......... .......... ..........  2%  168M 1s\n",
            "  1400K .......... .......... .......... .......... ..........  2%  142M 1s\n",
            "  1450K .......... .......... .......... .......... ..........  2%  123M 1s\n",
            "  1500K .......... .......... .......... .......... ..........  2%  244M 1s\n",
            "  1550K .......... .......... .......... .......... ..........  2% 25.3M 1s\n",
            "  1600K .......... .......... .......... .......... ..........  2%  258M 1s\n",
            "  1650K .......... .......... .......... .......... ..........  2%  286M 1s\n",
            "  1700K .......... .......... .......... .......... ..........  3%  213M 1s\n",
            "  1750K .......... .......... .......... .......... ..........  3%  239M 1s\n",
            "  1800K .......... .......... .......... .......... ..........  3%  260M 1s\n",
            "  1850K .......... .......... .......... .......... ..........  3%  259M 1s\n",
            "  1900K .......... .......... .......... .......... ..........  3%  264M 1s\n",
            "  1950K .......... .......... .......... .......... ..........  3%  189M 1s\n",
            "  2000K .......... .......... .......... .......... ..........  3%  269M 1s\n",
            "  2050K .......... .......... .......... .......... ..........  3%  288M 1s\n",
            "  2100K .......... .......... .......... .......... ..........  3%  239M 1s\n",
            "  2150K .......... .......... .......... .......... ..........  3%  238M 1s\n",
            "  2200K .......... .......... .......... .......... ..........  3%  230M 1s\n",
            "  2250K .......... .......... .......... .......... ..........  4%  293M 1s\n",
            "  2300K .......... .......... .......... .......... ..........  4%  272M 1s\n",
            "  2350K .......... .......... .......... .......... ..........  4%  196M 1s\n",
            "  2400K .......... .......... .......... .......... ..........  4%  240M 1s\n",
            "  2450K .......... .......... .......... .......... ..........  4%  237M 1s\n",
            "  2500K .......... .......... .......... .......... ..........  4%  254M 1s\n",
            "  2550K .......... .......... .......... .......... ..........  4%  226M 1s\n",
            "  2600K .......... .......... .......... .......... ..........  4% 82.5M 1s\n",
            "  2650K .......... .......... .......... .......... ..........  4% 51.5M 1s\n",
            "  2700K .......... .......... .......... .......... ..........  4% 53.9M 1s\n",
            "  2750K .......... .......... .......... .......... ..........  4% 45.6M 1s\n",
            "  2800K .......... .......... .......... .......... ..........  4% 49.9M 1s\n",
            "  2850K .......... .......... .......... .......... ..........  5% 51.4M 1s\n",
            "  2900K .......... .......... .......... .......... ..........  5% 50.1M 1s\n",
            "  2950K .......... .......... .......... .......... ..........  5% 42.7M 1s\n",
            "  3000K .......... .......... .......... .......... ..........  5% 56.0M 1s\n",
            "  3050K .......... .......... .......... .......... ..........  5% 52.1M 1s\n",
            "  3100K .......... .......... .......... .......... ..........  5% 51.5M 1s\n",
            "  3150K .......... .......... .......... .......... ..........  5% 41.3M 1s\n",
            "  3200K .......... .......... .......... .......... ..........  5% 72.5M 1s\n",
            "  3250K .......... .......... .......... .......... ..........  5%  247M 1s\n",
            "  3300K .......... .......... .......... .......... ..........  5%  232M 1s\n",
            "  3350K .......... .......... .......... .......... ..........  5%  224M 1s\n",
            "  3400K .......... .......... .......... .......... ..........  6%  263M 1s\n",
            "  3450K .......... .......... .......... .......... ..........  6%  250M 1s\n",
            "  3500K .......... .......... .......... .......... ..........  6%  255M 1s\n",
            "  3550K .......... .......... .......... .......... ..........  6%  195M 1s\n",
            "  3600K .......... .......... .......... .......... ..........  6%  135M 1s\n",
            "  3650K .......... .......... .......... .......... ..........  6%  173M 1s\n",
            "  3700K .......... .......... .......... .......... ..........  6%  190M 1s\n",
            "  3750K .......... .......... .......... .......... ..........  6%  177M 1s\n",
            "  3800K .......... .......... .......... .......... ..........  6%  227M 1s\n",
            "  3850K .......... .......... .......... .......... ..........  6%  110M 1s\n",
            "  3900K .......... .......... .......... .......... ..........  6%  201M 1s\n",
            "  3950K .......... .......... .......... .......... ..........  7%  203M 1s\n",
            "  4000K .......... .......... .......... .......... ..........  7%  234M 1s\n",
            "  4050K .......... .......... .......... .......... ..........  7%  198M 1s\n",
            "  4100K .......... .......... .......... .......... ..........  7%  185M 1s\n",
            "  4150K .......... .......... .......... .......... ..........  7%  219M 1s\n",
            "  4200K .......... .......... .......... .......... ..........  7%  216M 1s\n",
            "  4250K .......... .......... .......... .......... ..........  7%  238M 1s\n",
            "  4300K .......... .......... .......... .......... ..........  7%  236M 1s\n",
            "  4350K .......... .......... .......... .......... ..........  7%  190M 1s\n",
            "  4400K .......... .......... .......... .......... ..........  7%  262M 1s\n",
            "  4450K .......... .......... .......... .......... ..........  7%  252M 1s\n",
            "  4500K .......... .......... .......... .......... ..........  7%  260M 1s\n",
            "  4550K .......... .......... .......... .......... ..........  8%  204M 1s\n",
            "  4600K .......... .......... .......... .......... ..........  8%  195M 1s\n",
            "  4650K .......... .......... .......... .......... ..........  8%  191M 1s\n",
            "  4700K .......... .......... .......... .......... ..........  8%  243M 1s\n",
            "  4750K .......... .......... .......... .......... ..........  8%  193M 1s\n",
            "  4800K .......... .......... .......... .......... ..........  8%  206M 1s\n",
            "  4850K .......... .......... .......... .......... ..........  8%  213M 1s\n",
            "  4900K .......... .......... .......... .......... ..........  8%  234M 1s\n",
            "  4950K .......... .......... .......... .......... ..........  8%  231M 1s\n",
            "  5000K .......... .......... .......... .......... ..........  8%  302M 1s\n",
            "  5050K .......... .......... .......... .......... ..........  8%  444M 1s\n",
            "  5100K .......... .......... .......... .......... ..........  9%  433M 1s\n",
            "  5150K .......... .......... .......... .......... ..........  9%  373M 1s\n",
            "  5200K .......... .......... .......... .......... ..........  9%  436M 1s\n",
            "  5250K .......... .......... .......... .......... ..........  9%  422M 1s\n",
            "  5300K .......... .......... .......... .......... ..........  9%  437M 1s\n",
            "  5350K .......... .......... .......... .......... ..........  9%  382M 1s\n",
            "  5400K .......... .......... .......... .......... ..........  9%  223M 1s\n",
            "  5450K .......... .......... .......... .......... ..........  9%  242M 1s\n",
            "  5500K .......... .......... .......... .......... ..........  9%  255M 1s\n",
            "  5550K .......... .......... .......... .......... ..........  9%  208M 1s\n",
            "  5600K .......... .......... .......... .......... ..........  9%  237M 1s\n",
            "  5650K .......... .......... .......... .......... ..........  9%  255M 1s\n",
            "  5700K .......... .......... .......... .......... .......... 10%  260M 1s\n",
            "  5750K .......... .......... .......... .......... .......... 10%  232M 1s\n",
            "  5800K .......... .......... .......... .......... .......... 10%  259M 1s\n",
            "  5850K .......... .......... .......... .......... .......... 10%  259M 1s\n",
            "  5900K .......... .......... .......... .......... .......... 10%  235M 1s\n",
            "  5950K .......... .......... .......... .......... .......... 10%  191M 1s\n",
            "  6000K .......... .......... .......... .......... .......... 10%  203M 1s\n",
            "  6050K .......... .......... .......... .......... .......... 10%  220M 1s\n",
            "  6100K .......... .......... .......... .......... .......... 10%  228M 1s\n",
            "  6150K .......... .......... .......... .......... .......... 10%  159M 1s\n",
            "  6200K .......... .......... .......... .......... .......... 10%  232M 1s\n",
            "  6250K .......... .......... .......... .......... .......... 11%  257M 1s\n",
            "  6300K .......... .......... .......... .......... .......... 11%  234M 1s\n",
            "  6350K .......... .......... .......... .......... .......... 11%  204M 1s\n",
            "  6400K .......... .......... .......... .......... .......... 11%  260M 1s\n",
            "  6450K .......... .......... .......... .......... .......... 11%  247M 1s\n",
            "  6500K .......... .......... .......... .......... .......... 11%  267M 1s\n",
            "  6550K .......... .......... .......... .......... .......... 11%  228M 1s\n",
            "  6600K .......... .......... .......... .......... .......... 11%  243M 1s\n",
            "  6650K .......... .......... .......... .......... .......... 11%  237M 1s\n",
            "  6700K .......... .......... .......... .......... .......... 11%  251M 1s\n",
            "  6750K .......... .......... .......... .......... .......... 11%  184M 1s\n",
            "  6800K .......... .......... .......... .......... .......... 11%  223M 1s\n",
            "  6850K .......... .......... .......... .......... .......... 12%  309M 0s\n",
            "  6900K .......... .......... .......... .......... .......... 12%  308M 0s\n",
            "  6950K .......... .......... .......... .......... .......... 12%  368M 0s\n",
            "  7000K .......... .......... .......... .......... .......... 12%  421M 0s\n",
            "  7050K .......... .......... .......... .......... .......... 12%  441M 0s\n",
            "  7100K .......... .......... .......... .......... .......... 12%  453M 0s\n",
            "  7150K .......... .......... .......... .......... .......... 12%  212M 0s\n",
            "  7200K .......... .......... .......... .......... .......... 12%  242M 0s\n",
            "  7250K .......... .......... .......... .......... .......... 12%  254M 0s\n",
            "  7300K .......... .......... .......... .......... .......... 12%  246M 0s\n",
            "  7350K .......... .......... .......... .......... .......... 12%  222M 0s\n",
            "  7400K .......... .......... .......... .......... .......... 13%  238M 0s\n",
            "  7450K .......... .......... .......... .......... .......... 13%  258M 0s\n",
            "  7500K .......... .......... .......... .......... .......... 13%  260M 0s\n",
            "  7550K .......... .......... .......... .......... .......... 13%  204M 0s\n",
            "  7600K .......... .......... .......... .......... .......... 13%  261M 0s\n",
            "  7650K .......... .......... .......... .......... .......... 13%  239M 0s\n",
            "  7700K .......... .......... .......... .......... .......... 13%  119M 0s\n",
            "  7750K .......... .......... .......... .......... .......... 13%  215M 0s\n",
            "  7800K .......... .......... .......... .......... .......... 13%  245M 0s\n",
            "  7850K .......... .......... .......... .......... .......... 13%  256M 0s\n",
            "  7900K .......... .......... .......... .......... .......... 13%  255M 0s\n",
            "  7950K .......... .......... .......... .......... .......... 14%  209M 0s\n",
            "  8000K .......... .......... .......... .......... .......... 14%  252M 0s\n",
            "  8050K .......... .......... .......... .......... .......... 14%  251M 0s\n",
            "  8100K .......... .......... .......... .......... .......... 14%  250M 0s\n",
            "  8150K .......... .......... .......... .......... .......... 14%  217M 0s\n",
            "  8200K .......... .......... .......... .......... .......... 14%  248M 0s\n",
            "  8250K .......... .......... .......... .......... .......... 14%  253M 0s\n",
            "  8300K .......... .......... .......... .......... .......... 14%  232M 0s\n",
            "  8350K .......... .......... .......... .......... .......... 14%  205M 0s\n",
            "  8400K .......... .......... .......... .......... .......... 14%  255M 0s\n",
            "  8450K .......... .......... .......... .......... .......... 14%  235M 0s\n",
            "  8500K .......... .......... .......... .......... .......... 14%  257M 0s\n",
            "  8550K .......... .......... .......... .......... .......... 15%  219M 0s\n",
            "  8600K .......... .......... .......... .......... .......... 15%  246M 0s\n",
            "  8650K .......... .......... .......... .......... .......... 15% 10.1M 0s\n",
            "  8700K .......... .......... .......... .......... .......... 15%  207M 0s\n",
            "  8750K .......... .......... .......... .......... .......... 15%  159M 0s\n",
            "  8800K .......... .......... .......... .......... .......... 15%  219M 0s\n",
            "  8850K .......... .......... .......... .......... .......... 15%  201M 0s\n",
            "  8900K .......... .......... .......... .......... .......... 15%  237M 0s\n",
            "  8950K .......... .......... .......... .......... .......... 15%  199M 0s\n",
            "  9000K .......... .......... .......... .......... .......... 15%  203M 0s\n",
            "  9050K .......... .......... .......... .......... .......... 15%  239M 0s\n",
            "  9100K .......... .......... .......... .......... .......... 16%  249M 0s\n",
            "  9150K .......... .......... .......... .......... .......... 16%  178M 0s\n",
            "  9200K .......... .......... .......... .......... .......... 16%  196M 0s\n",
            "  9250K .......... .......... .......... .......... .......... 16%  198M 0s\n",
            "  9300K .......... .......... .......... .......... .......... 16%  210M 0s\n",
            "  9350K .......... .......... .......... .......... .......... 16%  190M 0s\n",
            "  9400K .......... .......... .......... .......... .......... 16%  203M 0s\n",
            "  9450K .......... .......... .......... .......... .......... 16%  200M 0s\n",
            "  9500K .......... .......... .......... .......... .......... 16%  198M 0s\n",
            "  9550K .......... .......... .......... .......... .......... 16%  210M 0s\n",
            "  9600K .......... .......... .......... .......... .......... 16%  251M 0s\n",
            "  9650K .......... .......... .......... .......... .......... 16%  222M 0s\n",
            "  9700K .......... .......... .......... .......... .......... 17%  231M 0s\n",
            "  9750K .......... .......... .......... .......... .......... 17%  195M 0s\n",
            "  9800K .......... .......... .......... .......... .......... 17%  262M 0s\n",
            "  9850K .......... .......... .......... .......... .......... 17%  247M 0s\n",
            "  9900K .......... .......... .......... .......... .......... 17%  241M 0s\n",
            "  9950K .......... .......... .......... .......... .......... 17%  212M 0s\n",
            " 10000K .......... .......... .......... .......... .......... 17%  255M 0s\n",
            " 10050K .......... .......... .......... .......... .......... 17%  258M 0s\n",
            " 10100K .......... .......... .......... .......... .......... 17%  240M 0s\n",
            " 10150K .......... .......... .......... .......... .......... 17%  217M 0s\n",
            " 10200K .......... .......... .......... .......... .......... 17%  209M 0s\n",
            " 10250K .......... .......... .......... .......... .......... 18%  205M 0s\n",
            " 10300K .......... .......... .......... .......... .......... 18%  217M 0s\n",
            " 10350K .......... .......... .......... .......... .......... 18%  192M 0s\n",
            " 10400K .......... .......... .......... .......... .......... 18%  231M 0s\n",
            " 10450K .......... .......... .......... .......... .......... 18%  437M 0s\n",
            " 10500K .......... .......... .......... .......... .......... 18%  435M 0s\n",
            " 10550K .......... .......... .......... .......... .......... 18%  404M 0s\n",
            " 10600K .......... .......... .......... .......... .......... 18%  459M 0s\n",
            " 10650K .......... .......... .......... .......... .......... 18%  317M 0s\n",
            " 10700K .......... .......... .......... .......... .......... 18%  241M 0s\n",
            " 10750K .......... .......... .......... .......... .......... 18%  147M 0s\n",
            " 10800K .......... .......... .......... .......... .......... 19%  234M 0s\n",
            " 10850K .......... .......... .......... .......... .......... 19%  237M 0s\n",
            " 10900K .......... .......... .......... .......... .......... 19%  255M 0s\n",
            " 10950K .......... .......... .......... .......... .......... 19%  223M 0s\n",
            " 11000K .......... .......... .......... .......... .......... 19%  211M 0s\n",
            " 11050K .......... .......... .......... .......... .......... 19%  236M 0s\n",
            " 11100K .......... .......... .......... .......... .......... 19%  249M 0s\n",
            " 11150K .......... .......... .......... .......... .......... 19%  204M 0s\n",
            " 11200K .......... .......... .......... .......... .......... 19%  245M 0s\n",
            " 11250K .......... .......... .......... .......... .......... 19%  251M 0s\n",
            " 11300K .......... .......... .......... .......... .......... 19%  245M 0s\n",
            " 11350K .......... .......... .......... .......... .......... 19%  225M 0s\n",
            " 11400K .......... .......... .......... .......... .......... 20%  250M 0s\n",
            " 11450K .......... .......... .......... .......... .......... 20%  234M 0s\n",
            " 11500K .......... .......... .......... .......... .......... 20%  252M 0s\n",
            " 11550K .......... .......... .......... .......... .......... 20% 23.3M 0s\n",
            " 11600K .......... .......... .......... .......... .......... 20%  196M 0s\n",
            " 11650K .......... .......... .......... .......... .......... 20%  184M 0s\n",
            " 11700K .......... .......... .......... .......... .......... 20%  194M 0s\n",
            " 11750K .......... .......... .......... .......... .......... 20%  178M 0s\n",
            " 11800K .......... .......... .......... .......... .......... 20%  222M 0s\n",
            " 11850K .......... .......... .......... .......... .......... 20%  257M 0s\n",
            " 11900K .......... .......... .......... .......... .......... 20%  229M 0s\n",
            " 11950K .......... .......... .......... .......... .......... 21%  182M 0s\n",
            " 12000K .......... .......... .......... .......... .......... 21%  206M 0s\n",
            " 12050K .......... .......... .......... .......... .......... 21%  216M 0s\n",
            " 12100K .......... .......... .......... .......... .......... 21%  197M 0s\n",
            " 12150K .......... .......... .......... .......... .......... 21%  192M 0s\n",
            " 12200K .......... .......... .......... .......... .......... 21%  214M 0s\n",
            " 12250K .......... .......... .......... .......... .......... 21%  196M 0s\n",
            " 12300K .......... .......... .......... .......... .......... 21%  224M 0s\n",
            " 12350K .......... .......... .......... .......... .......... 21%  181M 0s\n",
            " 12400K .......... .......... .......... .......... .......... 21%  224M 0s\n",
            " 12450K .......... .......... .......... .......... .......... 21%  203M 0s\n",
            " 12500K .......... .......... .......... .......... .......... 21%  208M 0s\n",
            " 12550K .......... .......... .......... .......... .......... 22%  189M 0s\n",
            " 12600K .......... .......... .......... .......... .......... 22%  216M 0s\n",
            " 12650K .......... .......... .......... .......... .......... 22%  196M 0s\n",
            " 12700K .......... .......... .......... .......... .......... 22%  255M 0s\n",
            " 12750K .......... .......... .......... .......... .......... 22%  208M 0s\n",
            " 12800K .......... .......... .......... .......... .......... 22%  258M 0s\n",
            " 12850K .......... .......... .......... .......... .......... 22%  258M 0s\n",
            " 12900K .......... .......... .......... .......... .......... 22%  235M 0s\n",
            " 12950K .......... .......... .......... .......... .......... 22%  193M 0s\n",
            " 13000K .......... .......... .......... .......... .......... 22%  209M 0s\n",
            " 13050K .......... .......... .......... .......... .......... 22%  241M 0s\n",
            " 13100K .......... .......... .......... .......... .......... 23%  244M 0s\n",
            " 13150K .......... .......... .......... .......... .......... 23%  210M 0s\n",
            " 13200K .......... .......... .......... .......... .......... 23%  212M 0s\n",
            " 13250K .......... .......... .......... .......... .......... 23%  201M 0s\n",
            " 13300K .......... .......... .......... .......... .......... 23%  207M 0s\n",
            " 13350K .......... .......... .......... .......... .......... 23%  166M 0s\n",
            " 13400K .......... .......... .......... .......... .......... 23%  204M 0s\n",
            " 13450K .......... .......... .......... .......... .......... 23%  204M 0s\n",
            " 13500K .......... .......... .......... .......... .......... 23%  203M 0s\n",
            " 13550K .......... .......... .......... .......... .......... 23%  149M 0s\n",
            " 13600K .......... .......... .......... .......... .......... 23%  171M 0s\n",
            " 13650K .......... .......... .......... .......... .......... 23%  160M 0s\n",
            " 13700K .......... .......... .......... .......... .......... 24%  158M 0s\n",
            " 13750K .......... .......... .......... .......... .......... 24%  174M 0s\n",
            " 13800K .......... .......... .......... .......... .......... 24%  234M 0s\n",
            " 13850K .......... .......... .......... .......... .......... 24%  243M 0s\n",
            " 13900K .......... .......... .......... .......... .......... 24%  199M 0s\n",
            " 13950K .......... .......... .......... .......... .......... 24%  197M 0s\n",
            " 14000K .......... .......... .......... .......... .......... 24%  254M 0s\n",
            " 14050K .......... .......... .......... .......... .......... 24%  201M 0s\n",
            " 14100K .......... .......... .......... .......... .......... 24%  204M 0s\n",
            " 14150K .......... .......... .......... .......... .......... 24%  207M 0s\n",
            " 14200K .......... .......... .......... .......... .......... 24%  240M 0s\n",
            " 14250K .......... .......... .......... .......... .......... 25%  250M 0s\n",
            " 14300K .......... .......... .......... .......... .......... 25%  271M 0s\n",
            " 14350K .......... .......... .......... .......... .......... 25%  202M 0s\n",
            " 14400K .......... .......... .......... .......... .......... 25%  247M 0s\n",
            " 14450K .......... .......... .......... .......... .......... 25%  238M 0s\n",
            " 14500K .......... .......... .......... .......... .......... 25%  267M 0s\n",
            " 14550K .......... .......... .......... .......... .......... 25%  234M 0s\n",
            " 14600K .......... .......... .......... .......... .......... 25%  225M 0s\n",
            " 14650K .......... .......... .......... .......... .......... 25%  218M 0s\n",
            " 14700K .......... .......... .......... .......... .......... 25%  190M 0s\n",
            " 14750K .......... .......... .......... .......... .......... 25%  168M 0s\n",
            " 14800K .......... .......... .......... .......... .......... 26%  201M 0s\n",
            " 14850K .......... .......... .......... .......... .......... 26%  198M 0s\n",
            " 14900K .......... .......... .......... .......... .......... 26%  191M 0s\n",
            " 14950K .......... .......... .......... .......... .......... 26%  205M 0s\n",
            " 15000K .......... .......... .......... .......... .......... 26%  225M 0s\n",
            " 15050K .......... .......... .......... .......... .......... 26%  230M 0s\n",
            " 15100K .......... .......... .......... .......... .......... 26%  238M 0s\n",
            " 15150K .......... .......... .......... .......... .......... 26%  183M 0s\n",
            " 15200K .......... .......... .......... .......... .......... 26%  194M 0s\n",
            " 15250K .......... .......... .......... .......... .......... 26%  227M 0s\n",
            " 15300K .......... .......... .......... .......... .......... 26%  202M 0s\n",
            " 15350K .......... .......... .......... .......... .......... 26%  214M 0s\n",
            " 15400K .......... .......... .......... .......... .......... 27%  240M 0s\n",
            " 15450K .......... .......... .......... .......... .......... 27%  224M 0s\n",
            " 15500K .......... .......... .......... .......... .......... 27%  254M 0s\n",
            " 15550K .......... .......... .......... .......... .......... 27%  199M 0s\n",
            " 15600K .......... .......... .......... .......... .......... 27%  261M 0s\n",
            " 15650K .......... .......... .......... .......... .......... 27%  256M 0s\n",
            " 15700K .......... .......... .......... .......... .......... 27%  240M 0s\n",
            " 15750K .......... .......... .......... .......... .......... 27%  204M 0s\n",
            " 15800K .......... .......... .......... .......... .......... 27%  190M 0s\n",
            " 15850K .......... .......... .......... .......... .......... 27%  185M 0s\n",
            " 15900K .......... .......... .......... .......... .......... 27%  215M 0s\n",
            " 15950K .......... .......... .......... .......... .......... 28%  199M 0s\n",
            " 16000K .......... .......... .......... .......... .......... 28%  263M 0s\n",
            " 16050K .......... .......... .......... .......... .......... 28%  253M 0s\n",
            " 16100K .......... .......... .......... .......... .......... 28%  253M 0s\n",
            " 16150K .......... .......... .......... .......... .......... 28%  188M 0s\n",
            " 16200K .......... .......... .......... .......... .......... 28%  190M 0s\n",
            " 16250K .......... .......... .......... .......... .......... 28%  229M 0s\n",
            " 16300K .......... .......... .......... .......... .......... 28%  255M 0s\n",
            " 16350K .......... .......... .......... .......... .......... 28%  197M 0s\n",
            " 16400K .......... .......... .......... .......... .......... 28%  202M 0s\n",
            " 16450K .......... .......... .......... .......... .......... 28%  220M 0s\n",
            " 16500K .......... .......... .......... .......... .......... 28%  236M 0s\n",
            " 16550K .......... .......... .......... .......... .......... 29%  183M 0s\n",
            " 16600K .......... .......... .......... .......... .......... 29%  255M 0s\n",
            " 16650K .......... .......... .......... .......... .......... 29%  246M 0s\n",
            " 16700K .......... .......... .......... .......... .......... 29%  229M 0s\n",
            " 16750K .......... .......... .......... .......... .......... 29%  179M 0s\n",
            " 16800K .......... .......... .......... .......... .......... 29%  232M 0s\n",
            " 16850K .......... .......... .......... .......... .......... 29%  239M 0s\n",
            " 16900K .......... .......... .......... .......... .......... 29%  254M 0s\n",
            " 16950K .......... .......... .......... .......... .......... 29%  231M 0s\n",
            " 17000K .......... .......... .......... .......... .......... 29%  253M 0s\n",
            " 17050K .......... .......... .......... .......... .......... 29%  241M 0s\n",
            " 17100K .......... .......... .......... .......... .......... 30%  255M 0s\n",
            " 17150K .......... .......... .......... .......... .......... 30%  205M 0s\n",
            " 17200K .......... .......... .......... .......... .......... 30%  221M 0s\n",
            " 17250K .......... .......... .......... .......... .......... 30%  187M 0s\n",
            " 17300K .......... .......... .......... .......... .......... 30%  254M 0s\n",
            " 17350K .......... .......... .......... .......... .......... 30%  221M 0s\n",
            " 17400K .......... .......... .......... .......... .......... 30%  260M 0s\n",
            " 17450K .......... .......... .......... .......... .......... 30%  253M 0s\n",
            " 17500K .......... .......... .......... .......... .......... 30%  249M 0s\n",
            " 17550K .......... .......... .......... .......... .......... 30%  207M 0s\n",
            " 17600K .......... .......... .......... .......... .......... 30%  245M 0s\n",
            " 17650K .......... .......... .......... .......... .......... 30%  257M 0s\n",
            " 17700K .......... .......... .......... .......... .......... 31%  245M 0s\n",
            " 17750K .......... .......... .......... .......... .......... 31%  217M 0s\n",
            " 17800K .......... .......... .......... .......... .......... 31%  215M 0s\n",
            " 17850K .......... .......... .......... .......... .......... 31%  191M 0s\n",
            " 17900K .......... .......... .......... .......... .......... 31%  199M 0s\n",
            " 17950K .......... .......... .......... .......... .......... 31%  196M 0s\n",
            " 18000K .......... .......... .......... .......... .......... 31%  240M 0s\n",
            " 18050K .......... .......... .......... .......... .......... 31%  220M 0s\n",
            " 18100K .......... .......... .......... .......... .......... 31%  203M 0s\n",
            " 18150K .......... .......... .......... .......... .......... 31%  197M 0s\n",
            " 18200K .......... .......... .......... .......... .......... 31%  231M 0s\n",
            " 18250K .......... .......... .......... .......... .......... 32%  238M 0s\n",
            " 18300K .......... .......... .......... .......... .......... 32%  254M 0s\n",
            " 18350K .......... .......... .......... .......... .......... 32%  189M 0s\n",
            " 18400K .......... .......... .......... .......... .......... 32%  199M 0s\n",
            " 18450K .......... .......... .......... .......... .......... 32%  196M 0s\n",
            " 18500K .......... .......... .......... .......... .......... 32%  197M 0s\n",
            " 18550K .......... .......... .......... .......... .......... 32%  189M 0s\n",
            " 18600K .......... .......... .......... .......... .......... 32%  201M 0s\n",
            " 18650K .......... .......... .......... .......... .......... 32%  237M 0s\n",
            " 18700K .......... .......... .......... .......... .......... 32%  210M 0s\n",
            " 18750K .......... .......... .......... .......... .......... 32%  179M 0s\n",
            " 18800K .......... .......... .......... .......... .......... 33%  204M 0s\n",
            " 18850K .......... .......... .......... .......... .......... 33%  217M 0s\n",
            " 18900K .......... .......... .......... .......... .......... 33%  195M 0s\n",
            " 18950K .......... .......... .......... .......... .......... 33%  182M 0s\n",
            " 19000K .......... .......... .......... .......... .......... 33%  249M 0s\n",
            " 19050K .......... .......... .......... .......... .......... 33%  252M 0s\n",
            " 19100K .......... .......... .......... .......... .......... 33%  246M 0s\n",
            " 19150K .......... .......... .......... .......... .......... 33%  213M 0s\n",
            " 19200K .......... .......... .......... .......... .......... 33%  245M 0s\n",
            " 19250K .......... .......... .......... .......... .......... 33%  247M 0s\n",
            " 19300K .......... .......... .......... .......... .......... 33%  235M 0s\n",
            " 19350K .......... .......... .......... .......... .......... 33%  180M 0s\n",
            " 19400K .......... .......... .......... .......... .......... 34%  208M 0s\n",
            " 19450K .......... .......... .......... .......... .......... 34%  254M 0s\n",
            " 19500K .......... .......... .......... .......... .......... 34%  245M 0s\n",
            " 19550K .......... .......... .......... .......... .......... 34%  208M 0s\n",
            " 19600K .......... .......... .......... .......... .......... 34%  322M 0s\n",
            " 19650K .......... .......... .......... .......... .......... 34%  444M 0s\n",
            " 19700K .......... .......... .......... .......... .......... 34%  440M 0s\n",
            " 19750K .......... .......... .......... .......... .......... 34%  364M 0s\n",
            " 19800K .......... .......... .......... .......... .......... 34%  386M 0s\n",
            " 19850K .......... .......... .......... .......... .......... 34%  427M 0s\n",
            " 19900K .......... .......... .......... .......... .......... 34%  439M 0s\n",
            " 19950K .......... .......... .......... .......... .......... 35%  273M 0s\n",
            " 20000K .......... .......... .......... .......... .......... 35%  234M 0s\n",
            " 20050K .......... .......... .......... .......... .......... 35%  236M 0s\n",
            " 20100K .......... .......... .......... .......... .......... 35%  190M 0s\n",
            " 20150K .......... .......... .......... .......... .......... 35%  196M 0s\n",
            " 20200K .......... .......... .......... .......... .......... 35%  192M 0s\n",
            " 20250K .......... .......... .......... .......... .......... 35%  234M 0s\n",
            " 20300K .......... .......... .......... .......... .......... 35%  233M 0s\n",
            " 20350K .......... .......... .......... .......... .......... 35%  180M 0s\n",
            " 20400K .......... .......... .......... .......... .......... 35%  229M 0s\n",
            " 20450K .......... .......... .......... .......... .......... 35%  235M 0s\n",
            " 20500K .......... .......... .......... .......... .......... 35%  228M 0s\n",
            " 20550K .......... .......... .......... .......... .......... 36%  206M 0s\n",
            " 20600K .......... .......... .......... .......... .......... 36%  265M 0s\n",
            " 20650K .......... .......... .......... .......... .......... 36%  228M 0s\n",
            " 20700K .......... .......... .......... .......... .......... 36%  226M 0s\n",
            " 20750K .......... .......... .......... .......... .......... 36%  203M 0s\n",
            " 20800K .......... .......... .......... .......... .......... 36%  251M 0s\n",
            " 20850K .......... .......... .......... .......... .......... 36%  241M 0s\n",
            " 20900K .......... .......... .......... .......... .......... 36%  260M 0s\n",
            " 20950K .......... .......... .......... .......... .......... 36%  225M 0s\n",
            " 21000K .......... .......... .......... .......... .......... 36%  239M 0s\n",
            " 21050K .......... .......... .......... .......... .......... 36%  256M 0s\n",
            " 21100K .......... .......... .......... .......... .......... 37%  209M 0s\n",
            " 21150K .......... .......... .......... .......... .......... 37%  177M 0s\n",
            " 21200K .......... .......... .......... .......... .......... 37%  250M 0s\n",
            " 21250K .......... .......... .......... .......... .......... 37%  245M 0s\n",
            " 21300K .......... .......... .......... .......... .......... 37%  252M 0s\n",
            " 21350K .......... .......... .......... .......... .......... 37%  215M 0s\n",
            " 21400K .......... .......... .......... .......... .......... 37%  254M 0s\n",
            " 21450K .......... .......... .......... .......... .......... 37%  245M 0s\n",
            " 21500K .......... .......... .......... .......... .......... 37%  243M 0s\n",
            " 21550K .......... .......... .......... .......... .......... 37%  213M 0s\n",
            " 21600K .......... .......... .......... .......... .......... 37%  236M 0s\n",
            " 21650K .......... .......... .......... .......... .......... 38%  252M 0s\n",
            " 21700K .......... .......... .......... .......... .......... 38%  258M 0s\n",
            " 21750K .......... .......... .......... .......... .......... 38%  210M 0s\n",
            " 21800K .......... .......... .......... .......... .......... 38%  243M 0s\n",
            " 21850K .......... .......... .......... .......... .......... 38%  251M 0s\n",
            " 21900K .......... .......... .......... .......... .......... 38%  253M 0s\n",
            " 21950K .......... .......... .......... .......... .......... 38%  178M 0s\n",
            " 22000K .......... .......... .......... .......... .......... 38%  276M 0s\n",
            " 22050K .......... .......... .......... .......... .......... 38%  214M 0s\n",
            " 22100K .......... .......... .......... .......... .......... 38%  288M 0s\n",
            " 22150K .......... .......... .......... .......... .......... 38%  254M 0s\n",
            " 22200K .......... .......... .......... .......... .......... 38%  279M 0s\n",
            " 22250K .......... .......... .......... .......... .......... 39%  234M 0s\n",
            " 22300K .......... .......... .......... .......... .......... 39%  302M 0s\n",
            " 22350K .......... .......... .......... .......... .......... 39%  232M 0s\n",
            " 22400K .......... .......... .......... .......... .......... 39%  308M 0s\n",
            " 22450K .......... .......... .......... .......... .......... 39%  264M 0s\n",
            " 22500K .......... .......... .......... .......... .......... 39%  265M 0s\n",
            " 22550K .......... .......... .......... .......... .......... 39%  264M 0s\n",
            " 22600K .......... .......... .......... .......... .......... 39%  233M 0s\n",
            " 22650K .......... .......... .......... .......... .......... 39%  276M 0s\n",
            " 22700K .......... .......... .......... .......... .......... 39%  132M 0s\n",
            " 22750K .......... .......... .......... .......... .......... 39%  173M 0s\n",
            " 22800K .......... .......... .......... .......... .......... 40%  171M 0s\n",
            " 22850K .......... .......... .......... .......... .......... 40%  165M 0s\n",
            " 22900K .......... .......... .......... .......... .......... 40% 60.2M 0s\n",
            " 22950K .......... .......... .......... .......... .......... 40%  177M 0s\n",
            " 23000K .......... .......... .......... .......... .......... 40%  218M 0s\n",
            " 23050K .......... .......... .......... .......... .......... 40%  216M 0s\n",
            " 23100K .......... .......... .......... .......... .......... 40%  205M 0s\n",
            " 23150K .......... .......... .......... .......... .......... 40%  179M 0s\n",
            " 23200K .......... .......... .......... .......... .......... 40%  237M 0s\n",
            " 23250K .......... .......... .......... .......... .......... 40%  233M 0s\n",
            " 23300K .......... .......... .......... .......... .......... 40%  217M 0s\n",
            " 23350K .......... .......... .......... .......... .......... 40%  207M 0s\n",
            " 23400K .......... .......... .......... .......... .......... 41%  285M 0s\n",
            " 23450K .......... .......... .......... .......... .......... 41%  276M 0s\n",
            " 23500K .......... .......... .......... .......... .......... 41%  307M 0s\n",
            " 23550K .......... .......... .......... .......... .......... 41%  238M 0s\n",
            " 23600K .......... .......... .......... .......... .......... 41%  306M 0s\n",
            " 23650K .......... .......... .......... .......... .......... 41%  306M 0s\n",
            " 23700K .......... .......... .......... .......... .......... 41%  265M 0s\n",
            " 23750K .......... .......... .......... .......... .......... 41%  258M 0s\n",
            " 23800K .......... .......... .......... .......... .......... 41%  322M 0s\n",
            " 23850K .......... .......... .......... .......... .......... 41%  266M 0s\n",
            " 23900K .......... .......... .......... .......... .......... 41%  260M 0s\n",
            " 23950K .......... .......... .......... .......... .......... 42%  217M 0s\n",
            " 24000K .......... .......... .......... .......... .......... 42%  244M 0s\n",
            " 24050K .......... .......... .......... .......... .......... 42%  263M 0s\n",
            " 24100K .......... .......... .......... .......... .......... 42%  248M 0s\n",
            " 24150K .......... .......... .......... .......... .......... 42%  238M 0s\n",
            " 24200K .......... .......... .......... .......... .......... 42%  261M 0s\n",
            " 24250K .......... .......... .......... .......... .......... 42%  222M 0s\n",
            " 24300K .......... .......... .......... .......... .......... 42%  197M 0s\n",
            " 24350K .......... .......... .......... .......... .......... 42%  219M 0s\n",
            " 24400K .......... .......... .......... .......... .......... 42%  250M 0s\n",
            " 24450K .......... .......... .......... .......... .......... 42%  257M 0s\n",
            " 24500K .......... .......... .......... .......... .......... 42%  254M 0s\n",
            " 24550K .......... .......... .......... .......... .......... 43%  176M 0s\n",
            " 24600K .......... .......... .......... .......... .......... 43%  214M 0s\n",
            " 24650K .......... .......... .......... .......... .......... 43%  264M 0s\n",
            " 24700K .......... .......... .......... .......... .......... 43%  251M 0s\n",
            " 24750K .......... .......... .......... .......... .......... 43%  217M 0s\n",
            " 24800K .......... .......... .......... .......... .......... 43%  247M 0s\n",
            " 24850K .......... .......... .......... .......... .......... 43%  254M 0s\n",
            " 24900K .......... .......... .......... .......... .......... 43%  266M 0s\n",
            " 24950K .......... .......... .......... .......... .......... 43%  211M 0s\n",
            " 25000K .......... .......... .......... .......... .......... 43%  198M 0s\n",
            " 25050K .......... .......... .......... .......... .......... 43%  193M 0s\n",
            " 25100K .......... .......... .......... .......... .......... 44%  184M 0s\n",
            " 25150K .......... .......... .......... .......... .......... 44%  158M 0s\n",
            " 25200K .......... .......... .......... .......... .......... 44%  189M 0s\n",
            " 25250K .......... .......... .......... .......... .......... 44%  160M 0s\n",
            " 25300K .......... .......... .......... .......... .......... 44%  200M 0s\n",
            " 25350K .......... .......... .......... .......... .......... 44%  203M 0s\n",
            " 25400K .......... .......... .......... .......... .......... 44%  190M 0s\n",
            " 25450K .......... .......... .......... .......... .......... 44%  234M 0s\n",
            " 25500K .......... .......... .......... .......... .......... 44%  252M 0s\n",
            " 25550K .......... .......... .......... .......... .......... 44%  211M 0s\n",
            " 25600K .......... .......... .......... .......... .......... 44%  265M 0s\n",
            " 25650K .......... .......... .......... .......... .......... 45%  244M 0s\n",
            " 25700K .......... .......... .......... .......... .......... 45%  258M 0s\n",
            " 25750K .......... .......... .......... .......... .......... 45%  227M 0s\n",
            " 25800K .......... .......... .......... .......... .......... 45%  242M 0s\n",
            " 25850K .......... .......... .......... .......... .......... 45%  257M 0s\n",
            " 25900K .......... .......... .......... .......... .......... 45%  195M 0s\n",
            " 25950K .......... .......... .......... .......... .......... 45%  193M 0s\n",
            " 26000K .......... .......... .......... .......... .......... 45%  247M 0s\n",
            " 26050K .......... .......... .......... .......... .......... 45%  330M 0s\n",
            " 26100K .......... .......... .......... .......... .......... 45%  435M 0s\n",
            " 26150K .......... .......... .......... .......... .......... 45%  242M 0s\n",
            " 26200K .......... .......... .......... .......... .......... 45%  256M 0s\n",
            " 26250K .......... .......... .......... .......... .......... 46%  255M 0s\n",
            " 26300K .......... .......... .......... .......... .......... 46%  243M 0s\n",
            " 26350K .......... .......... .......... .......... .......... 46%  191M 0s\n",
            " 26400K .......... .......... .......... .......... .......... 46%  199M 0s\n",
            " 26450K .......... .......... .......... .......... .......... 46%  211M 0s\n",
            " 26500K .......... .......... .......... .......... .......... 46%  206M 0s\n",
            " 26550K .......... .......... .......... .......... .......... 46%  192M 0s\n",
            " 26600K .......... .......... .......... .......... .......... 46%  205M 0s\n",
            " 26650K .......... .......... .......... .......... .......... 46%  215M 0s\n",
            " 26700K .......... .......... .......... .......... .......... 46%  207M 0s\n",
            " 26750K .......... .......... .......... .......... .......... 46%  165M 0s\n",
            " 26800K .......... .......... .......... .......... .......... 47%  215M 0s\n",
            " 26850K .......... .......... .......... .......... .......... 47%  206M 0s\n",
            " 26900K .......... .......... .......... .......... .......... 47%  194M 0s\n",
            " 26950K .......... .......... .......... .......... .......... 47%  192M 0s\n",
            " 27000K .......... .......... .......... .......... .......... 47%  212M 0s\n",
            " 27050K .......... .......... .......... .......... .......... 47%  216M 0s\n",
            " 27100K .......... .......... .......... .......... .......... 47%  241M 0s\n",
            " 27150K .......... .......... .......... .......... .......... 47%  208M 0s\n",
            " 27200K .......... .......... .......... .......... .......... 47%  248M 0s\n",
            " 27250K .......... .......... .......... .......... .......... 47%  236M 0s\n",
            " 27300K .......... .......... .......... .......... .......... 47%  249M 0s\n",
            " 27350K .......... .......... .......... .......... .......... 47%  222M 0s\n",
            " 27400K .......... .......... .......... .......... .......... 48%  251M 0s\n",
            " 27450K .......... .......... .......... .......... .......... 48%  259M 0s\n",
            " 27500K .......... .......... .......... .......... .......... 48%  217M 0s\n",
            " 27550K .......... .......... .......... .......... .......... 48%  165M 0s\n",
            " 27600K .......... .......... .......... .......... .......... 48%  229M 0s\n",
            " 27650K .......... .......... .......... .......... .......... 48%  255M 0s\n",
            " 27700K .......... .......... .......... .......... .......... 48%  240M 0s\n",
            " 27750K .......... .......... .......... .......... .......... 48%  228M 0s\n",
            " 27800K .......... .......... .......... .......... .......... 48%  250M 0s\n",
            " 27850K .......... .......... .......... .......... .......... 48%  243M 0s\n",
            " 27900K .......... .......... .......... .......... .......... 48%  255M 0s\n",
            " 27950K .......... .......... .......... .......... .......... 49%  171M 0s\n",
            " 28000K .......... .......... .......... .......... .......... 49%  251M 0s\n",
            " 28050K .......... .......... .......... .......... .......... 49%  241M 0s\n",
            " 28100K .......... .......... .......... .......... .......... 49%  199M 0s\n",
            " 28150K .......... .......... .......... .......... .......... 49%  184M 0s\n",
            " 28200K .......... .......... .......... .......... .......... 49%  250M 0s\n",
            " 28250K .......... .......... .......... .......... .......... 49%  261M 0s\n",
            " 28300K .......... .......... .......... .......... .......... 49%  252M 0s\n",
            " 28350K .......... .......... .......... .......... .......... 49%  219M 0s\n",
            " 28400K .......... .......... .......... .......... .......... 49%  243M 0s\n",
            " 28450K .......... .......... .......... .......... .......... 49%  246M 0s\n",
            " 28500K .......... .......... .......... .......... .......... 50%  253M 0s\n",
            " 28550K .......... .......... .......... .......... .......... 50%  215M 0s\n",
            " 28600K .......... .......... .......... .......... .......... 50%  254M 0s\n",
            " 28650K .......... .......... .......... .......... .......... 50%  247M 0s\n",
            " 28700K .......... .......... .......... .......... .......... 50%  267M 0s\n",
            " 28750K .......... .......... .......... .......... .......... 50%  181M 0s\n",
            " 28800K .......... .......... .......... .......... .......... 50%  219M 0s\n",
            " 28850K .......... .......... .......... .......... .......... 50%  255M 0s\n",
            " 28900K .......... .......... .......... .......... .......... 50%  244M 0s\n",
            " 28950K .......... .......... .......... .......... .......... 50%  216M 0s\n",
            " 29000K .......... .......... .......... .......... .......... 50%  262M 0s\n",
            " 29050K .......... .......... .......... .......... .......... 50%  242M 0s\n",
            " 29100K .......... .......... .......... .......... .......... 51%  256M 0s\n",
            " 29150K .......... .......... .......... .......... .......... 51%  209M 0s\n",
            " 29200K .......... .......... .......... .......... .......... 51%  267M 0s\n",
            " 29250K .......... .......... .......... .......... .......... 51%  266M 0s\n",
            " 29300K .......... .......... .......... .......... .......... 51%  211M 0s\n",
            " 29350K .......... .......... .......... .......... .......... 51%  216M 0s\n",
            " 29400K .......... .......... .......... .......... .......... 51%  240M 0s\n",
            " 29450K .......... .......... .......... .......... .......... 51%  245M 0s\n",
            " 29500K .......... .......... .......... .......... .......... 51%  241M 0s\n",
            " 29550K .......... .......... .......... .......... .......... 51%  191M 0s\n",
            " 29600K .......... .......... .......... .......... .......... 51%  241M 0s\n",
            " 29650K .......... .......... .......... .......... .......... 52%  225M 0s\n",
            " 29700K .......... .......... .......... .......... .......... 52%  239M 0s\n",
            " 29750K .......... .......... .......... .......... .......... 52%  191M 0s\n",
            " 29800K .......... .......... .......... .......... .......... 52%  205M 0s\n",
            " 29850K .......... .......... .......... .......... .......... 52%  237M 0s\n",
            " 29900K .......... .......... .......... .......... .......... 52%  228M 0s\n",
            " 29950K .......... .......... .......... .......... .......... 52%  210M 0s\n",
            " 30000K .......... .......... .......... .......... .......... 52%  259M 0s\n",
            " 30050K .......... .......... .......... .......... .......... 52%  194M 0s\n",
            " 30100K .......... .......... .......... .......... .......... 52%  214M 0s\n",
            " 30150K .......... .......... .......... .......... .......... 52%  191M 0s\n",
            " 30200K .......... .......... .......... .......... .......... 52%  195M 0s\n",
            " 30250K .......... .......... .......... .......... .......... 53%  208M 0s\n",
            " 30300K .......... .......... .......... .......... .......... 53%  199M 0s\n",
            " 30350K .......... .......... .......... .......... .......... 53%  156M 0s\n",
            " 30400K .......... .......... .......... .......... .......... 53%  204M 0s\n",
            " 30450K .......... .......... .......... .......... .......... 53%  206M 0s\n",
            " 30500K .......... .......... .......... .......... .......... 53%  210M 0s\n",
            " 30550K .......... .......... .......... .......... .......... 53%  188M 0s\n",
            " 30600K .......... .......... .......... .......... .......... 53%  206M 0s\n",
            " 30650K .......... .......... .......... .......... .......... 53%  147M 0s\n",
            " 30700K .......... .......... .......... .......... .......... 53%  255M 0s\n",
            " 30750K .......... .......... .......... .......... .......... 53%  204M 0s\n",
            " 30800K .......... .......... .......... .......... .......... 54%  220M 0s\n",
            " 30850K .......... .......... .......... .......... .......... 54%  334M 0s\n",
            " 30900K .......... .......... .......... .......... .......... 54%  243M 0s\n",
            " 30950K .......... .......... .......... .......... .......... 54%  223M 0s\n",
            " 31000K .......... .......... .......... .......... .......... 54%  260M 0s\n",
            " 31050K .......... .......... .......... .......... .......... 54%  205M 0s\n",
            " 31100K .......... .......... .......... .......... .......... 54%  228M 0s\n",
            " 31150K .......... .......... .......... .......... .......... 54%  191M 0s\n",
            " 31200K .......... .......... .......... .......... .......... 54%  257M 0s\n",
            " 31250K .......... .......... .......... .......... .......... 54%  250M 0s\n",
            " 31300K .......... .......... .......... .......... .......... 54%  262M 0s\n",
            " 31350K .......... .......... .......... .......... .......... 54%  232M 0s\n",
            " 31400K .......... .......... .......... .......... .......... 55%  413M 0s\n",
            " 31450K .......... .......... .......... .......... .......... 55%  451M 0s\n",
            " 31500K .......... .......... .......... .......... .......... 55%  441M 0s\n",
            " 31550K .......... .......... .......... .......... .......... 55%  357M 0s\n",
            " 31600K .......... .......... .......... .......... .......... 55%  405M 0s\n",
            " 31650K .......... .......... .......... .......... .......... 55%  429M 0s\n",
            " 31700K .......... .......... .......... .......... .......... 55%  359M 0s\n",
            " 31750K .......... .......... .......... .......... .......... 55%  210M 0s\n",
            " 31800K .......... .......... .......... .......... .......... 55%  244M 0s\n",
            " 31850K .......... .......... .......... .......... .......... 55%  224M 0s\n",
            " 31900K .......... .......... .......... .......... .......... 55%  230M 0s\n",
            " 31950K .......... .......... .......... .......... .......... 56%  196M 0s\n",
            " 32000K .......... .......... .......... .......... .......... 56%  213M 0s\n",
            " 32050K .......... .......... .......... .......... .......... 56%  216M 0s\n",
            " 32100K .......... .......... .......... .......... .......... 56%  218M 0s\n",
            " 32150K .......... .......... .......... .......... .......... 56%  261M 0s\n",
            " 32200K .......... .......... .......... .......... .......... 56%  218M 0s\n",
            " 32250K .......... .......... .......... .......... .......... 56%  283M 0s\n",
            " 32300K .......... .......... .......... .......... .......... 56%  253M 0s\n",
            " 32350K .......... .......... .......... .......... .......... 56%  250M 0s\n",
            " 32400K .......... .......... .......... .......... .......... 56%  290M 0s\n",
            " 32450K .......... .......... .......... .......... .......... 56%  433M 0s\n",
            " 32500K .......... .......... .......... .......... .......... 57%  179M 0s\n",
            " 32550K .......... .......... .......... .......... .......... 57%  188M 0s\n",
            " 32600K .......... .......... .......... .......... .......... 57%  188M 0s\n",
            " 32650K .......... .......... .......... .......... .......... 57%  192M 0s\n",
            " 32700K .......... .......... .......... .......... .......... 57%  213M 0s\n",
            " 32750K .......... .......... .......... .......... .......... 57%  186M 0s\n",
            " 32800K .......... .......... .......... .......... .......... 57%  225M 0s\n",
            " 32850K .......... .......... .......... .......... .......... 57%  246M 0s\n",
            " 32900K .......... .......... .......... .......... .......... 57%  228M 0s\n",
            " 32950K .......... .......... .......... .......... .......... 57%  217M 0s\n",
            " 33000K .......... .......... .......... .......... .......... 57%  250M 0s\n",
            " 33050K .......... .......... .......... .......... .......... 57%  245M 0s\n",
            " 33100K .......... .......... .......... .......... .......... 58%  247M 0s\n",
            " 33150K .......... .......... .......... .......... .......... 58%  205M 0s\n",
            " 33200K .......... .......... .......... .......... .......... 58%  253M 0s\n",
            " 33250K .......... .......... .......... .......... .......... 58%  253M 0s\n",
            " 33300K .......... .......... .......... .......... .......... 58%  249M 0s\n",
            " 33350K .......... .......... .......... .......... .......... 58%  193M 0s\n",
            " 33400K .......... .......... .......... .......... .......... 58%  217M 0s\n",
            " 33450K .......... .......... .......... .......... .......... 58%  204M 0s\n",
            " 33500K .......... .......... .......... .......... .......... 58%  213M 0s\n",
            " 33550K .......... .......... .......... .......... .......... 58%  184M 0s\n",
            " 33600K .......... .......... .......... .......... .......... 58%  185M 0s\n",
            " 33650K .......... .......... .......... .......... .......... 59%  215M 0s\n",
            " 33700K .......... .......... .......... .......... .......... 59%  208M 0s\n",
            " 33750K .......... .......... .......... .......... .......... 59%  217M 0s\n",
            " 33800K .......... .......... .......... .......... .......... 59%  235M 0s\n",
            " 33850K .......... .......... .......... .......... .......... 59%  205M 0s\n",
            " 33900K .......... .......... .......... .......... .......... 59%  222M 0s\n",
            " 33950K .......... .......... .......... .......... .......... 59%  205M 0s\n",
            " 34000K .......... .......... .......... .......... .......... 59%  238M 0s\n",
            " 34050K .......... .......... .......... .......... .......... 59%  206M 0s\n",
            " 34100K .......... .......... .......... .......... .......... 59%  227M 0s\n",
            " 34150K .......... .......... .......... .......... .......... 59%  196M 0s\n",
            " 34200K .......... .......... .......... .......... .......... 59%  226M 0s\n",
            " 34250K .......... .......... .......... .......... .......... 60%  202M 0s\n",
            " 34300K .......... .......... .......... .......... .......... 60%  208M 0s\n",
            " 34350K .......... .......... .......... .......... .......... 60%  176M 0s\n",
            " 34400K .......... .......... .......... .......... .......... 60%  232M 0s\n",
            " 34450K .......... .......... .......... .......... .......... 60%  227M 0s\n",
            " 34500K .......... .......... .......... .......... .......... 60%  203M 0s\n",
            " 34550K .......... .......... .......... .......... .......... 60%  209M 0s\n",
            " 34600K .......... .......... .......... .......... .......... 60%  382M 0s\n",
            " 34650K .......... .......... .......... .......... .......... 60%  434M 0s\n",
            " 34700K .......... .......... .......... .......... .......... 60%  357M 0s\n",
            " 34750K .......... .......... .......... .......... .......... 60%  181M 0s\n",
            " 34800K .......... .......... .......... .......... .......... 61%  251M 0s\n",
            " 34850K .......... .......... .......... .......... .......... 61%  223M 0s\n",
            " 34900K .......... .......... .......... .......... .......... 61%  282M 0s\n",
            " 34950K .......... .......... .......... .......... .......... 61%  201M 0s\n",
            " 35000K .......... .......... .......... .......... .......... 61%  258M 0s\n",
            " 35050K .......... .......... .......... .......... .......... 61%  281M 0s\n",
            " 35100K .......... .......... .......... .......... .......... 61%  204M 0s\n",
            " 35150K .......... .......... .......... .......... .......... 61%  187M 0s\n",
            " 35200K .......... .......... .......... .......... .......... 61%  221M 0s\n",
            " 35250K .......... .......... .......... .......... .......... 61%  313M 0s\n",
            " 35300K .......... .......... .......... .......... .......... 61%  250M 0s\n",
            " 35350K .......... .......... .......... .......... .......... 61%  209M 0s\n",
            " 35400K .......... .......... .......... .......... .......... 62%  281M 0s\n",
            " 35450K .......... .......... .......... .......... .......... 62%  251M 0s\n",
            " 35500K .......... .......... .......... .......... .......... 62%  261M 0s\n",
            " 35550K .......... .......... .......... .......... .......... 62%  284M 0s\n",
            " 35600K .......... .......... .......... .......... .......... 62%  278M 0s\n",
            " 35650K .......... .......... .......... .......... .......... 62%  368M 0s\n",
            " 35700K .......... .......... .......... .......... .......... 62%  290M 0s\n",
            " 35750K .......... .......... .......... .......... .......... 62%  237M 0s\n",
            " 35800K .......... .......... .......... .......... .......... 62%  288M 0s\n",
            " 35850K .......... .......... .......... .......... .......... 62%  406M 0s\n",
            " 35900K .......... .......... .......... .......... .......... 62%  441M 0s\n",
            " 35950K .......... .......... .......... .......... .......... 63%  315M 0s\n",
            " 36000K .......... .......... .......... .......... .......... 63%  266M 0s\n",
            " 36050K .......... .......... .......... .......... .......... 63%  251M 0s\n",
            " 36100K .......... .......... .......... .......... .......... 63%  257M 0s\n",
            " 36150K .......... .......... .......... .......... .......... 63%  250M 0s\n",
            " 36200K .......... .......... .......... .......... .......... 63%  364M 0s\n",
            " 36250K .......... .......... .......... .......... .......... 63%  386M 0s\n",
            " 36300K .......... .......... .......... .......... .......... 63%  401M 0s\n",
            " 36350K .......... .......... .......... .......... .......... 63%  237M 0s\n",
            " 36400K .......... .......... .......... .......... .......... 63%  293M 0s\n",
            " 36450K .......... .......... .......... .......... .......... 63%  295M 0s\n",
            " 36500K .......... .......... .......... .......... .......... 64%  250M 0s\n",
            " 36550K .......... .......... .......... .......... .......... 64%  252M 0s\n",
            " 36600K .......... .......... .......... .......... .......... 64%  280M 0s\n",
            " 36650K .......... .......... .......... .......... .......... 64% 55.7M 0s\n",
            " 36700K .......... .......... .......... .......... .......... 64% 56.2M 0s\n",
            " 36750K .......... .......... .......... .......... .......... 64% 60.0M 0s\n",
            " 36800K .......... .......... .......... .......... .......... 64%  219M 0s\n",
            " 36850K .......... .......... .......... .......... .......... 64%  241M 0s\n",
            " 36900K .......... .......... .......... .......... .......... 64%  241M 0s\n",
            " 36950K .......... .......... .......... .......... .......... 64%  226M 0s\n",
            " 37000K .......... .......... .......... .......... .......... 64%  254M 0s\n",
            " 37050K .......... .......... .......... .......... .......... 64%  201M 0s\n",
            " 37100K .......... .......... .......... .......... .......... 65%  194M 0s\n",
            " 37150K .......... .......... .......... .......... .......... 65%  180M 0s\n",
            " 37200K .......... .......... .......... .......... .......... 65%  234M 0s\n",
            " 37250K .......... .......... .......... .......... .......... 65%  238M 0s\n",
            " 37300K .......... .......... .......... .......... .......... 65%  258M 0s\n",
            " 37350K .......... .......... .......... .......... .......... 65%  210M 0s\n",
            " 37400K .......... .......... .......... .......... .......... 65%  253M 0s\n",
            " 37450K .......... .......... .......... .......... .......... 65%  247M 0s\n",
            " 37500K .......... .......... .......... .......... .......... 65%  236M 0s\n",
            " 37550K .......... .......... .......... .......... .......... 65%  205M 0s\n",
            " 37600K .......... .......... .......... .......... .......... 65%  237M 0s\n",
            " 37650K .......... .......... .......... .......... .......... 66%  206M 0s\n",
            " 37700K .......... .......... .......... .......... .......... 66%  231M 0s\n",
            " 37750K .......... .......... .......... .......... .......... 66%  227M 0s\n",
            " 37800K .......... .......... .......... .......... .......... 66%  236M 0s\n",
            " 37850K .......... .......... .......... .......... .......... 66%  248M 0s\n",
            " 37900K .......... .......... .......... .......... .......... 66%  259M 0s\n",
            " 37950K .......... .......... .......... .......... .......... 66%  204M 0s\n",
            " 38000K .......... .......... .......... .......... .......... 66%  252M 0s\n",
            " 38050K .......... .......... .......... .......... .......... 66%  222M 0s\n",
            " 38100K .......... .......... .......... .......... .......... 66%  235M 0s\n",
            " 38150K .......... .......... .......... .......... .......... 66%  206M 0s\n",
            " 38200K .......... .......... .......... .......... .......... 66%  252M 0s\n",
            " 38250K .......... .......... .......... .......... .......... 67%  254M 0s\n",
            " 38300K .......... .......... .......... .......... .......... 67%  238M 0s\n",
            " 38350K .......... .......... .......... .......... .......... 67%  189M 0s\n",
            " 38400K .......... .......... .......... .......... .......... 67%  202M 0s\n",
            " 38450K .......... .......... .......... .......... .......... 67%  181M 0s\n",
            " 38500K .......... .......... .......... .......... .......... 67%  193M 0s\n",
            " 38550K .......... .......... .......... .......... .......... 67%  183M 0s\n",
            " 38600K .......... .......... .......... .......... .......... 67%  209M 0s\n",
            " 38650K .......... .......... .......... .......... .......... 67%  182M 0s\n",
            " 38700K .......... .......... .......... .......... .......... 67%  197M 0s\n",
            " 38750K .......... .......... .......... .......... .......... 67%  215M 0s\n",
            " 38800K .......... .......... .......... .......... .......... 68%  236M 0s\n",
            " 38850K .......... .......... .......... .......... .......... 68%  380M 0s\n",
            " 38900K .......... .......... .......... .......... .......... 68%  342M 0s\n",
            " 38950K .......... .......... .......... .......... .......... 68%  151M 0s\n",
            " 39000K .......... .......... .......... .......... .......... 68%  199M 0s\n",
            " 39050K .......... .......... .......... .......... .......... 68%  200M 0s\n",
            " 39100K .......... .......... .......... .......... .......... 68%  226M 0s\n",
            " 39150K .......... .......... .......... .......... .......... 68%  182M 0s\n",
            " 39200K .......... .......... .......... .......... .......... 68%  270M 0s\n",
            " 39250K .......... .......... .......... .......... .......... 68%  264M 0s\n",
            " 39300K .......... .......... .......... .......... .......... 68%  274M 0s\n",
            " 39350K .......... .......... .......... .......... .......... 69%  261M 0s\n",
            " 39400K .......... .......... .......... .......... .......... 69%  325M 0s\n",
            " 39450K .......... .......... .......... .......... .......... 69%  446M 0s\n",
            " 39500K .......... .......... .......... .......... .......... 69%  427M 0s\n",
            " 39550K .......... .......... .......... .......... .......... 69%  361M 0s\n",
            " 39600K .......... .......... .......... .......... .......... 69%  447M 0s\n",
            " 39650K .......... .......... .......... .......... .......... 69%  409M 0s\n",
            " 39700K .......... .......... .......... .......... .......... 69%  428M 0s\n",
            " 39750K .......... .......... .......... .......... .......... 69%  374M 0s\n",
            " 39800K .......... .......... .......... .......... .......... 69%  335M 0s\n",
            " 39850K .......... .......... .......... .......... .......... 69%  258M 0s\n",
            " 39900K .......... .......... .......... .......... .......... 69%  242M 0s\n",
            " 39950K .......... .......... .......... .......... .......... 70%  206M 0s\n",
            " 40000K .......... .......... .......... .......... .......... 70%  257M 0s\n",
            " 40050K .......... .......... .......... .......... .......... 70%  246M 0s\n",
            " 40100K .......... .......... .......... .......... .......... 70%  257M 0s\n",
            " 40150K .......... .......... .......... .......... .......... 70%  181M 0s\n",
            " 40200K .......... .......... .......... .......... .......... 70%  189M 0s\n",
            " 40250K .......... .......... .......... .......... .......... 70%  187M 0s\n",
            " 40300K .......... .......... .......... .......... .......... 70%  216M 0s\n",
            " 40350K .......... .......... .......... .......... .......... 70%  193M 0s\n",
            " 40400K .......... .......... .......... .......... .......... 70%  190M 0s\n",
            " 40450K .......... .......... .......... .......... .......... 70%  201M 0s\n",
            " 40500K .......... .......... .......... .......... .......... 71%  207M 0s\n",
            " 40550K .......... .......... .......... .......... .......... 71%  222M 0s\n",
            " 40600K .......... .......... .......... .......... .......... 71%  206M 0s\n",
            " 40650K .......... .......... .......... .......... .......... 71%  207M 0s\n",
            " 40700K .......... .......... .......... .......... .......... 71%  215M 0s\n",
            " 40750K .......... .......... .......... .......... .......... 71%  178M 0s\n",
            " 40800K .......... .......... .......... .......... .......... 71%  230M 0s\n",
            " 40850K .......... .......... .......... .......... .......... 71%  230M 0s\n",
            " 40900K .......... .......... .......... .......... .......... 71%  238M 0s\n",
            " 40950K .......... .......... .......... .......... .......... 71%  201M 0s\n",
            " 41000K .......... .......... .......... .......... .......... 71%  230M 0s\n",
            " 41050K .......... .......... .......... .......... .......... 71%  210M 0s\n",
            " 41100K .......... .......... .......... .......... .......... 72%  236M 0s\n",
            " 41150K .......... .......... .......... .......... .......... 72%  177M 0s\n",
            " 41200K .......... .......... .......... .......... .......... 72%  228M 0s\n",
            " 41250K .......... .......... .......... .......... .......... 72%  222M 0s\n",
            " 41300K .......... .......... .......... .......... .......... 72%  240M 0s\n",
            " 41350K .......... .......... .......... .......... .......... 72%  205M 0s\n",
            " 41400K .......... .......... .......... .......... .......... 72%  224M 0s\n",
            " 41450K .......... .......... .......... .......... .......... 72%  257M 0s\n",
            " 41500K .......... .......... .......... .......... .......... 72%  254M 0s\n",
            " 41550K .......... .......... .......... .......... .......... 72%  215M 0s\n",
            " 41600K .......... .......... .......... .......... .......... 72%  245M 0s\n",
            " 41650K .......... .......... .......... .......... .......... 73%  213M 0s\n",
            " 41700K .......... .......... .......... .......... .......... 73%  193M 0s\n",
            " 41750K .......... .......... .......... .......... .......... 73%  181M 0s\n",
            " 41800K .......... .......... .......... .......... .......... 73%  188M 0s\n",
            " 41850K .......... .......... .......... .......... .......... 73%  210M 0s\n",
            " 41900K .......... .......... .......... .......... .......... 73%  203M 0s\n",
            " 41950K .......... .......... .......... .......... .......... 73%  185M 0s\n",
            " 42000K .......... .......... .......... .......... .......... 73%  226M 0s\n",
            " 42050K .......... .......... .......... .......... .......... 73%  250M 0s\n",
            " 42100K .......... .......... .......... .......... .......... 73%  249M 0s\n",
            " 42150K .......... .......... .......... .......... .......... 73%  188M 0s\n",
            " 42200K .......... .......... .......... .......... .......... 73%  222M 0s\n",
            " 42250K .......... .......... .......... .......... .......... 74%  246M 0s\n",
            " 42300K .......... .......... .......... .......... .......... 74%  231M 0s\n",
            " 42350K .......... .......... .......... .......... .......... 74%  183M 0s\n",
            " 42400K .......... .......... .......... .......... .......... 74%  252M 0s\n",
            " 42450K .......... .......... .......... .......... .......... 74%  254M 0s\n",
            " 42500K .......... .......... .......... .......... .......... 74%  258M 0s\n",
            " 42550K .......... .......... .......... .......... .......... 74%  280M 0s\n",
            " 42600K .......... .......... .......... .......... .......... 74%  438M 0s\n",
            " 42650K .......... .......... .......... .......... .......... 74%  329M 0s\n",
            " 42700K .......... .......... .......... .......... .......... 74%  376M 0s\n",
            " 42750K .......... .......... .......... .......... .......... 74%  176M 0s\n",
            " 42800K .......... .......... .......... .......... .......... 75%  250M 0s\n",
            " 42850K .......... .......... .......... .......... .......... 75%  249M 0s\n",
            " 42900K .......... .......... .......... .......... .......... 75%  244M 0s\n",
            " 42950K .......... .......... .......... .......... .......... 75%  208M 0s\n",
            " 43000K .......... .......... .......... .......... .......... 75%  243M 0s\n",
            " 43050K .......... .......... .......... .......... .......... 75%  260M 0s\n",
            " 43100K .......... .......... .......... .......... .......... 75%  255M 0s\n",
            " 43150K .......... .......... .......... .......... .......... 75%  207M 0s\n",
            " 43200K .......... .......... .......... .......... .......... 75%  259M 0s\n",
            " 43250K .......... .......... .......... .......... .......... 75%  243M 0s\n",
            " 43300K .......... .......... .......... .......... .......... 75%  251M 0s\n",
            " 43350K .......... .......... .......... .......... .......... 76%  234M 0s\n",
            " 43400K .......... .......... .......... .......... .......... 76%  203M 0s\n",
            " 43450K .......... .......... .......... .......... .......... 76%  202M 0s\n",
            " 43500K .......... .......... .......... .......... .......... 76%  213M 0s\n",
            " 43550K .......... .......... .......... .......... .......... 76%  162M 0s\n",
            " 43600K .......... .......... .......... .......... .......... 76%  200M 0s\n",
            " 43650K .......... .......... .......... .......... .......... 76%  178M 0s\n",
            " 43700K .......... .......... .......... .......... .......... 76%  213M 0s\n",
            " 43750K .......... .......... .......... .......... .......... 76%  178M 0s\n",
            " 43800K .......... .......... .......... .......... .......... 76%  216M 0s\n",
            " 43850K .......... .......... .......... .......... .......... 76%  251M 0s\n",
            " 43900K .......... .......... .......... .......... .......... 76%  223M 0s\n",
            " 43950K .......... .......... .......... .......... .......... 77%  209M 0s\n",
            " 44000K .......... .......... .......... .......... .......... 77%  199M 0s\n",
            " 44050K .......... .......... .......... .......... .......... 77%  224M 0s\n",
            " 44100K .......... .......... .......... .......... .......... 77%  251M 0s\n",
            " 44150K .......... .......... .......... .......... .......... 77%  218M 0s\n",
            " 44200K .......... .......... .......... .......... .......... 77%  263M 0s\n",
            " 44250K .......... .......... .......... .......... .......... 77%  258M 0s\n",
            " 44300K .......... .......... .......... .......... .......... 77%  256M 0s\n",
            " 44350K .......... .......... .......... .......... .......... 77%  230M 0s\n",
            " 44400K .......... .......... .......... .......... .......... 77%  433M 0s\n",
            " 44450K .......... .......... .......... .......... .......... 77%  433M 0s\n",
            " 44500K .......... .......... .......... .......... .......... 78%  439M 0s\n",
            " 44550K .......... .......... .......... .......... .......... 78%  376M 0s\n",
            " 44600K .......... .......... .......... .......... .......... 78%  404M 0s\n",
            " 44650K .......... .......... .......... .......... .......... 78%  422M 0s\n",
            " 44700K .......... .......... .......... .......... .......... 78%  419M 0s\n",
            " 44750K .......... .......... .......... .......... .......... 78%  214M 0s\n",
            " 44800K .......... .......... .......... .......... .......... 78%  246M 0s\n",
            " 44850K .......... .......... .......... .......... .......... 78%  260M 0s\n",
            " 44900K .......... .......... .......... .......... .......... 78%  257M 0s\n",
            " 44950K .......... .......... .......... .......... .......... 78%  228M 0s\n",
            " 45000K .......... .......... .......... .......... .......... 78%  237M 0s\n",
            " 45050K .......... .......... .......... .......... .......... 78%  261M 0s\n",
            " 45100K .......... .......... .......... .......... .......... 79%  268M 0s\n",
            " 45150K .......... .......... .......... .......... .......... 79%  199M 0s\n",
            " 45200K .......... .......... .......... .......... .......... 79%  252M 0s\n",
            " 45250K .......... .......... .......... .......... .......... 79%  244M 0s\n",
            " 45300K .......... .......... .......... .......... .......... 79%  259M 0s\n",
            " 45350K .......... .......... .......... .......... .......... 79%  228M 0s\n",
            " 45400K .......... .......... .......... .......... .......... 79%  212M 0s\n",
            " 45450K .......... .......... .......... .......... .......... 79%  203M 0s\n",
            " 45500K .......... .......... .......... .......... .......... 79%  245M 0s\n",
            " 45550K .......... .......... .......... .......... .......... 79%  225M 0s\n",
            " 45600K .......... .......... .......... .......... .......... 79%  248M 0s\n",
            " 45650K .......... .......... .......... .......... .......... 80%  391M 0s\n",
            " 45700K .......... .......... .......... .......... .......... 80%  290M 0s\n",
            " 45750K .......... .......... .......... .......... .......... 80%  206M 0s\n",
            " 45800K .......... .......... .......... .......... .......... 80%  268M 0s\n",
            " 45850K .......... .......... .......... .......... .......... 80%  252M 0s\n",
            " 45900K .......... .......... .......... .......... .......... 80%  244M 0s\n",
            " 45950K .......... .......... .......... .......... .......... 80%  222M 0s\n",
            " 46000K .......... .......... .......... .......... .......... 80%  335M 0s\n",
            " 46050K .......... .......... .......... .......... .......... 80%  341M 0s\n",
            " 46100K .......... .......... .......... .......... .......... 80%  311M 0s\n",
            " 46150K .......... .......... .......... .......... .......... 80%  232M 0s\n",
            " 46200K .......... .......... .......... .......... .......... 81%  215M 0s\n",
            " 46250K .......... .......... .......... .......... .......... 81%  237M 0s\n",
            " 46300K .......... .......... .......... .......... .......... 81%  334M 0s\n",
            " 46350K .......... .......... .......... .......... .......... 81%  216M 0s\n",
            " 46400K .......... .......... .......... .......... .......... 81%  442M 0s\n",
            " 46450K .......... .......... .......... .......... .......... 81%  414M 0s\n",
            " 46500K .......... .......... .......... .......... .......... 81%  435M 0s\n",
            " 46550K .......... .......... .......... .......... .......... 81%  397M 0s\n",
            " 46600K .......... .......... .......... .......... .......... 81%  318M 0s\n",
            " 46650K .......... .......... .......... .......... .......... 81%  251M 0s\n",
            " 46700K .......... .......... .......... .......... .......... 81%  246M 0s\n",
            " 46750K .......... .......... .......... .......... .......... 81%  206M 0s\n",
            " 46800K .......... .......... .......... .......... .......... 82%  248M 0s\n",
            " 46850K .......... .......... .......... .......... .......... 82%  241M 0s\n",
            " 46900K .......... .......... .......... .......... .......... 82%  260M 0s\n",
            " 46950K .......... .......... .......... .......... .......... 82%  226M 0s\n",
            " 47000K .......... .......... .......... .......... .......... 82%  245M 0s\n",
            " 47050K .......... .......... .......... .......... .......... 82%  257M 0s\n",
            " 47100K .......... .......... .......... .......... .......... 82%  243M 0s\n",
            " 47150K .......... .......... .......... .......... .......... 82%  181M 0s\n",
            " 47200K .......... .......... .......... .......... .......... 82%  233M 0s\n",
            " 47250K .......... .......... .......... .......... .......... 82%  220M 0s\n",
            " 47300K .......... .......... .......... .......... .......... 82%  232M 0s\n",
            " 47350K .......... .......... .......... .......... .......... 83%  192M 0s\n",
            " 47400K .......... .......... .......... .......... .......... 83%  210M 0s\n",
            " 47450K .......... .......... .......... .......... .......... 83%  195M 0s\n",
            " 47500K .......... .......... .......... .......... .......... 83%  209M 0s\n",
            " 47550K .......... .......... .......... .......... .......... 83%  174M 0s\n",
            " 47600K .......... .......... .......... .......... .......... 83%  209M 0s\n",
            " 47650K .......... .......... .......... .......... .......... 83%  204M 0s\n",
            " 47700K .......... .......... .......... .......... .......... 83%  207M 0s\n",
            " 47750K .......... .......... .......... .......... .......... 83%  205M 0s\n",
            " 47800K .......... .......... .......... .......... .......... 83%  258M 0s\n",
            " 47850K .......... .......... .......... .......... .......... 83%  223M 0s\n",
            " 47900K .......... .......... .......... .......... .......... 83%  221M 0s\n",
            " 47950K .......... .......... .......... .......... .......... 84%  187M 0s\n",
            " 48000K .......... .......... .......... .......... .......... 84%  218M 0s\n",
            " 48050K .......... .......... .......... .......... .......... 84%  208M 0s\n",
            " 48100K .......... .......... .......... .......... .......... 84%  244M 0s\n",
            " 48150K .......... .......... .......... .......... .......... 84%  220M 0s\n",
            " 48200K .......... .......... .......... .......... .......... 84%  241M 0s\n",
            " 48250K .......... .......... .......... .......... .......... 84%  259M 0s\n",
            " 48300K .......... .......... .......... .......... .......... 84%  225M 0s\n",
            " 48350K .......... .......... .......... .......... .......... 84%  212M 0s\n",
            " 48400K .......... .......... .......... .......... .......... 84%  235M 0s\n",
            " 48450K .......... .......... .......... .......... .......... 84%  259M 0s\n",
            " 48500K .......... .......... .......... .......... .......... 85%  315M 0s\n",
            " 48550K .......... .......... .......... .......... .......... 85%  357M 0s\n",
            " 48600K .......... .......... .......... .......... .......... 85%  418M 0s\n",
            " 48650K .......... .......... .......... .......... .......... 85%  375M 0s\n",
            " 48700K .......... .......... .......... .......... .......... 85%  420M 0s\n",
            " 48750K .......... .......... .......... .......... .......... 85%  348M 0s\n",
            " 48800K .......... .......... .......... .......... .......... 85%  205M 0s\n",
            " 48850K .......... .......... .......... .......... .......... 85%  225M 0s\n",
            " 48900K .......... .......... .......... .......... .......... 85%  214M 0s\n",
            " 48950K .......... .......... .......... .......... .......... 85%  192M 0s\n",
            " 49000K .......... .......... .......... .......... .......... 85%  206M 0s\n",
            " 49050K .......... .......... .......... .......... .......... 85%  248M 0s\n",
            " 49100K .......... .......... .......... .......... .......... 86%  257M 0s\n",
            " 49150K .......... .......... .......... .......... .......... 86%  209M 0s\n",
            " 49200K .......... .......... .......... .......... .......... 86%  251M 0s\n",
            " 49250K .......... .......... .......... .......... .......... 86%  235M 0s\n",
            " 49300K .......... .......... .......... .......... .......... 86%  213M 0s\n",
            " 49350K .......... .......... .......... .......... .......... 86%  190M 0s\n",
            " 49400K .......... .......... .......... .......... .......... 86%  209M 0s\n",
            " 49450K .......... .......... .......... .......... .......... 86%  209M 0s\n",
            " 49500K .......... .......... .......... .......... .......... 86%  205M 0s\n",
            " 49550K .......... .......... .......... .......... .......... 86%  172M 0s\n",
            " 49600K .......... .......... .......... .......... .......... 86%  199M 0s\n",
            " 49650K .......... .......... .......... .......... .......... 87%  184M 0s\n",
            " 49700K .......... .......... .......... .......... .......... 87%  206M 0s\n",
            " 49750K .......... .......... .......... .......... .......... 87%  186M 0s\n",
            " 49800K .......... .......... .......... .......... .......... 87%  212M 0s\n",
            " 49850K .......... .......... .......... .......... .......... 87%  223M 0s\n",
            " 49900K .......... .......... .......... .......... .......... 87%  260M 0s\n",
            " 49950K .......... .......... .......... .......... .......... 87%  210M 0s\n",
            " 50000K .......... .......... .......... .......... .......... 87%  237M 0s\n",
            " 50050K .......... .......... .......... .......... .......... 87%  248M 0s\n",
            " 50100K .......... .......... .......... .......... .......... 87%  217M 0s\n",
            " 50150K .......... .......... .......... .......... .......... 87%  186M 0s\n",
            " 50200K .......... .......... .......... .......... .......... 88%  194M 0s\n",
            " 50250K .......... .......... .......... .......... .......... 88%  235M 0s\n",
            " 50300K .......... .......... .......... .......... .......... 88%  223M 0s\n",
            " 50350K .......... .......... .......... .......... .......... 88%  211M 0s\n",
            " 50400K .......... .......... .......... .......... .......... 88%  260M 0s\n",
            " 50450K .......... .......... .......... .......... .......... 88%  195M 0s\n",
            " 50500K .......... .......... .......... .......... .......... 88%  205M 0s\n",
            " 50550K .......... .......... .......... .......... .......... 88%  183M 0s\n",
            " 50600K .......... .......... .......... .......... .......... 88%  239M 0s\n",
            " 50650K .......... .......... .......... .......... .......... 88%  245M 0s\n",
            " 50700K .......... .......... .......... .......... .......... 88%  255M 0s\n",
            " 50750K .......... .......... .......... .......... .......... 88%  199M 0s\n",
            " 50800K .......... .......... .......... .......... .......... 89%  215M 0s\n",
            " 50850K .......... .......... .......... .......... .......... 89%  239M 0s\n",
            " 50900K .......... .......... .......... .......... .......... 89%  234M 0s\n",
            " 50950K .......... .......... .......... .......... .......... 89%  208M 0s\n",
            " 51000K .......... .......... .......... .......... .......... 89%  233M 0s\n",
            " 51050K .......... .......... .......... .......... .......... 89%  236M 0s\n",
            " 51100K .......... .......... .......... .......... .......... 89%  258M 0s\n",
            " 51150K .......... .......... .......... .......... .......... 89%  209M 0s\n",
            " 51200K .......... .......... .......... .......... .......... 89%  233M 0s\n",
            " 51250K .......... .......... .......... .......... .......... 89%  236M 0s\n",
            " 51300K .......... .......... .......... .......... .......... 89%  202M 0s\n",
            " 51350K .......... .......... .......... .......... .......... 90%  181M 0s\n",
            " 51400K .......... .......... .......... .......... .......... 90%  216M 0s\n",
            " 51450K .......... .......... .......... .......... .......... 90%  215M 0s\n",
            " 51500K .......... .......... .......... .......... .......... 90%  248M 0s\n",
            " 51550K .......... .......... .......... .......... .......... 90%  202M 0s\n",
            " 51600K .......... .......... .......... .......... .......... 90%  271M 0s\n",
            " 51650K .......... .......... .......... .......... .......... 90%  433M 0s\n",
            " 51700K .......... .......... .......... .......... .......... 90%  364M 0s\n",
            " 51750K .......... .......... .......... .......... .......... 90%  338M 0s\n",
            " 51800K .......... .......... .......... .......... .......... 90%  441M 0s\n",
            " 51850K .......... .......... .......... .......... .......... 90%  276M 0s\n",
            " 51900K .......... .......... .......... .......... .......... 90%  231M 0s\n",
            " 51950K .......... .......... .......... .......... .......... 91%  197M 0s\n",
            " 52000K .......... .......... .......... .......... .......... 91%  239M 0s\n",
            " 52050K .......... .......... .......... .......... .......... 91%  395M 0s\n",
            " 52100K .......... .......... .......... .......... .......... 91%  450M 0s\n",
            " 52150K .......... .......... .......... .......... .......... 91%  262M 0s\n",
            " 52200K .......... .......... .......... .......... .......... 91%  254M 0s\n",
            " 52250K .......... .......... .......... .......... .......... 91%  263M 0s\n",
            " 52300K .......... .......... .......... .......... .......... 91%  232M 0s\n",
            " 52350K .......... .......... .......... .......... .......... 91%  265M 0s\n",
            " 52400K .......... .......... .......... .......... .......... 91%  433M 0s\n",
            " 52450K .......... .......... .......... .......... .......... 91%  450M 0s\n",
            " 52500K .......... .......... .......... .......... .......... 92%  394M 0s\n",
            " 52550K .......... .......... .......... .......... .......... 92%  397M 0s\n",
            " 52600K .......... .......... .......... .......... .......... 92%  459M 0s\n",
            " 52650K .......... .......... .......... .......... .......... 92%  304M 0s\n",
            " 52700K .......... .......... .......... .......... .......... 92%  261M 0s\n",
            " 52750K .......... .......... .......... .......... .......... 92%  206M 0s\n",
            " 52800K .......... .......... .......... .......... .......... 92%  247M 0s\n",
            " 52850K .......... .......... .......... .......... .......... 92%  254M 0s\n",
            " 52900K .......... .......... .......... .......... .......... 92%  248M 0s\n",
            " 52950K .......... .......... .......... .......... .......... 92%  230M 0s\n",
            " 53000K .......... .......... .......... .......... .......... 92%  251M 0s\n",
            " 53050K .......... .......... .......... .......... .......... 92%  251M 0s\n",
            " 53100K .......... .......... .......... .......... .......... 93%  259M 0s\n",
            " 53150K .......... .......... .......... .......... .......... 93%  207M 0s\n",
            " 53200K .......... .......... .......... .......... .......... 93%  206M 0s\n",
            " 53250K .......... .......... .......... .......... .......... 93%  194M 0s\n",
            " 53300K .......... .......... .......... .......... .......... 93%  197M 0s\n",
            " 53350K .......... .......... .......... .......... .......... 93%  174M 0s\n",
            " 53400K .......... .......... .......... .......... .......... 93%  191M 0s\n",
            " 53450K .......... .......... .......... .......... .......... 93%  225M 0s\n",
            " 53500K .......... .......... .......... .......... .......... 93%  185M 0s\n",
            " 53550K .......... .......... .......... .......... .......... 93%  188M 0s\n",
            " 53600K .......... .......... .......... .......... .......... 93%  209M 0s\n",
            " 53650K .......... .......... .......... .......... .......... 94%  220M 0s\n",
            " 53700K .......... .......... .......... .......... .......... 94%  233M 0s\n",
            " 53750K .......... .......... .......... .......... .......... 94%  176M 0s\n",
            " 53800K .......... .......... .......... .......... .......... 94%  257M 0s\n",
            " 53850K .......... .......... .......... .......... .......... 94%  253M 0s\n",
            " 53900K .......... .......... .......... .......... .......... 94%  211M 0s\n",
            " 53950K .......... .......... .......... .......... .......... 94%  188M 0s\n",
            " 54000K .......... .......... .......... .......... .......... 94%  205M 0s\n",
            " 54050K .......... .......... .......... .......... .......... 94%  236M 0s\n",
            " 54100K .......... .......... .......... .......... .......... 94%  230M 0s\n",
            " 54150K .......... .......... .......... .......... .......... 94%  218M 0s\n",
            " 54200K .......... .......... .......... .......... .......... 95%  247M 0s\n",
            " 54250K .......... .......... .......... .......... .......... 95%  259M 0s\n",
            " 54300K .......... .......... .......... .......... .......... 95%  260M 0s\n",
            " 54350K .......... .......... .......... .......... .......... 95%  213M 0s\n",
            " 54400K .......... .......... .......... .......... .......... 95%  265M 0s\n",
            " 54450K .......... .......... .......... .......... .......... 95%  250M 0s\n",
            " 54500K .......... .......... .......... .......... .......... 95%  245M 0s\n",
            " 54550K .......... .......... .......... .......... .......... 95%  194M 0s\n",
            " 54600K .......... .......... .......... .......... .......... 95%  201M 0s\n",
            " 54650K .......... .......... .......... .......... .......... 95%  254M 0s\n",
            " 54700K .......... .......... .......... .......... .......... 95%  209M 0s\n",
            " 54750K .......... .......... .......... .......... .......... 95%  191M 0s\n",
            " 54800K .......... .......... .......... .......... .......... 96%  183M 0s\n",
            " 54850K .......... .......... .......... .......... .......... 96%  196M 0s\n",
            " 54900K .......... .......... .......... .......... .......... 96%  193M 0s\n",
            " 54950K .......... .......... .......... .......... .......... 96%  213M 0s\n",
            " 55000K .......... .......... .......... .......... .......... 96%  238M 0s\n",
            " 55050K .......... .......... .......... .......... .......... 96%  263M 0s\n",
            " 55100K .......... .......... .......... .......... .......... 96%  250M 0s\n",
            " 55150K .......... .......... .......... .......... .......... 96%  211M 0s\n",
            " 55200K .......... .......... .......... .......... .......... 96%  257M 0s\n",
            " 55250K .......... .......... .......... .......... .......... 96%  258M 0s\n",
            " 55300K .......... .......... .......... .......... .......... 96%  217M 0s\n",
            " 55350K .......... .......... .......... .......... .......... 97%  207M 0s\n",
            " 55400K .......... .......... .......... .......... .......... 97%  255M 0s\n",
            " 55450K .......... .......... .......... .......... .......... 97%  250M 0s\n",
            " 55500K .......... .......... .......... .......... .......... 97%  237M 0s\n",
            " 55550K .......... .......... .......... .......... .......... 97%  214M 0s\n",
            " 55600K .......... .......... .......... .......... .......... 97%  247M 0s\n",
            " 55650K .......... .......... .......... .......... .......... 97%  254M 0s\n",
            " 55700K .......... .......... .......... .......... .......... 97%  254M 0s\n",
            " 55750K .......... .......... .......... .......... .......... 97%  217M 0s\n",
            " 55800K .......... .......... .......... .......... .......... 97%  254M 0s\n",
            " 55850K .......... .......... .......... .......... .......... 97%  242M 0s\n",
            " 55900K .......... .......... .......... .......... .......... 97%  242M 0s\n",
            " 55950K .......... .......... .......... .......... .......... 98%  217M 0s\n",
            " 56000K .......... .......... .......... .......... .......... 98%  236M 0s\n",
            " 56050K .......... .......... .......... .......... .......... 98%  243M 0s\n",
            " 56100K .......... .......... .......... .......... .......... 98%  238M 0s\n",
            " 56150K .......... .......... .......... .......... .......... 98%  224M 0s\n",
            " 56200K .......... .......... .......... .......... .......... 98%  261M 0s\n",
            " 56250K .......... .......... .......... .......... .......... 98%  245M 0s\n",
            " 56300K .......... .......... .......... .......... .......... 98%  258M 0s\n",
            " 56350K .......... .......... .......... .......... .......... 98%  208M 0s\n",
            " 56400K .......... .......... .......... .......... .......... 98%  256M 0s\n",
            " 56450K .......... .......... .......... .......... .......... 98%  261M 0s\n",
            " 56500K .......... .......... .......... .......... .......... 99%  249M 0s\n",
            " 56550K .......... .......... .......... .......... .......... 99%  230M 0s\n",
            " 56600K .......... .......... .......... .......... .......... 99%  237M 0s\n",
            " 56650K .......... .......... .......... .......... .......... 99%  260M 0s\n",
            " 56700K .......... .......... .......... .......... .......... 99%  254M 0s\n",
            " 56750K .......... .......... .......... .......... .......... 99%  201M 0s\n",
            " 56800K .......... .......... .......... .......... .......... 99%  257M 0s\n",
            " 56850K .......... .......... .......... .......... .......... 99%  243M 0s\n",
            " 56900K .......... .......... .......... .......... .......... 99%  257M 0s\n",
            " 56950K .......... .......... .......... .......... .......... 99%  230M 0s\n",
            " 57000K .......... .......... .......... .......... .......... 99%  235M 0s\n",
            " 57050K .......... .......... .......... .......... ........  100%  245M=0.3s\n",
            "\n",
            "2022-04-16 22:22:24 (192 MB/s) - ‘Miniconda3-4.5.4-Linux-x86_64.sh’ saved [58468498/58468498]\n",
            "\n",
            "Python 3.6.5 :: Anaconda, Inc.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "conda install --channel defaults conda python=3.7 --yes\n",
        "conda update --channel defaults --all --yes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "svkqJx4w-_Rj",
        "outputId": "2830a22a-8e82-4444-c135-06a963db0bcd"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Solving environment: ...working... done\n",
            "\n",
            "## Package Plan ##\n",
            "\n",
            "  environment location: /usr/local\n",
            "\n",
            "  added / updated specs: \n",
            "    - conda\n",
            "    - python=3.7\n",
            "\n",
            "\n",
            "The following packages will be downloaded:\n",
            "\n",
            "    package                    |            build\n",
            "    ---------------------------|-----------------\n",
            "    conda-4.12.0               |   py37h06a4308_0        16.9 MB\n",
            "    wheel-0.37.1               |     pyhd3eb1b0_0          31 KB\n",
            "    cffi-1.15.0                |   py37hd667e15_1         224 KB\n",
            "    ruamel_yaml-0.15.100       |   py37h27cfd23_0         267 KB\n",
            "    pycosat-0.6.3              |   py37h27cfd23_0         108 KB\n",
            "    xz-5.2.5                   |       h7b6447c_0         438 KB\n",
            "    libffi-3.3                 |       he6710b0_2          54 KB\n",
            "    pip-21.2.2                 |   py37h06a4308_0         2.0 MB\n",
            "    pyopenssl-22.0.0           |     pyhd3eb1b0_0          49 KB\n",
            "    conda-package-handling-1.8.1|   py37h7f8727e_0         954 KB\n",
            "    tqdm-4.63.0                |     pyhd3eb1b0_0          80 KB\n",
            "    idna-3.3                   |     pyhd3eb1b0_0          55 KB\n",
            "    zlib-1.2.11                |       h7f8727e_4         125 KB\n",
            "    pysocks-1.7.1              |           py37_1          27 KB\n",
            "    openssl-1.1.1n             |       h7f8727e_0         3.8 MB\n",
            "    setuptools-61.2.0          |   py37h06a4308_0         1.3 MB\n",
            "    requests-2.27.1            |     pyhd3eb1b0_0          52 KB\n",
            "    libgcc-ng-9.1.0            |       hdf63c60_0         8.1 MB\n",
            "    tk-8.6.11                  |       h1ccaba5_0         3.2 MB\n",
            "    python-3.7.13              |       h12debd9_0        53.5 MB\n",
            "    charset-normalizer-2.0.4   |     pyhd3eb1b0_0          33 KB\n",
            "    colorama-0.4.4             |     pyhd3eb1b0_0          21 KB\n",
            "    _libgcc_mutex-0.1          |             main           3 KB\n",
            "    ncurses-6.3                |       h7f8727e_2         1.0 MB\n",
            "    brotlipy-0.7.0             |py37h27cfd23_1003         350 KB\n",
            "    pycparser-2.21             |     pyhd3eb1b0_0          94 KB\n",
            "    libstdcxx-ng-9.1.0         |       hdf63c60_0         4.0 MB\n",
            "    yaml-0.2.5                 |       h7b6447c_0          87 KB\n",
            "    readline-8.1.2             |       h7f8727e_1         423 KB\n",
            "    urllib3-1.26.8             |     pyhd3eb1b0_0         100 KB\n",
            "    certifi-2021.10.8          |   py37h06a4308_2         156 KB\n",
            "    ld_impl_linux-64-2.35.1    |       h7274673_9         637 KB\n",
            "    cryptography-36.0.0        |   py37h9ce1e76_0         1.5 MB\n",
            "    ca-certificates-2022.3.29  |       h06a4308_0         124 KB\n",
            "    sqlite-3.38.2              |       hc218d9a_0         1.5 MB\n",
            "    ------------------------------------------------------------\n",
            "                                           Total:       101.4 MB\n",
            "\n",
            "The following NEW packages will be INSTALLED:\n",
            "\n",
            "    _libgcc_mutex:          0.1-main               \n",
            "    brotlipy:               0.7.0-py37h27cfd23_1003\n",
            "    charset-normalizer:     2.0.4-pyhd3eb1b0_0     \n",
            "    colorama:               0.4.4-pyhd3eb1b0_0     \n",
            "    conda-package-handling: 1.8.1-py37h7f8727e_0   \n",
            "    ld_impl_linux-64:       2.35.1-h7274673_9      \n",
            "    tqdm:                   4.63.0-pyhd3eb1b0_0    \n",
            "\n",
            "The following packages will be UPDATED:\n",
            "\n",
            "    ca-certificates:        2018.03.07-0            --> 2022.3.29-h06a4308_0    \n",
            "    certifi:                2018.4.16-py36_0        --> 2021.10.8-py37h06a4308_2\n",
            "    cffi:                   1.11.5-py36h9745a5d_0   --> 1.15.0-py37hd667e15_1   \n",
            "    conda:                  4.5.4-py36_0            --> 4.12.0-py37h06a4308_0   \n",
            "    cryptography:           2.2.2-py36h14c3975_0    --> 36.0.0-py37h9ce1e76_0   \n",
            "    idna:                   2.6-py36h82fb2a8_1      --> 3.3-pyhd3eb1b0_0        \n",
            "    libffi:                 3.2.1-hd88cf55_4        --> 3.3-he6710b0_2          \n",
            "    libgcc-ng:              7.2.0-hdf63c60_3        --> 9.1.0-hdf63c60_0        \n",
            "    libstdcxx-ng:           7.2.0-hdf63c60_3        --> 9.1.0-hdf63c60_0        \n",
            "    ncurses:                6.1-hf484d3e_0          --> 6.3-h7f8727e_2          \n",
            "    openssl:                1.0.2o-h20670df_0       --> 1.1.1n-h7f8727e_0       \n",
            "    pip:                    10.0.1-py36_0           --> 21.2.2-py37h06a4308_0   \n",
            "    pycosat:                0.6.3-py36h0a5515d_0    --> 0.6.3-py37h27cfd23_0    \n",
            "    pycparser:              2.18-py36hf9f622e_1     --> 2.21-pyhd3eb1b0_0       \n",
            "    pyopenssl:              18.0.0-py36_0           --> 22.0.0-pyhd3eb1b0_0     \n",
            "    pysocks:                1.6.8-py36_0            --> 1.7.1-py37_1            \n",
            "    python:                 3.6.5-hc3d631a_2        --> 3.7.13-h12debd9_0       \n",
            "    readline:               7.0-ha6073c6_4          --> 8.1.2-h7f8727e_1        \n",
            "    requests:               2.18.4-py36he2e5f8d_1   --> 2.27.1-pyhd3eb1b0_0     \n",
            "    ruamel_yaml:            0.15.37-py36h14c3975_2  --> 0.15.100-py37h27cfd23_0 \n",
            "    setuptools:             39.2.0-py36_0           --> 61.2.0-py37h06a4308_0   \n",
            "    sqlite:                 3.23.1-he433501_0       --> 3.38.2-hc218d9a_0       \n",
            "    tk:                     8.6.7-hc745277_3        --> 8.6.11-h1ccaba5_0       \n",
            "    urllib3:                1.22-py36hbe7ace6_0     --> 1.26.8-pyhd3eb1b0_0     \n",
            "    wheel:                  0.31.1-py36_0           --> 0.37.1-pyhd3eb1b0_0     \n",
            "    xz:                     5.2.4-h14c3975_4        --> 5.2.5-h7b6447c_0        \n",
            "    yaml:                   0.1.7-had09818_2        --> 0.2.5-h7b6447c_0        \n",
            "    zlib:                   1.2.11-ha838bed_2       --> 1.2.11-h7f8727e_4       \n",
            "\n",
            "\n",
            "Downloading and Extracting Packages\n",
            "Preparing transaction: ...working... done\n",
            "Verifying transaction: ...working... done\n",
            "Executing transaction: ...working... done\n",
            "Collecting package metadata (current_repodata.json): ...working... done\n",
            "Solving environment: ...working... done\n",
            "\n",
            "## Package Plan ##\n",
            "\n",
            "  environment location: /usr/local\n",
            "\n",
            "\n",
            "The following packages will be downloaded:\n",
            "\n",
            "    package                    |            build\n",
            "    ---------------------------|-----------------\n",
            "    _openmp_mutex-4.5          |            1_gnu          22 KB\n",
            "    libgcc-ng-9.3.0            |      h5101ec6_17         4.8 MB\n",
            "    libgomp-9.3.0              |      h5101ec6_17         311 KB\n",
            "    libstdcxx-ng-9.3.0         |      hd4cf53a_17         3.1 MB\n",
            "    ------------------------------------------------------------\n",
            "                                           Total:         8.2 MB\n",
            "\n",
            "The following NEW packages will be INSTALLED:\n",
            "\n",
            "  _openmp_mutex      pkgs/main/linux-64::_openmp_mutex-4.5-1_gnu\n",
            "  libgomp            pkgs/main/linux-64::libgomp-9.3.0-h5101ec6_17\n",
            "\n",
            "The following packages will be UPDATED:\n",
            "\n",
            "  libgcc-ng                                9.1.0-hdf63c60_0 --> 9.3.0-h5101ec6_17\n",
            "  libstdcxx-ng                             9.1.0-hdf63c60_0 --> 9.3.0-hd4cf53a_17\n",
            "\n",
            "\n",
            "\n",
            "Downloading and Extracting Packages\n",
            "\rlibgcc-ng-9.3.0      | 4.8 MB    |            |   0% \rlibgcc-ng-9.3.0      | 4.8 MB    | ###6       |  36% \rlibgcc-ng-9.3.0      | 4.8 MB    | ########## | 100% \rlibgcc-ng-9.3.0      | 4.8 MB    | ########## | 100% \n",
            "\rlibgomp-9.3.0        | 311 KB    |            |   0% \rlibgomp-9.3.0        | 311 KB    | ########## | 100% \n",
            "\r_openmp_mutex-4.5    | 22 KB     |            |   0% \r_openmp_mutex-4.5    | 22 KB     | ########## | 100% \n",
            "\rlibstdcxx-ng-9.3.0   | 3.1 MB    |            |   0% \rlibstdcxx-ng-9.3.0   | 3.1 MB    | ########## | 100% \rlibstdcxx-ng-9.3.0   | 3.1 MB    | ########## | 100% \n",
            "Preparing transaction: ...working... done\n",
            "Verifying transaction: ...working... done\n",
            "Executing transaction: ...working... done\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rconda-4.12.0         | 16.9 MB |            |   0% \rconda-4.12.0         | 16.9 MB | 9          |   9% \rconda-4.12.0         | 16.9 MB | #######    |  70% \rconda-4.12.0         | 16.9 MB | #########  |  91% \rconda-4.12.0         | 16.9 MB | ########## | 100% \n",
            "\rwheel-0.37.1         |   31 KB |            |   0% \rwheel-0.37.1         |   31 KB | ########## | 100% \n",
            "\rcffi-1.15.0          |  224 KB |            |   0% \rcffi-1.15.0          |  224 KB | ########## | 100% \n",
            "\rruamel_yaml-0.15.100 |  267 KB |            |   0% \rruamel_yaml-0.15.100 |  267 KB | ########## | 100% \n",
            "\rpycosat-0.6.3        |  108 KB |            |   0% \rpycosat-0.6.3        |  108 KB | ########## | 100% \n",
            "\rxz-5.2.5             |  438 KB |            |   0% \rxz-5.2.5             |  438 KB | #########9 | 100% \rxz-5.2.5             |  438 KB | ########## | 100% \n",
            "\rlibffi-3.3           |   54 KB |            |   0% \rlibffi-3.3           |   54 KB | ########## | 100% \n",
            "\rpip-21.2.2           |  2.0 MB |            |   0% \rpip-21.2.2           |  2.0 MB | #######7   |  78% \rpip-21.2.2           |  2.0 MB | #########5 |  95% \rpip-21.2.2           |  2.0 MB | ########## | 100% \n",
            "\rpyopenssl-22.0.0     |   49 KB |            |   0% \rpyopenssl-22.0.0     |   49 KB | ########## | 100% \n",
            "\rconda-package-handli |  954 KB |            |   0% \rconda-package-handli |  954 KB | ########6  |  87% \rconda-package-handli |  954 KB | ########## | 100% \n",
            "\rtqdm-4.63.0          |   80 KB |            |   0% \rtqdm-4.63.0          |   80 KB | ########## | 100% \n",
            "\ridna-3.3             |   55 KB |            |   0% \ridna-3.3             |   55 KB | ########## | 100% \n",
            "\rzlib-1.2.11          |  125 KB |            |   0% \rzlib-1.2.11          |  125 KB | ########## | 100% \n",
            "\rpysocks-1.7.1        |   27 KB |            |   0% \rpysocks-1.7.1        |   27 KB | ########## | 100% \n",
            "\ropenssl-1.1.1n       |  3.8 MB |            |   0% \ropenssl-1.1.1n       |  3.8 MB | #######7   |  77% \ropenssl-1.1.1n       |  3.8 MB | #########9 | 100% \ropenssl-1.1.1n       |  3.8 MB | ########## | 100% \n",
            "\rsetuptools-61.2.0    |  1.3 MB |            |   0% \rsetuptools-61.2.0    |  1.3 MB | #######9   |  80% \rsetuptools-61.2.0    |  1.3 MB | ########## | 100% \n",
            "\rrequests-2.27.1      |   52 KB |            |   0% \rrequests-2.27.1      |   52 KB | ########## | 100% \n",
            "\rlibgcc-ng-9.1.0      |  8.1 MB |            |   0% \rlibgcc-ng-9.1.0      |  8.1 MB | #######5   |  76% \rlibgcc-ng-9.1.0      |  8.1 MB | #########6 |  96% \rlibgcc-ng-9.1.0      |  8.1 MB | ########## | 100% \n",
            "\rtk-8.6.11            |  3.2 MB |            |   0% \rtk-8.6.11            |  3.2 MB | #######6   |  77% \rtk-8.6.11            |  3.2 MB | #########7 |  98% \rtk-8.6.11            |  3.2 MB | ########## | 100% \n",
            "\rpython-3.7.13        | 53.5 MB |            |   0% \rpython-3.7.13        | 53.5 MB | #6         |  17% \rpython-3.7.13        | 53.5 MB | ####1      |  42% \rpython-3.7.13        | 53.5 MB | #####2     |  53% \rpython-3.7.13        | 53.5 MB | ######3    |  64% \rpython-3.7.13        | 53.5 MB | #######5   |  75% \rpython-3.7.13        | 53.5 MB | ########4  |  85% \rpython-3.7.13        | 53.5 MB | #########1 |  92% \rpython-3.7.13        | 53.5 MB | #########7 |  97% \rpython-3.7.13        | 53.5 MB | ########## | 100% \n",
            "\rcharset-normalizer-2 |   33 KB |            |   0% \rcharset-normalizer-2 |   33 KB | ########## | 100% \n",
            "\rcolorama-0.4.4       |   21 KB |            |   0% \rcolorama-0.4.4       |   21 KB | ########## | 100% \n",
            "\r_libgcc_mutex-0.1    |    3 KB |            |   0% \r_libgcc_mutex-0.1    |    3 KB | ########## | 100% \n",
            "\rncurses-6.3          |  1.0 MB |            |   0% \rncurses-6.3          |  1.0 MB | ########7  |  87% \rncurses-6.3          |  1.0 MB | ########## | 100% \n",
            "\rbrotlipy-0.7.0       |  350 KB |            |   0% \rbrotlipy-0.7.0       |  350 KB | ########## | 100% \n",
            "\rpycparser-2.21       |   94 KB |            |   0% \rpycparser-2.21       |   94 KB | ########## | 100% \n",
            "\rlibstdcxx-ng-9.1.0   |  4.0 MB |            |   0% \rlibstdcxx-ng-9.1.0   |  4.0 MB | #######8   |  78% \rlibstdcxx-ng-9.1.0   |  4.0 MB | ########## | 100% \n",
            "\ryaml-0.2.5           |   87 KB |            |   0% \ryaml-0.2.5           |   87 KB | ########## | 100% \n",
            "\rreadline-8.1.2       |  423 KB |            |   0% \rreadline-8.1.2       |  423 KB | ########## | 100% \n",
            "\rurllib3-1.26.8       |  100 KB |            |   0% \rurllib3-1.26.8       |  100 KB | ########## | 100% \n",
            "\rcertifi-2021.10.8    |  156 KB |            |   0% \rcertifi-2021.10.8    |  156 KB | ########## | 100% \n",
            "\rld_impl_linux-64-2.3 |  637 KB |            |   0% \rld_impl_linux-64-2.3 |  637 KB | #########1 |  92% \rld_impl_linux-64-2.3 |  637 KB | ########## | 100% \n",
            "\rcryptography-36.0.0  |  1.5 MB |            |   0% \rcryptography-36.0.0  |  1.5 MB | #######7   |  78% \rcryptography-36.0.0  |  1.5 MB | #########7 |  98% \rcryptography-36.0.0  |  1.5 MB | ########## | 100% \n",
            "\rca-certificates-2022 |  124 KB |            |   0% \rca-certificates-2022 |  124 KB | ########## | 100% \n",
            "\rsqlite-3.38.2        |  1.5 MB |            |   0% \rsqlite-3.38.2        |  1.5 MB | ########2  |  83% \rsqlite-3.38.2        |  1.5 MB | ########## | 100% \n",
            "\n",
            "The environment is inconsistent, please check the package plan carefully\n",
            "The following packages are causing the inconsistency:\n",
            "\n",
            "  - defaults/linux-64::asn1crypto==0.24.0=py36_0\n",
            "  - defaults/linux-64::six==1.11.0=py36h372c433_1\n",
            "  - defaults/linux-64::chardet==3.0.4=py36h0f667ec_1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mTZMR43S_pbs",
        "outputId": "6b8d7e0b-881d-4ecc-d30c-975fcd646225"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['',\n",
              " '/content',\n",
              " '/env/python',\n",
              " '/usr/lib/python37.zip',\n",
              " '/usr/lib/python3.7',\n",
              " '/usr/lib/python3.7/lib-dynload',\n",
              " '/usr/local/lib/python3.7/dist-packages',\n",
              " '/usr/lib/python3/dist-packages',\n",
              " '/usr/local/lib/python3.7/dist-packages/IPython/extensions',\n",
              " '/root/.ipython']"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "_ = (sys.path\n",
        "        .append(\"/usr/local/lib/python3.7/site-packages\"))"
      ],
      "metadata": {
        "id": "WGxCzFAZ_4Mx"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/ozanozyegen/evaluation-local-explanation-time-series"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tKp3QPYJ5Xo2",
        "outputId": "aed8bd5f-448d-4b0b-9c22-468d76e74d68"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'evaluation-local-explanation-time-series'...\n",
            "remote: Enumerating objects: 65, done.\u001b[K\n",
            "remote: Counting objects: 100% (65/65), done.\u001b[K\n",
            "remote: Compressing objects: 100% (63/63), done.\u001b[K\n",
            "remote: Total 65 (delta 4), reused 52 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (65/65), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd evaluation-local-explanation-time-series/src"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CGsWsrLtQdAy",
        "outputId": "0c5cb599-d1cf-48b9-d880-41a26bc3d9e7"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/evaluation-local-explanation-time-series/src\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!conda env create -n eval-metric -f xai.yml"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3sUC2W7sC1CK",
        "outputId": "e72410d8-117e-41bf-e89d-3d6bde93ad40"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting package metadata (repodata.json): - \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\bdone\n",
            "Solving environment: \\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\bdone\n",
            "\n",
            "Downloading and Extracting Packages\n",
            "importlib_metadata-2 | 3 KB      | : 100% 1.0/1 [00:00<00:00,  9.28it/s]\n",
            "ipykernel-5.3.4      | 179 KB    | : 100% 1.0/1 [00:00<00:00,  4.62it/s]\n",
            "mkl_fft-1.2.1        | 148 KB    | : 100% 1.0/1 [00:00<00:00,  4.02it/s]\n",
            "cycler-0.10.0        | 13 KB     | : 100% 1.0/1 [00:00<00:00,  9.84it/s]\n",
            "mkl-2020.2           | 138.3 MB  | : 100% 1.0/1 [00:04<00:00,  4.80s/it]               \n",
            "mccabe-0.6.1         | 14 KB     | : 100% 1.0/1 [00:00<00:00,  8.53it/s]\n",
            "libgfortran-ng-7.3.0 | 1006 KB   | : 100% 1.0/1 [00:00<00:00,  5.90it/s]\n",
            "google-auth-oauthlib | 18 KB     | : 100% 1.0/1 [00:00<00:00,  4.96it/s]               \n",
            "mistune-0.8.4        | 54 KB     | : 100% 1.0/1 [00:00<00:00,  7.27it/s]\n",
            "freetype-2.10.4      | 596 KB    | : 100% 1.0/1 [00:00<00:00,  7.14it/s]\n",
            "rsa-4.7              | 30 KB     | : 100% 1.0/1 [00:00<00:00,  5.29it/s]               \n",
            "jpeg-9b              | 214 KB    | : 100% 1.0/1 [00:00<00:00,  8.87it/s]\n",
            "requests-oauthlib-1. | 23 KB     | : 100% 1.0/1 [00:00<00:00,  9.28it/s]\n",
            "threadpoolctl-2.1.0  | 15 KB     | : 100% 1.0/1 [00:00<00:00, 20.07it/s]\n",
            "nbconvert-6.0.7      | 484 KB    | : 100% 1.0/1 [00:00<00:00,  6.50it/s]\n",
            "pycodestyle-2.6.0    | 38 KB     | : 100% 1.0/1 [00:00<00:00, 32.81it/s]\n",
            "libuuid-1.0.3        | 15 KB     | : 100% 1.0/1 [00:00<00:00,  9.11it/s]\n",
            "cython-0.29.21       | 1.9 MB    | : 100% 1.0/1 [00:00<00:00,  5.06it/s]\n",
            "prompt_toolkit-3.0.8 | 4 KB      | : 100% 1.0/1 [00:00<00:00, 21.24it/s]\n",
            "zstd-1.4.5           | 619 KB    | : 100% 1.0/1 [00:00<00:00,  6.17it/s]\n",
            "pip-21.0.1           | 1.8 MB    | : 100% 1.0/1 [00:00<00:00,  4.67it/s]\n",
            "libprotobuf-3.14.0   | 2.0 MB    | : 100% 1.0/1 [00:00<00:00,  5.71it/s]\n",
            "grpcio-1.35.0        | 1.8 MB    | : 100% 1.0/1 [00:00<00:00,  1.72it/s]\n",
            "libxml2-2.9.10       | 1.2 MB    | : 100% 1.0/1 [00:00<00:00,  6.30it/s]\n",
            "requests-2.25.1      | 52 KB     | : 100% 1.0/1 [00:00<00:00,  9.29it/s]\n",
            "joblib-1.0.1         | 208 KB    | : 100% 1.0/1 [00:00<00:00,  7.15it/s]\n",
            "webencodings-0.5.1   | 19 KB     | : 100% 1.0/1 [00:00<00:00,  9.23it/s]\n",
            "jupyter_core-4.7.1   | 68 KB     | : 100% 1.0/1 [00:00<00:00,  7.89it/s]\n",
            "argon2-cffi-20.1.0   | 46 KB     | : 100% 1.0/1 [00:00<00:00,  8.63it/s]\n",
            "certifi-2020.12.5    | 141 KB    | : 100% 1.0/1 [00:00<00:00,  9.42it/s]\n",
            "scipy-1.6.0          | 15.4 MB   | : 100% 1.0/1 [00:01<00:00,  1.16s/it]\n",
            "_tflow_select-2.1.0  | 2 KB      | : 100% 1.0/1 [00:00<00:00,  9.51it/s]\n",
            "mkl-service-2.3.0    | 52 KB     | : 100% 1.0/1 [00:00<00:00,  9.27it/s]\n",
            "importlib-metadata-2 | 28 KB     | : 100% 1.0/1 [00:00<00:00, 18.59it/s]\n",
            "protobuf-3.14.0      | 303 KB    | : 100% 1.0/1 [00:00<00:00,  4.85it/s]\n",
            "blas-1.0             | 1 KB      | : 100% 1.0/1 [00:00<00:00, 18.64it/s]\n",
            "pyopenssl-20.0.1     | 49 KB     | : 100% 1.0/1 [00:00<00:00,  8.91it/s]\n",
            "wheel-0.36.2         | 33 KB     | : 100% 1.0/1 [00:00<00:00,  8.43it/s]\n",
            "glib-2.67.4          | 3.0 MB    | : 100% 1.0/1 [00:00<00:00,  4.47it/s]\n",
            "terminado-0.9.2      | 25 KB     | : 100% 1.0/1 [00:00<00:00,  5.41it/s]\n",
            "libtiff-4.1.0        | 449 KB    | : 100% 1.0/1 [00:00<00:00,  8.19it/s]\n",
            "pyrsistent-0.17.3    | 89 KB     | : 100% 1.0/1 [00:00<00:00,  5.42it/s]                \n",
            "widgetsnbextension-3 | 1.8 MB    | : 100% 1.0/1 [00:00<00:00,  2.06it/s]\n",
            "dbus-1.13.18         | 504 KB    | : 100% 1.0/1 [00:00<00:00,  8.27it/s]\n",
            "sqlite-3.33.0        | 1.1 MB    | : 100% 1.0/1 [00:00<00:00,  7.63it/s]\n",
            "cupti-10.1.168       | 1.4 MB    | : 100% 1.0/1 [00:00<00:00,  6.59it/s]\n",
            "blinker-1.4          | 23 KB     | : 100% 1.0/1 [00:00<00:00,  9.95it/s]\n",
            "numpy-base-1.19.2    | 4.1 MB    | : 100% 1.0/1 [00:00<00:00,  3.38it/s]\n",
            "jupyterlab_pygments- | 8 KB      | : 100% 1.0/1 [00:00<00:00,  9.59it/s]\n",
            "qtconsole-5.0.2      | 97 KB     | : 100% 1.0/1 [00:00<00:00,  5.67it/s]\n",
            "tensorflow-estimator | 254 KB    | : 100% 1.0/1 [00:00<00:00,  5.23it/s]\n",
            "ipython-7.20.0       | 991 KB    | : 100% 1.0/1 [00:00<00:00,  2.70it/s]\n",
            "openssl-1.1.1j       | 2.5 MB    | : 100% 1.0/1 [00:00<00:00,  5.56it/s]\n",
            "jupyter-1.0.0        | 6 KB      | : 100% 1.0/1 [00:00<00:00,  9.55it/s]\n",
            "ipython_genutils-0.2 | 27 KB     | : 100% 1.0/1 [00:00<00:00,  8.83it/s]\n",
            "qt-5.9.7             | 68.5 MB   | : 100% 1.0/1 [00:02<00:00,  2.71s/it]               \n",
            "qtpy-1.9.0           | 34 KB     | : 100% 1.0/1 [00:00<00:00, 19.42it/s]\n",
            "yarl-1.6.3           | 133 KB    | : 100% 1.0/1 [00:00<00:00,  8.43it/s]\n",
            "bleach-3.3.0         | 113 KB    | : 100% 1.0/1 [00:00<00:00,  8.70it/s]\n",
            "ipywidgets-7.6.3     | 105 KB    | : 100% 1.0/1 [00:00<00:00,  5.02it/s]               \n",
            "nbclient-0.5.2       | 58 KB     | : 100% 1.0/1 [00:00<00:00,  5.39it/s]\n",
            "scikit-learn-0.23.2  | 5.0 MB    | : 100% 1.0/1 [00:00<00:00,  2.39it/s]\n",
            "readline-8.1         | 362 KB    | : 100% 1.0/1 [00:00<00:00,  8.06it/s]\n",
            "ld_impl_linux-64-2.3 | 653 KB    | : 100% 1.0/1 [00:00<00:00,  6.66it/s]\n",
            "typed-ast-1.4.2      | 185 KB    | : 100% 1.0/1 [00:00<00:00,  3.50it/s]\n",
            "hdf5-1.10.6          | 3.7 MB    | : 100% 1.0/1 [00:00<00:00,  3.90it/s]\n",
            "lazy-object-proxy-1. | 28 KB     | : 100% 1.0/1 [00:00<00:00,  4.90it/s]               \n",
            "tqdm-4.56.0          | 80 KB     | : 100% 1.0/1 [00:00<00:00,  5.33it/s]\n",
            "jupyter_console-6.2. | 22 KB     | : 100% 1.0/1 [00:00<00:00, 21.36it/s]\n",
            "pandocfilters-1.4.3  | 14 KB     | : 100% 1.0/1 [00:00<00:00,  5.85it/s]\n",
            "markdown-3.3.3       | 127 KB    | : 100% 1.0/1 [00:00<00:00,  3.02it/s]\n",
            "tensorboard-plugin-w | 630 KB    | : 100% 1.0/1 [00:00<00:00,  7.49it/s]\n",
            "cudnn-7.6.5          | 179.9 MB  | : 100% 1.0/1 [00:03<00:00,  3.97s/it]               \n",
            "zeromq-4.3.3         | 500 KB    | : 100% 1.0/1 [00:00<00:00,  7.20it/s]\n",
            "cachetools-4.2.1     | 13 KB     | : 100% 1.0/1 [00:00<00:00,  5.16it/s]\n",
            "wcwidth-0.2.5        | 29 KB     | : 100% 1.0/1 [00:00<00:00,  8.74it/s]\n",
            "aiohttp-3.6.3        | 544 KB    | : 100% 1.0/1 [00:00<00:00,  2.15it/s]\n",
            "pyzmq-20.0.0         | 439 KB    | : 100% 1.0/1 [00:00<00:00,  7.13it/s]\n",
            "tensorflow-gpu-2.2.0 | 3 KB      | : 100% 1.0/1 [00:00<00:00,  9.17it/s]\n",
            "zipp-3.4.0           | 15 KB     | : 100% 1.0/1 [00:00<00:00,  9.28it/s]\n",
            "autopep8-1.5.5       | 42 KB     | : 100% 1.0/1 [00:00<00:00,  3.52it/s]                \n",
            "decorator-4.4.2      | 12 KB     | : 100% 1.0/1 [00:00<00:00, 41.85it/s]\n",
            "click-7.1.2          | 64 KB     | : 100% 1.0/1 [00:00<00:00, 27.45it/s]\n",
            "sip-4.19.8           | 274 KB    | : 100% 1.0/1 [00:00<00:00,  8.59it/s]\n",
            "libsodium-1.0.18     | 244 KB    | : 100% 1.0/1 [00:00<00:00,  8.62it/s]\n",
            "ca-certificates-2021 | 118 KB    | : 100% 1.0/1 [00:00<00:00,  9.69it/s]\n",
            "mkl_random-1.1.1     | 322 KB    | : 100% 1.0/1 [00:00<00:00,  8.60it/s]\n",
            "cudatoolkit-10.1.243 | 347.4 MB  | : 100% 1.0/1 [00:09<00:00,  9.69s/it]\n",
            "python-dateutil-2.8. | 221 KB    | : 100% 1.0/1 [00:00<00:00, 16.95it/s]\n",
            "astunparse-1.6.3     | 17 KB     | : 100% 1.0/1 [00:00<00:00, 11.12it/s]\n",
            "wrapt-1.12.1         | 49 KB     | : 100% 1.0/1 [00:00<00:00, 10.92it/s]\n",
            "gast-0.3.3           | 12 KB     | : 100% 1.0/1 [00:00<00:00, 13.55it/s]\n",
            "cffi-1.14.5          | 224 KB    | : 100% 1.0/1 [00:00<00:00,  5.70it/s]               \n",
            "pycparser-2.20       | 94 KB     | : 100% 1.0/1 [00:00<00:00,  9.12it/s]\n",
            "markupsafe-1.1.1     | 26 KB     | : 100% 1.0/1 [00:00<00:00, 10.65it/s]\n",
            "async_generator-1.10 | 39 KB     | : 100% 1.0/1 [00:00<00:00,  9.19it/s]\n",
            "tk-8.6.10            | 3.0 MB    | : 100% 1.0/1 [00:00<00:00,  5.58it/s]\n",
            "urllib3-1.26.3       | 105 KB    | : 100% 1.0/1 [00:00<00:00, 10.87it/s]\n",
            "zlib-1.2.11          | 103 KB    | : 100% 1.0/1 [00:00<00:00, 10.99it/s]\n",
            "nest-asyncio-1.5.1   | 10 KB     | : 100% 1.0/1 [00:00<00:00, 11.63it/s]\n",
            "traitlets-5.0.5      | 81 KB     | : 100% 1.0/1 [00:00<00:00, 22.74it/s]\n",
            "idna-2.10            | 52 KB     | : 100% 1.0/1 [00:00<00:00, 26.97it/s]\n",
            "jupyter_client-6.1.7 | 76 KB     | : 100% 1.0/1 [00:00<00:00, 17.08it/s]\n",
            "async-timeout-3.0.1  | 13 KB     | : 100% 1.0/1 [00:00<00:00,  6.43it/s]\n",
            "astroid-2.5          | 278 KB    | : 100% 1.0/1 [00:00<00:00,  5.73it/s]\n",
            "pygments-2.7.4       | 676 KB    | : 100% 1.0/1 [00:00<00:00,  8.23it/s]\n",
            "pcre-8.44            | 212 KB    | : 100% 1.0/1 [00:00<00:00, 11.41it/s]\n",
            "six-1.15.0           | 27 KB     | : 100% 1.0/1 [00:00<00:00, 11.93it/s]\n",
            "setuptools-52.0.0    | 710 KB    | : 100% 1.0/1 [00:00<00:00,  7.59it/s]\n",
            "oauthlib-3.1.0       | 91 KB     | : 100% 1.0/1 [00:00<00:00, 10.37it/s]\n",
            "notebook-6.2.0       | 4.1 MB    | : 100% 1.0/1 [00:00<00:00,  1.11it/s]\n",
            "h5py-2.10.0          | 902 KB    | : 100% 1.0/1 [00:00<00:00,  8.13it/s]\n",
            "_libgcc_mutex-0.1    | 2 KB      | : 100% 1.0/1 [00:00<00:00, 12.78it/s]\n",
            "pexpect-4.8.0        | 53 KB     | : 100% 1.0/1 [00:00<00:00, 10.22it/s]\n",
            "lz4-c-1.9.3          | 186 KB    | : 100% 1.0/1 [00:00<00:00, 11.17it/s]\n",
            "prompt-toolkit-3.0.8 | 235 KB    | : 100% 1.0/1 [00:00<00:00, 10.41it/s]\n",
            "opt_einsum-3.1.0     | 48 KB     | : 100% 1.0/1 [00:00<00:00,  7.56it/s]               \n",
            "python-3.7.9         | 45.3 MB   | : 100% 1.0/1 [00:01<00:00,  1.15s/it]\n",
            "isort-5.7.0          | 82 KB     | : 100% 1.0/1 [00:00<00:00,  3.42it/s]               \n",
            "c-ares-1.17.1        | 108 KB    | : 100% 1.0/1 [00:00<00:00, 11.25it/s]\n",
            "attrs-20.3.0         | 43 KB     | : 100% 1.0/1 [00:00<00:00, 11.89it/s]\n",
            "pylint-2.6.0         | 436 KB    | : 100% 1.0/1 [00:00<00:00,  5.57it/s]\n",
            "icu-58.2             | 10.5 MB   | : 100% 1.0/1 [00:00<00:00,  2.04it/s]               \n",
            "pickleshare-0.7.5    | 13 KB     | : 100% 1.0/1 [00:00<00:00, 10.54it/s]\n",
            "send2trash-1.5.0     | 14 KB     | : 100% 1.0/1 [00:00<00:00, 11.05it/s]\n",
            "seaborn-0.11.1       | 212 KB    | : 100% 1.0/1 [00:00<00:00,  9.28it/s]\n",
            "ncurses-6.2          | 817 KB    | : 100% 1.0/1 [00:00<00:00,  3.83it/s]\n",
            "numpy-1.19.2         | 22 KB     | : 100% 1.0/1 [00:00<00:00, 11.07it/s]\n",
            "matplotlib-3.3.4     | 26 KB     | : 100% 1.0/1 [00:00<00:00, 11.19it/s]\n",
            "lcms2-2.11           | 307 KB    | : 100% 1.0/1 [00:00<00:00, 10.42it/s]\n",
            "matplotlib-base-3.3. | 5.1 MB    | : 100% 1.0/1 [00:00<00:00,  3.66it/s]\n",
            "pillow-8.1.0         | 623 KB    | : 100% 1.0/1 [00:00<00:00,  9.20it/s]\n",
            "defusedxml-0.6.0     | 23 KB     | : 100% 1.0/1 [00:00<00:00, 27.96it/s]\n",
            "pandas-1.2.2         | 8.5 MB    | : 100% 1.0/1 [00:00<00:00,  1.01it/s]               \n",
            "parso-0.7.0          | 72 KB     | : 100% 1.0/1 [00:00<00:00,  9.69it/s]\n",
            "pandoc-2.11          | 9.6 MB    | : 100% 1.0/1 [00:00<00:00,  2.09it/s]              \n",
            "packaging-20.9       | 37 KB     | : 100% 1.0/1 [00:00<00:00,  8.54it/s]\n",
            "absl-py-0.11.0       | 103 KB    | : 100% 1.0/1 [00:00<00:00,  5.51it/s]                \n",
            "deprecated-1.2.10    | 11 KB     | : 100% 1.0/1 [00:00<00:00,  3.85it/s]\n",
            "expat-2.2.10         | 153 KB    | : 100% 1.0/1 [00:00<00:00,  9.84it/s]\n",
            "gst-plugins-base-1.1 | 4.9 MB    | : 100% 1.0/1 [00:00<00:00,  4.52it/s]\n",
            "tensorboard-2.3.0    | 5.2 MB    | : 100% 1.0/1 [00:00<00:00,  3.39it/s]\n",
            "python_abi-3.7       | 4 KB      | : 100% 1.0/1 [00:00<00:00, 25.65it/s]\n",
            "pyasn1-modules-0.2.8 | 72 KB     | : 100% 1.0/1 [00:00<00:00, 10.51it/s]\n",
            "google-auth-1.27.0   | 72 KB     | : 100% 1.0/1 [00:00<00:00,  6.13it/s]\n",
            "testpath-0.4.4       | 85 KB     | : 100% 1.0/1 [00:00<00:00, 21.27it/s]\n",
            "olefile-0.46         | 50 KB     | : 100% 1.0/1 [00:00<00:00, 11.42it/s]\n",
            "pytz-2021.1          | 181 KB    | : 100% 1.0/1 [00:00<00:00,  7.57it/s]\n",
            "pyasn1-0.4.8         | 53 KB     | : 100% 1.0/1 [00:00<00:00, 20.25it/s]\n",
            "cryptography-3.3.1   | 572 KB    | : 100% 1.0/1 [00:00<00:00,  7.94it/s]\n",
            "tensorflow-2.2.0     | 4 KB      | : 100% 1.0/1 [00:00<00:00,  6.31it/s]\n",
            "jinja2-2.11.3        | 101 KB    | : 100% 1.0/1 [00:00<00:00, 11.77it/s]\n",
            "pyparsing-2.4.7      | 59 KB     | : 100% 1.0/1 [00:00<00:00, 23.59it/s]\n",
            "libxcb-1.14          | 505 KB    | : 100% 1.0/1 [00:00<00:00,  9.43it/s]\n",
            "libpng-1.6.37        | 278 KB    | : 100% 1.0/1 [00:00<00:00, 11.97it/s]\n",
            "libedit-3.1.20191231 | 116 KB    | : 100% 1.0/1 [00:00<00:00, 11.46it/s]\n",
            "toml-0.10.1          | 20 KB     | : 100% 1.0/1 [00:00<00:00, 10.53it/s]\n",
            "backcall-0.2.0       | 13 KB     | : 100% 1.0/1 [00:00<00:00, 39.37it/s]\n",
            "chardet-3.0.4        | 175 KB    | : 100% 1.0/1 [00:00<00:00, 10.24it/s]\n",
            "pyqt-5.9.2           | 4.5 MB    | : 100% 1.0/1 [00:00<00:00,  3.04it/s]\n",
            "keras-preprocessing- | 35 KB     | : 100% 1.0/1 [00:00<00:00, 12.27it/s]\n",
            "werkzeug-1.0.1       | 239 KB    | : 100% 1.0/1 [00:00<00:00, 13.83it/s]\n",
            "jupyterlab_widgets-1 | 109 KB    | : 100% 1.0/1 [00:00<00:00, 10.09it/s]\n",
            "tornado-6.1          | 589 KB    | : 100% 1.0/1 [00:00<00:00,  9.09it/s]\n",
            "tensorflow-base-2.2. | 181.7 MB  | : 100% 1.0/1 [00:11<00:00, 11.31s/it]               \n",
            "fontconfig-2.13.1    | 250 KB    | : 100% 1.0/1 [00:00<00:00, 10.97it/s]\n",
            "google-pasta-0.2.0   | 46 KB     | : 100% 1.0/1 [00:00<00:00,  6.96it/s]\n",
            "ptyprocess-0.7.0     | 17 KB     | : 100% 1.0/1 [00:00<00:00, 13.79it/s]\n",
            "gstreamer-1.14.0     | 3.2 MB    | : 100% 1.0/1 [00:00<00:00,  5.37it/s]\n",
            "coverage-5.4         | 259 KB    | : 100% 1.0/1 [00:00<00:00,  3.41it/s]                 \n",
            "intel-openmp-2020.2  | 786 KB    | : 100% 1.0/1 [00:00<00:00,  9.55it/s]\n",
            "entrypoints-0.3      | 12 KB     | : 100% 1.0/1 [00:00<00:00, 11.88it/s]\n",
            "jsonschema-3.2.0     | 45 KB     | : 100% 1.0/1 [00:00<00:00, 17.05it/s]\n",
            "pyjwt-1.7.1          | 33 KB     | : 100% 1.0/1 [00:00<00:00, 12.36it/s]\n",
            "nbformat-5.1.2       | 68 KB     | : 100% 1.0/1 [00:00<00:00,  5.42it/s]               \n",
            "kiwisolver-1.3.1     | 80 KB     | : 100% 1.0/1 [00:00<00:00, 12.34it/s]\n",
            "prometheus_client-0. | 45 KB     | : 100% 1.0/1 [00:00<00:00, 11.67it/s]\n",
            "multidict-4.7.6      | 65 KB     | : 100% 1.0/1 [00:00<00:00,  6.06it/s]                \n",
            "jedi-0.17.2          | 918 KB    | : 100% 1.0/1 [00:00<00:00,  1.42it/s]\n",
            "termcolor-1.1.0      | 9 KB      | : 100% 1.0/1 [00:00<00:00, 11.97it/s]\n",
            "Preparing transaction: | \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\bdone\n",
            "Verifying transaction: | \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\bdone\n",
            "Executing transaction: / \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ Enabling notebook extension jupyter-js-widgets/extension...\n",
            "Paths used for configuration of notebook: \n",
            "    \t/usr/local/envs/eval-metric/etc/jupyter/nbconfig/notebook.d/widgetsnbextension.json\n",
            "    \t/usr/local/envs/eval-metric/etc/jupyter/nbconfig/notebook.json\n",
            "Paths used for configuration of notebook: \n",
            "    \t/usr/local/envs/eval-metric/etc/jupyter/nbconfig/notebook.d/widgetsnbextension.json\n",
            "      - Validating: \u001b[32mOK\u001b[0m\n",
            "Paths used for configuration of notebook: \n",
            "    \t/usr/local/envs/eval-metric/etc/jupyter/nbconfig/notebook.d/widgetsnbextension.json\n",
            "    \t/usr/local/envs/eval-metric/etc/jupyter/nbconfig/notebook.json\n",
            "\n",
            "\b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\bdone\n",
            "Installing pip dependencies: \\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ Ran pip subprocess with arguments:\n",
            "['/usr/local/envs/eval-metric/bin/python', '-m', 'pip', 'install', '-U', '-r', '/content/evaluation-local-explanation-time-series/src/condaenv.1ywodn4y.requirements.txt']\n",
            "Pip subprocess output:\n",
            "Collecting cloudpickle==1.6.0\n",
            "  Downloading cloudpickle-1.6.0-py3-none-any.whl (23 kB)\n",
            "Collecting configparser==5.0.1\n",
            "  Downloading configparser-5.0.1-py3-none-any.whl (22 kB)\n",
            "Collecting docker-pycreds==0.4.0\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Collecting gitdb==4.0.5\n",
            "  Downloading gitdb-4.0.5-py3-none-any.whl (63 kB)\n",
            "Collecting gitpython==3.1.13\n",
            "  Downloading GitPython-3.1.13-py3-none-any.whl (159 kB)\n",
            "Collecting gql==0.2.0\n",
            "  Downloading gql-0.2.0.tar.gz (18 kB)\n",
            "Collecting graphql-core==1.1\n",
            "  Downloading graphql-core-1.1.tar.gz (70 kB)\n",
            "Collecting llvmlite==0.35.0\n",
            "  Downloading llvmlite-0.35.0-cp37-cp37m-manylinux2010_x86_64.whl (25.3 MB)\n",
            "Collecting numba==0.52.0\n",
            "  Downloading numba-0.52.0-cp37-cp37m-manylinux2014_x86_64.whl (3.2 MB)\n",
            "Collecting nvidia-ml-py3==7.352.0\n",
            "  Downloading nvidia-ml-py3-7.352.0.tar.gz (19 kB)\n",
            "Collecting pathtools==0.1.2\n",
            "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
            "Collecting promise==2.3\n",
            "  Downloading promise-2.3.tar.gz (19 kB)\n",
            "Collecting psutil==5.8.0\n",
            "  Downloading psutil-5.8.0-cp37-cp37m-manylinux2010_x86_64.whl (296 kB)\n",
            "Collecting pyyaml==5.4.1\n",
            "  Downloading PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636 kB)\n",
            "Collecting sentry-sdk==0.20.3\n",
            "  Downloading sentry_sdk-0.20.3-py2.py3-none-any.whl (131 kB)\n",
            "Collecting shap==0.39.0\n",
            "  Downloading shap-0.39.0.tar.gz (356 kB)\n",
            "Collecting shortuuid==1.0.1\n",
            "  Downloading shortuuid-1.0.1-py3-none-any.whl (7.5 kB)\n",
            "Collecting slicer==0.0.7\n",
            "  Downloading slicer-0.0.7-py3-none-any.whl (14 kB)\n",
            "Collecting smmap==3.0.5\n",
            "  Downloading smmap-3.0.5-py2.py3-none-any.whl (25 kB)\n",
            "Collecting subprocess32==3.5.4\n",
            "  Downloading subprocess32-3.5.4.tar.gz (97 kB)\n",
            "Collecting wandb==0.9.7\n",
            "  Downloading wandb-0.9.7-py2.py3-none-any.whl (1.4 MB)\n",
            "Collecting watchdog==2.0.2\n",
            "  Downloading watchdog-2.0.2-py3-none-manylinux2014_x86_64.whl (74 kB)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/envs/eval-metric/lib/python3.7/site-packages (from docker-pycreds==0.4.0->-r /content/evaluation-local-explanation-time-series/src/condaenv.1ywodn4y.requirements.txt (line 3)) (1.15.0)\n",
            "Requirement already satisfied: requests<3,>=2.12 in /usr/local/envs/eval-metric/lib/python3.7/site-packages (from gql==0.2.0->-r /content/evaluation-local-explanation-time-series/src/condaenv.1ywodn4y.requirements.txt (line 6)) (2.25.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/envs/eval-metric/lib/python3.7/site-packages (from numba==0.52.0->-r /content/evaluation-local-explanation-time-series/src/condaenv.1ywodn4y.requirements.txt (line 9)) (52.0.0.post20210125)\n",
            "Requirement already satisfied: numpy>=1.15 in /usr/local/envs/eval-metric/lib/python3.7/site-packages (from numba==0.52.0->-r /content/evaluation-local-explanation-time-series/src/condaenv.1ywodn4y.requirements.txt (line 9)) (1.19.2)\n",
            "Requirement already satisfied: urllib3>=1.10.0 in /usr/local/envs/eval-metric/lib/python3.7/site-packages (from sentry-sdk==0.20.3->-r /content/evaluation-local-explanation-time-series/src/condaenv.1ywodn4y.requirements.txt (line 15)) (1.26.3)\n",
            "Requirement already satisfied: certifi in /usr/local/envs/eval-metric/lib/python3.7/site-packages (from sentry-sdk==0.20.3->-r /content/evaluation-local-explanation-time-series/src/condaenv.1ywodn4y.requirements.txt (line 15)) (2020.12.5)\n",
            "Requirement already satisfied: scipy in /usr/local/envs/eval-metric/lib/python3.7/site-packages (from shap==0.39.0->-r /content/evaluation-local-explanation-time-series/src/condaenv.1ywodn4y.requirements.txt (line 16)) (1.6.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/envs/eval-metric/lib/python3.7/site-packages (from shap==0.39.0->-r /content/evaluation-local-explanation-time-series/src/condaenv.1ywodn4y.requirements.txt (line 16)) (0.23.2)\n",
            "Requirement already satisfied: pandas in /usr/local/envs/eval-metric/lib/python3.7/site-packages (from shap==0.39.0->-r /content/evaluation-local-explanation-time-series/src/condaenv.1ywodn4y.requirements.txt (line 16)) (1.2.2)\n",
            "Requirement already satisfied: tqdm>4.25.0 in /usr/local/envs/eval-metric/lib/python3.7/site-packages (from shap==0.39.0->-r /content/evaluation-local-explanation-time-series/src/condaenv.1ywodn4y.requirements.txt (line 16)) (4.56.0)\n",
            "Requirement already satisfied: Click>=7.0 in /usr/local/envs/eval-metric/lib/python3.7/site-packages (from wandb==0.9.7->-r /content/evaluation-local-explanation-time-series/src/condaenv.1ywodn4y.requirements.txt (line 21)) (7.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/envs/eval-metric/lib/python3.7/site-packages (from wandb==0.9.7->-r /content/evaluation-local-explanation-time-series/src/condaenv.1ywodn4y.requirements.txt (line 21)) (2.8.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/envs/eval-metric/lib/python3.7/site-packages (from requests<3,>=2.12->gql==0.2.0->-r /content/evaluation-local-explanation-time-series/src/condaenv.1ywodn4y.requirements.txt (line 6)) (2.10)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/envs/eval-metric/lib/python3.7/site-packages (from requests<3,>=2.12->gql==0.2.0->-r /content/evaluation-local-explanation-time-series/src/condaenv.1ywodn4y.requirements.txt (line 6)) (3.0.4)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/envs/eval-metric/lib/python3.7/site-packages (from pandas->shap==0.39.0->-r /content/evaluation-local-explanation-time-series/src/condaenv.1ywodn4y.requirements.txt (line 16)) (2021.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/envs/eval-metric/lib/python3.7/site-packages (from scikit-learn->shap==0.39.0->-r /content/evaluation-local-explanation-time-series/src/condaenv.1ywodn4y.requirements.txt (line 16)) (1.0.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/envs/eval-metric/lib/python3.7/site-packages (from scikit-learn->shap==0.39.0->-r /content/evaluation-local-explanation-time-series/src/condaenv.1ywodn4y.requirements.txt (line 16)) (2.1.0)\n",
            "Building wheels for collected packages: gql, graphql-core, nvidia-ml-py3, pathtools, promise, shap, subprocess32\n",
            "  Building wheel for gql (setup.py): started\n",
            "  Building wheel for gql (setup.py): finished with status 'done'\n",
            "  Created wheel for gql: filename=gql-0.2.0-py3-none-any.whl size=7630 sha256=d25ea6eb580a87a47d0b2fed184ec32321f2d84c095175946152f4f3ff184c38\n",
            "  Stored in directory: /root/.cache/pip/wheels/b6/9a/56/5456fd32264a8fc53eefcb2f74e24e99a7ef4eb40a9af5c905\n",
            "  Building wheel for graphql-core (setup.py): started\n",
            "  Building wheel for graphql-core (setup.py): finished with status 'done'\n",
            "  Created wheel for graphql-core: filename=graphql_core-1.1-py3-none-any.whl size=104649 sha256=12293e68eca7fbabb94005ad2a922adbc6ab72be6ce75ea01097c749b67b68d0\n",
            "  Stored in directory: /root/.cache/pip/wheels/6b/fd/8c/a20dd591c1a554070cc33fb58042867e6ac1c85395abe2e57a\n",
            "  Building wheel for nvidia-ml-py3 (setup.py): started\n",
            "  Building wheel for nvidia-ml-py3 (setup.py): finished with status 'done'\n",
            "  Created wheel for nvidia-ml-py3: filename=nvidia_ml_py3-7.352.0-py3-none-any.whl size=19191 sha256=d7d00128d0522bcc95d1071a3f8b9c23dcfc910f69f4a368d2026dc63c597aff\n",
            "  Stored in directory: /root/.cache/pip/wheels/df/99/da/c34f202dc8fd1dffd35e0ecf1a7d7f8374ca05fbcbaf974b83\n",
            "  Building wheel for pathtools (setup.py): started\n",
            "  Building wheel for pathtools (setup.py): finished with status 'done'\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8784 sha256=1cdaf84b436138fec734d8acf38daaef8a26e8d4b343499917a9e2632bafc0b8\n",
            "  Stored in directory: /root/.cache/pip/wheels/3e/31/09/fa59cef12cdcfecc627b3d24273699f390e71828921b2cbba2\n",
            "  Building wheel for promise (setup.py): started\n",
            "  Building wheel for promise (setup.py): finished with status 'done'\n",
            "  Created wheel for promise: filename=promise-2.3-py3-none-any.whl size=21494 sha256=47b02bd01362d7b15bc6a61a5c602eb3f2221a4375c11b1af52400d47bef2f0d\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/93/c6/762e359f8cb6a5b69c72235d798804cae523bbe41c2aa8333d\n",
            "  Building wheel for shap (setup.py): started\n",
            "  Building wheel for shap (setup.py): finished with status 'done'\n",
            "  Created wheel for shap: filename=shap-0.39.0-cp37-cp37m-linux_x86_64.whl size=494307 sha256=b1ab30192ce2126425001126ed4bed575a5c927ae5662e7a2e3456b3be710876\n",
            "  Stored in directory: /root/.cache/pip/wheels/ca/25/8f/6ae5df62c32651cd719e972e738a8aaa4a87414c4d2b14c9c0\n",
            "  Building wheel for subprocess32 (setup.py): started\n",
            "  Building wheel for subprocess32 (setup.py): finished with status 'done'\n",
            "  Created wheel for subprocess32: filename=subprocess32-3.5.4-py3-none-any.whl size=6488 sha256=a459a61ff76c0a6ea18c46633ab051bd77fa4fa4ebcc8f473109f0743404dd8a\n",
            "  Stored in directory: /root/.cache/pip/wheels/50/ca/fa/8fca8d246e64f19488d07567547ddec8eb084e8c0d7a59226a\n",
            "Successfully built gql graphql-core nvidia-ml-py3 pathtools promise shap subprocess32\n",
            "Installing collected packages: smmap, promise, llvmlite, graphql-core, gitdb, watchdog, subprocess32, slicer, shortuuid, sentry-sdk, pyyaml, psutil, nvidia-ml-py3, numba, gql, gitpython, docker-pycreds, configparser, cloudpickle, wandb, shap, pathtools\n",
            "Successfully installed cloudpickle-1.6.0 configparser-5.0.1 docker-pycreds-0.4.0 gitdb-4.0.5 gitpython-3.1.13 gql-0.2.0 graphql-core-1.1 llvmlite-0.35.0 numba-0.52.0 nvidia-ml-py3-7.352.0 pathtools-0.1.2 promise-2.3 psutil-5.8.0 pyyaml-5.4.1 sentry-sdk-0.20.3 shap-0.39.0 shortuuid-1.0.1 slicer-0.0.7 smmap-3.0.5 subprocess32-3.5.4 wandb-0.9.7 watchdog-2.0.2\n",
            "\n",
            "\b\bdone\n",
            "#\n",
            "# To activate this environment, use\n",
            "#\n",
            "#     $ conda activate eval-metric\n",
            "#\n",
            "# To deactivate an active environment, use\n",
            "#\n",
            "#     $ conda deactivate\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!conda info --envs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yM-SQWHfD_XB",
        "outputId": "bfe13edf-7713-46cb-ab4e-e326c26ededa"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# conda environments:\n",
            "#\n",
            "base                  *  /usr/local\n",
            "eval-metric              /usr/local/envs/eval-metric\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!conda list -n eval-metric"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X0L7C_kOA23a",
        "outputId": "bd822d4a-bc0b-4d8e-a1c2-613832dc891d"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# packages in environment at /usr/local/envs/eval-metric:\n",
            "#\n",
            "# Name                    Version                   Build  Channel\n",
            "_libgcc_mutex             0.1                        main    conda-forge\n",
            "_tflow_select             2.1.0                       gpu  \n",
            "absl-py                   0.11.0             pyhd3eb1b0_1  \n",
            "aiohttp                   3.6.3            py37h7b6447c_0  \n",
            "argon2-cffi               20.1.0           py37h27cfd23_1  \n",
            "astroid                   2.5              py37h06a4308_1  \n",
            "astunparse                1.6.3                      py_0  \n",
            "async-timeout             3.0.1            py37h06a4308_0  \n",
            "async_generator           1.10             py37h28b3542_0  \n",
            "attrs                     20.3.0             pyhd3eb1b0_0  \n",
            "autopep8                  1.5.5              pyhd3eb1b0_0  \n",
            "backcall                  0.2.0              pyhd3eb1b0_0  \n",
            "blas                      1.0                         mkl    conda-forge\n",
            "bleach                    3.3.0              pyhd3eb1b0_0  \n",
            "blinker                   1.4              py37h06a4308_0  \n",
            "brotlipy                  0.7.0           py37h27cfd23_1003  \n",
            "c-ares                    1.17.1               h27cfd23_0  \n",
            "ca-certificates           2021.1.19            h06a4308_1  \n",
            "cachetools                4.2.1              pyhd3eb1b0_0  \n",
            "certifi                   2020.12.5        py37h06a4308_0  \n",
            "cffi                      1.14.5           py37h261ae71_0  \n",
            "chardet                   3.0.4           py37h06a4308_1003  \n",
            "click                     7.1.2              pyhd3eb1b0_0  \n",
            "cloudpickle               1.6.0                    pypi_0    pypi\n",
            "configparser              5.0.1                    pypi_0    pypi\n",
            "coverage                  5.4              py37h27cfd23_2  \n",
            "cryptography              3.3.1            py37h3c74f83_1  \n",
            "cudatoolkit               10.1.243             h6bb024c_0  \n",
            "cudnn                     7.6.5                cuda10.1_0  \n",
            "cupti                     10.1.168                      0  \n",
            "cycler                    0.10.0                   py37_0  \n",
            "cython                    0.29.21          py37h2531618_0  \n",
            "dbus                      1.13.18              hb2f20db_0  \n",
            "decorator                 4.4.2              pyhd3eb1b0_0  \n",
            "defusedxml                0.6.0              pyhd3eb1b0_0  \n",
            "deprecated                1.2.10             pyh9f0ad1d_0    conda-forge\n",
            "docker-pycreds            0.4.0                    pypi_0    pypi\n",
            "entrypoints               0.3                      py37_0  \n",
            "expat                     2.2.10               he6710b0_2  \n",
            "fontconfig                2.13.1               h6c09931_0  \n",
            "freetype                  2.10.4               h5ab3b9f_0  \n",
            "gast                      0.3.3                      py_0    conda-forge\n",
            "gitdb                     4.0.5                    pypi_0    pypi\n",
            "gitpython                 3.1.13                   pypi_0    pypi\n",
            "glib                      2.67.4               h36276a3_1  \n",
            "google-auth               1.27.0             pyhd3eb1b0_0  \n",
            "google-auth-oauthlib      0.4.2              pyhd3eb1b0_2  \n",
            "google-pasta              0.2.0                      py_0  \n",
            "gql                       0.2.0                    pypi_0    pypi\n",
            "graphql-core              1.1                      pypi_0    pypi\n",
            "grpcio                    1.35.0           py37h2157cd5_1  \n",
            "gst-plugins-base          1.14.0               h8213a91_2  \n",
            "gstreamer                 1.14.0               h28cd5cc_2  \n",
            "h5py                      2.10.0           py37hd6299e0_1  \n",
            "hdf5                      1.10.6               hb1b8bf9_0  \n",
            "icu                       58.2                 he6710b0_3  \n",
            "idna                      2.10               pyhd3eb1b0_0  \n",
            "importlib-metadata        2.0.0                      py_1    conda-forge\n",
            "importlib_metadata        2.0.0                         1    conda-forge\n",
            "intel-openmp              2020.2                      254  \n",
            "ipykernel                 5.3.4            py37h5ca1d4c_0  \n",
            "ipython                   7.20.0           py37hb070fc8_1  \n",
            "ipython_genutils          0.2.0              pyhd3eb1b0_1  \n",
            "ipywidgets                7.6.3              pyhd3eb1b0_1  \n",
            "isort                     5.7.0              pyhd3eb1b0_0  \n",
            "jedi                      0.17.2           py37h06a4308_1  \n",
            "jinja2                    2.11.3             pyhd3eb1b0_0  \n",
            "joblib                    1.0.1              pyhd3eb1b0_0  \n",
            "jpeg                      9b                   h024ee3a_2  \n",
            "jsonschema                3.2.0                      py_2    conda-forge\n",
            "jupyter                   1.0.0                    py37_7  \n",
            "jupyter_client            6.1.7                      py_0    conda-forge\n",
            "jupyter_console           6.2.0                      py_0    conda-forge\n",
            "jupyter_core              4.7.1            py37h06a4308_0  \n",
            "jupyterlab_pygments       0.1.2                      py_0  \n",
            "jupyterlab_widgets        1.0.0              pyhd3eb1b0_1  \n",
            "keras-preprocessing       1.1.2              pyhd3eb1b0_0  \n",
            "kiwisolver                1.3.1            py37h2531618_0  \n",
            "lazy-object-proxy         1.5.2            py37h27cfd23_0  \n",
            "lcms2                     2.11                 h396b838_0  \n",
            "ld_impl_linux-64          2.33.1               h53a641e_7    conda-forge\n",
            "libedit                   3.1.20191231         h14c3975_1  \n",
            "libffi                    3.3                  he6710b0_2  \n",
            "libgcc-ng                 9.1.0                hdf63c60_0  \n",
            "libgfortran-ng            7.3.0                hdf63c60_0  \n",
            "libpng                    1.6.37               hbc83047_0  \n",
            "libprotobuf               3.14.0               h8c45485_0  \n",
            "libsodium                 1.0.18               h7b6447c_0  \n",
            "libstdcxx-ng              9.1.0                hdf63c60_0  \n",
            "libtiff                   4.1.0                h2733197_1  \n",
            "libuuid                   1.0.3                h1bed415_2  \n",
            "libxcb                    1.14                 h7b6447c_0  \n",
            "libxml2                   2.9.10               hb55368b_3  \n",
            "llvmlite                  0.35.0                   pypi_0    pypi\n",
            "lz4-c                     1.9.3                h2531618_0  \n",
            "markdown                  3.3.3            py37h06a4308_0  \n",
            "markupsafe                1.1.1            py37h14c3975_1  \n",
            "matplotlib                3.3.4            py37h06a4308_0  \n",
            "matplotlib-base           3.3.4            py37h62a2d02_0  \n",
            "mccabe                    0.6.1                    py37_1  \n",
            "mistune                   0.8.4           py37h14c3975_1001  \n",
            "mkl                       2020.2                      256  \n",
            "mkl-service               2.3.0            py37he8ac12f_0  \n",
            "mkl_fft                   1.2.1            py37h54f3939_0  \n",
            "mkl_random                1.1.1            py37h0573a6f_0  \n",
            "multidict                 4.7.6            py37h7b6447c_1  \n",
            "nbclient                  0.5.2              pyhd3eb1b0_0  \n",
            "nbconvert                 6.0.7                    py37_0  \n",
            "nbformat                  5.1.2              pyhd3eb1b0_1  \n",
            "ncurses                   6.2                  he6710b0_1  \n",
            "nest-asyncio              1.5.1              pyhd3eb1b0_0  \n",
            "notebook                  6.2.0            py37h06a4308_0  \n",
            "numba                     0.52.0                   pypi_0    pypi\n",
            "numpy                     1.19.2           py37h54aff64_0  \n",
            "numpy-base                1.19.2           py37hfa32c7d_0  \n",
            "nvidia-ml-py3             7.352.0                  pypi_0    pypi\n",
            "oauthlib                  3.1.0                      py_0  \n",
            "olefile                   0.46                     py37_0  \n",
            "openssl                   1.1.1j               h27cfd23_0  \n",
            "opt_einsum                3.1.0                      py_0    conda-forge\n",
            "packaging                 20.9               pyhd3eb1b0_0  \n",
            "pandas                    1.2.2            py37ha9443f7_0  \n",
            "pandoc                    2.11                 hb0f4dca_0  \n",
            "pandocfilters             1.4.3            py37h06a4308_1  \n",
            "parso                     0.7.0                      py_0  \n",
            "pathtools                 0.1.2                    pypi_0    pypi\n",
            "pcre                      8.44                 he6710b0_0  \n",
            "pexpect                   4.8.0              pyhd3eb1b0_3  \n",
            "pickleshare               0.7.5           pyhd3eb1b0_1003  \n",
            "pillow                    8.1.0            py37he98fc37_0  \n",
            "pip                       21.0.1           py37h06a4308_0  \n",
            "prometheus_client         0.9.0              pyhd3eb1b0_0  \n",
            "promise                   2.3                      pypi_0    pypi\n",
            "prompt-toolkit            3.0.8                      py_0    conda-forge\n",
            "prompt_toolkit            3.0.8                         0    conda-forge\n",
            "protobuf                  3.14.0           py37h2531618_1  \n",
            "psutil                    5.8.0                    pypi_0    pypi\n",
            "ptyprocess                0.7.0              pyhd3eb1b0_2  \n",
            "pyasn1                    0.4.8                      py_0    conda-forge\n",
            "pyasn1-modules            0.2.8                      py_0  \n",
            "pycodestyle               2.6.0              pyhd3eb1b0_0  \n",
            "pycparser                 2.20                       py_2  \n",
            "pygments                  2.7.4              pyhd3eb1b0_0  \n",
            "pyjwt                     1.7.1                    py37_0  \n",
            "pylint                    2.6.0                    py37_0  \n",
            "pyopenssl                 20.0.1             pyhd3eb1b0_1  \n",
            "pyparsing                 2.4.7              pyhd3eb1b0_0  \n",
            "pyqt                      5.9.2            py37h05f1152_2  \n",
            "pyrsistent                0.17.3           py37h7b6447c_0  \n",
            "pysocks                   1.7.1                    py37_1  \n",
            "python                    3.7.9                h7579374_0  \n",
            "python-dateutil           2.8.1              pyhd3eb1b0_0  \n",
            "python_abi                3.7                     1_cp37m    conda-forge\n",
            "pytz                      2021.1             pyhd3eb1b0_0  \n",
            "pyyaml                    5.4.1                    pypi_0    pypi\n",
            "pyzmq                     20.0.0           py37h2531618_1  \n",
            "qt                        5.9.7                h5867ecd_1  \n",
            "qtconsole                 5.0.2              pyhd3eb1b0_0  \n",
            "qtpy                      1.9.0                      py_0    conda-forge\n",
            "readline                  8.1                  h27cfd23_0  \n",
            "requests                  2.25.1             pyhd3eb1b0_0  \n",
            "requests-oauthlib         1.3.0                      py_0  \n",
            "rsa                       4.7                pyhd3eb1b0_1  \n",
            "scikit-learn              0.23.2           py37h0573a6f_0  \n",
            "scipy                     1.6.0            py37h91f5cce_0  \n",
            "seaborn                   0.11.1             pyhd3eb1b0_0  \n",
            "send2trash                1.5.0              pyhd3eb1b0_1  \n",
            "sentry-sdk                0.20.3                   pypi_0    pypi\n",
            "setuptools                52.0.0           py37h06a4308_0  \n",
            "shap                      0.39.0                   pypi_0    pypi\n",
            "shortuuid                 1.0.1                    pypi_0    pypi\n",
            "sip                       4.19.8           py37hf484d3e_0  \n",
            "six                       1.15.0           py37h06a4308_0  \n",
            "slicer                    0.0.7                    pypi_0    pypi\n",
            "smmap                     3.0.5                    pypi_0    pypi\n",
            "sqlite                    3.33.0               h62c20be_0  \n",
            "subprocess32              3.5.4                    pypi_0    pypi\n",
            "tensorboard               2.3.0              pyh4dce500_0  \n",
            "tensorboard-plugin-wit    1.6.0                      py_0  \n",
            "tensorflow                2.2.0           gpu_py37h1a511ff_0  \n",
            "tensorflow-base           2.2.0           gpu_py37h8a81be8_0  \n",
            "tensorflow-estimator      2.2.0              pyh208ff02_0  \n",
            "tensorflow-gpu            2.2.0                h0d30ee6_0  \n",
            "termcolor                 1.1.0            py37h06a4308_1  \n",
            "terminado                 0.9.2            py37h06a4308_0  \n",
            "testpath                  0.4.4              pyhd3eb1b0_0  \n",
            "threadpoolctl             2.1.0              pyh5ca1d4c_0    conda-forge\n",
            "tk                        8.6.10               hbc83047_0  \n",
            "toml                      0.10.1                     py_0  \n",
            "tornado                   6.1              py37h27cfd23_0  \n",
            "tqdm                      4.56.0             pyhd3eb1b0_0  \n",
            "traitlets                 5.0.5              pyhd3eb1b0_0  \n",
            "typed-ast                 1.4.2            py37h27cfd23_1  \n",
            "urllib3                   1.26.3             pyhd3eb1b0_0  \n",
            "wandb                     0.9.7                    pypi_0    pypi\n",
            "watchdog                  2.0.2                    pypi_0    pypi\n",
            "wcwidth                   0.2.5                      py_0  \n",
            "webencodings              0.5.1                    py37_1  \n",
            "werkzeug                  1.0.1              pyhd3eb1b0_0  \n",
            "wheel                     0.36.2             pyhd3eb1b0_0  \n",
            "widgetsnbextension        3.5.1                    py37_0    conda-forge\n",
            "wrapt                     1.12.1           py37h7b6447c_1  \n",
            "xz                        5.2.5                h7b6447c_0  \n",
            "yarl                      1.6.3            py37h27cfd23_0  \n",
            "zeromq                    4.3.3                he6710b0_3  \n",
            "zipp                      3.4.0              pyhd3eb1b0_0  \n",
            "zlib                      1.2.11               h7b6447c_3  \n",
            "zstd                      1.4.5                h9ceee32_0  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!source activate eval-metric && python raw_data_process.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MhT75466xr_E",
        "outputId": "5cab74c7-26fa-40d0-c676-83dbc2950ab3"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Store           False\n",
            "Dept            False\n",
            "Date            False\n",
            "Weekly_Sales    False\n",
            "year            False\n",
            "month           False\n",
            "weekofmonth     False\n",
            "day             False\n",
            "Type            False\n",
            "Size            False\n",
            "Temperature     False\n",
            "Fuel_Price      False\n",
            "MarkDown1        True\n",
            "MarkDown2        True\n",
            "MarkDown3        True\n",
            "MarkDown4        True\n",
            "MarkDown5        True\n",
            "CPI             False\n",
            "Unemployment    False\n",
            "IsHoliday       False\n",
            "dtype: bool\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!source activate eval-metric && python train_models.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AH2tG7IQFH6d",
        "outputId": "3283249b-cb0d-43b0-e6a8-3f8d134ca5e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "Epoch 74/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.8438 - nd: 0.6705 - val_loss: 0.4964 - val_nd: 0.3941\n",
            "Epoch 75/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.8286 - nd: 0.6567 - val_loss: 0.4836 - val_nd: 0.3944\n",
            "Epoch 76/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.8164 - nd: 0.6397 - val_loss: 0.4638 - val_nd: 0.3686\n",
            "Epoch 77/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.7940 - nd: 0.6167 - val_loss: 0.4402 - val_nd: 0.3648\n",
            "Epoch 78/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.7801 - nd: 0.5984 - val_loss: 0.4128 - val_nd: 0.3338\n",
            "Epoch 79/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.7685 - nd: 0.5862 - val_loss: 0.3975 - val_nd: 0.3285\n",
            "Epoch 80/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.7522 - nd: 0.5579 - val_loss: 0.3782 - val_nd: 0.3029\n",
            "Epoch 81/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.7373 - nd: 0.5560 - val_loss: 0.3560 - val_nd: 0.2955\n",
            "Epoch 82/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.7322 - nd: 0.5501 - val_loss: 0.3452 - val_nd: 0.2891\n",
            "Epoch 83/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.7083 - nd: 0.5342 - val_loss: 0.3330 - val_nd: 0.2933\n",
            "Epoch 84/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.7126 - nd: 0.5243 - val_loss: 0.3267 - val_nd: 0.3140\n",
            "Epoch 85/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.7026 - nd: 0.5299 - val_loss: 0.3194 - val_nd: 0.2862\n",
            "Epoch 86/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.6939 - nd: 0.5263 - val_loss: 0.3008 - val_nd: 0.2629\n",
            "Epoch 87/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.6952 - nd: 0.5147 - val_loss: 0.2889 - val_nd: 0.2591\n",
            "Epoch 88/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.6814 - nd: 0.4926 - val_loss: 0.2871 - val_nd: 0.2581\n",
            "Epoch 89/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.6786 - nd: 0.4832 - val_loss: 0.2647 - val_nd: 0.2280\n",
            "Epoch 90/200\n",
            "18/18 [==============================] - 0s 8ms/step - loss: 0.6659 - nd: 0.4759 - val_loss: 0.2608 - val_nd: 0.2309\n",
            "Epoch 91/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.6759 - nd: 0.4780 - val_loss: 0.2563 - val_nd: 0.2325\n",
            "Epoch 92/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.6611 - nd: 0.4663 - val_loss: 0.2588 - val_nd: 0.2412\n",
            "Epoch 93/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.6645 - nd: 0.4740 - val_loss: 0.2680 - val_nd: 0.2573\n",
            "Epoch 94/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.6500 - nd: 0.4703 - val_loss: 0.3001 - val_nd: 0.2840\n",
            "Epoch 95/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.6595 - nd: 0.4699 - val_loss: 0.2569 - val_nd: 0.2372\n",
            "Epoch 96/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.6466 - nd: 0.4618 - val_loss: 0.2639 - val_nd: 0.2494\n",
            "Epoch 97/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.6547 - nd: 0.4643 - val_loss: 0.2726 - val_nd: 0.2685\n",
            "Epoch 98/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.6491 - nd: 0.4684 - val_loss: 0.2563 - val_nd: 0.2410\n",
            "Epoch 99/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.6490 - nd: 0.4642 - val_loss: 0.2494 - val_nd: 0.2342\n",
            "Epoch 100/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.6579 - nd: 0.4638 - val_loss: 0.2852 - val_nd: 0.3116\n",
            "Epoch 101/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.6430 - nd: 0.4671 - val_loss: 0.2409 - val_nd: 0.2490\n",
            "Epoch 102/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.6409 - nd: 0.4586 - val_loss: 0.2517 - val_nd: 0.2712\n",
            "Epoch 103/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.6311 - nd: 0.4565 - val_loss: 0.2642 - val_nd: 0.2922\n",
            "Epoch 104/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.6401 - nd: 0.4618 - val_loss: 0.2617 - val_nd: 0.2913\n",
            "Epoch 105/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.6390 - nd: 0.4672 - val_loss: 0.2343 - val_nd: 0.2553\n",
            "Epoch 106/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.6337 - nd: 0.4589 - val_loss: 0.2253 - val_nd: 0.2425\n",
            "Epoch 107/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.6406 - nd: 0.4599 - val_loss: 0.2381 - val_nd: 0.2648\n",
            "Epoch 108/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.6236 - nd: 0.4524 - val_loss: 0.2445 - val_nd: 0.2743\n",
            "Epoch 109/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.6284 - nd: 0.4508 - val_loss: 0.2496 - val_nd: 0.2834\n",
            "Epoch 110/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.6456 - nd: 0.4586 - val_loss: 0.2441 - val_nd: 0.2802\n",
            "Epoch 111/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.6226 - nd: 0.4484 - val_loss: 0.2380 - val_nd: 0.2634\n",
            "Epoch 112/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.6330 - nd: 0.4557 - val_loss: 0.2218 - val_nd: 0.2384\n",
            "Epoch 113/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.6277 - nd: 0.4594 - val_loss: 0.2333 - val_nd: 0.2600\n",
            "Epoch 114/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.6164 - nd: 0.4496 - val_loss: 0.2589 - val_nd: 0.2911\n",
            "Epoch 115/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.6351 - nd: 0.4595 - val_loss: 0.2456 - val_nd: 0.2809\n",
            "Epoch 116/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.6197 - nd: 0.4489 - val_loss: 0.2306 - val_nd: 0.2603\n",
            "Epoch 117/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.6158 - nd: 0.4443 - val_loss: 0.2296 - val_nd: 0.2638\n",
            "Epoch 118/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.6115 - nd: 0.4444 - val_loss: 0.2366 - val_nd: 0.2689\n",
            "Epoch 119/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.6267 - nd: 0.4556 - val_loss: 0.2358 - val_nd: 0.2729\n",
            "Epoch 120/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.6180 - nd: 0.4477 - val_loss: 0.2599 - val_nd: 0.2799\n",
            "Epoch 121/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.6270 - nd: 0.4474 - val_loss: 0.2644 - val_nd: 0.2948\n",
            "Epoch 122/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.6125 - nd: 0.4407 - val_loss: 0.2531 - val_nd: 0.2785\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 70376\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                   epoch 121\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                    loss 0.6124555468559265\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                   _step 122\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                val_loss 0.2530970573425293\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                      nd 0.44067275524139404\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              _timestamp 1650174408.5909011\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                _runtime 2832.919161081314\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                  val_nd 0.27854618430137634\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           best_val_loss 0.2217962145805359\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              best_epoch 111\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             best-nd_std 0.033623937256504935\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     best-nrmse_mean_std 0.0528389798611405\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       best-nrmse_maxmin 0.03867679352182303\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   best-nrmse_maxmin_std 0.0055137151232024905\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         best-nrmse_mean 0.3706470625210869\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              best-nd_95 0.0019771863724727203\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      best-nrmse_mean_95 0.003107087374087819\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    best-nrmse_maxmin_95 0.0003242226607826442\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 best-nd 0.2718213377334421\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 7 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced breezy-cherry-327: https://app.wandb.ai/matintavakoli/Interpreting_TS/runs/rn3p8c3w\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.9.7\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Wandb version 0.12.14 is available!  To upgrade, please run:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in wandb/run-20220417_054653-1nrk1bad\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mexpert-smoke-328\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://app.wandb.ai/matintavakoli/Interpreting_TS\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://app.wandb.ai/matintavakoli/Interpreting_TS/runs/1nrk1bad\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb off` to turn off syncing.\n",
            "\n",
            "/usr/local/envs/eval-metric/lib/python3.7/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  return f(**kwargs)\n",
            "/usr/local/envs/eval-metric/lib/python3.7/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  return f(**kwargs)\n",
            "  4%|█▍                                     | 101/2660 [00:00<00:11, 226.72it/s]\n",
            "(9090, 30, 10) (9090, 6) (1111, 30, 10) (1111, 6)\n",
            "(9090, 30, 10) (9090, 6) (1111, 30, 10) (1111, 6)\n",
            "Model: \"sequential_51\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm_39 (LSTM)               (None, 30, 16)            1728      \n",
            "_________________________________________________________________\n",
            "lstm_40 (LSTM)               (None, 30, 16)            2112      \n",
            "_________________________________________________________________\n",
            "lstm_41 (LSTM)               (None, 16)                2112      \n",
            "_________________________________________________________________\n",
            "dropout_67 (Dropout)         (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_93 (Dense)             (None, 6)                 102       \n",
            "=================================================================\n",
            "Total params: 6,054\n",
            "Trainable params: 6,054\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/200\n",
            "18/18 [==============================] - 1s 56ms/step - loss: 0.9871 - nd: 0.9146 - val_loss: 0.8113 - val_nd: 0.9176\n",
            "Epoch 2/200\n",
            "18/18 [==============================] - 0s 11ms/step - loss: 0.9549 - nd: 0.8487 - val_loss: 0.8298 - val_nd: 0.9270\n",
            "Epoch 3/200\n",
            "18/18 [==============================] - 0s 11ms/step - loss: 0.8667 - nd: 0.7481 - val_loss: 0.8918 - val_nd: 0.9686\n",
            "Epoch 4/200\n",
            "18/18 [==============================] - 0s 9ms/step - loss: 0.7879 - nd: 0.6466 - val_loss: 0.8229 - val_nd: 0.8743\n",
            "Epoch 5/200\n",
            "18/18 [==============================] - 0s 12ms/step - loss: 0.7037 - nd: 0.5572 - val_loss: 0.6774 - val_nd: 0.6665\n",
            "Epoch 6/200\n",
            "18/18 [==============================] - 0s 12ms/step - loss: 0.6440 - nd: 0.5182 - val_loss: 0.5843 - val_nd: 0.5688\n",
            "Epoch 7/200\n",
            "18/18 [==============================] - 0s 12ms/step - loss: 0.6258 - nd: 0.4814 - val_loss: 0.5468 - val_nd: 0.5111\n",
            "Epoch 8/200\n",
            "18/18 [==============================] - 0s 12ms/step - loss: 0.5994 - nd: 0.4497 - val_loss: 0.5440 - val_nd: 0.4870\n",
            "Epoch 9/200\n",
            "18/18 [==============================] - 0s 11ms/step - loss: 0.5822 - nd: 0.4356 - val_loss: 0.4873 - val_nd: 0.4409\n",
            "Epoch 10/200\n",
            "18/18 [==============================] - 0s 11ms/step - loss: 0.5584 - nd: 0.4156 - val_loss: 0.4469 - val_nd: 0.4453\n",
            "Epoch 11/200\n",
            "18/18 [==============================] - 0s 9ms/step - loss: 0.5542 - nd: 0.4083 - val_loss: 0.4539 - val_nd: 0.4039\n",
            "Epoch 12/200\n",
            "18/18 [==============================] - 0s 12ms/step - loss: 0.5291 - nd: 0.3877 - val_loss: 0.4462 - val_nd: 0.3898\n",
            "Epoch 13/200\n",
            "18/18 [==============================] - 0s 12ms/step - loss: 0.5101 - nd: 0.3813 - val_loss: 0.3833 - val_nd: 0.3547\n",
            "Epoch 14/200\n",
            "18/18 [==============================] - 0s 12ms/step - loss: 0.5170 - nd: 0.3733 - val_loss: 0.3566 - val_nd: 0.3344\n",
            "Epoch 15/200\n",
            "18/18 [==============================] - 0s 11ms/step - loss: 0.4931 - nd: 0.3579 - val_loss: 0.3428 - val_nd: 0.3428\n",
            "Epoch 16/200\n",
            "18/18 [==============================] - 0s 12ms/step - loss: 0.4910 - nd: 0.3488 - val_loss: 0.3288 - val_nd: 0.3291\n",
            "Epoch 17/200\n",
            "18/18 [==============================] - 0s 14ms/step - loss: 0.4821 - nd: 0.3447 - val_loss: 0.3130 - val_nd: 0.3261\n",
            "Epoch 18/200\n",
            "18/18 [==============================] - 0s 12ms/step - loss: 0.4919 - nd: 0.3402 - val_loss: 0.2756 - val_nd: 0.2864\n",
            "Epoch 19/200\n",
            "18/18 [==============================] - 0s 9ms/step - loss: 0.4885 - nd: 0.3329 - val_loss: 0.2829 - val_nd: 0.2986\n",
            "Epoch 20/200\n",
            "18/18 [==============================] - 0s 12ms/step - loss: 0.4560 - nd: 0.3285 - val_loss: 0.2653 - val_nd: 0.2831\n",
            "Epoch 21/200\n",
            "18/18 [==============================] - 0s 10ms/step - loss: 0.4731 - nd: 0.3241 - val_loss: 0.2859 - val_nd: 0.3089\n",
            "Epoch 22/200\n",
            "18/18 [==============================] - 0s 10ms/step - loss: 0.4597 - nd: 0.3198 - val_loss: 0.2730 - val_nd: 0.2883\n",
            "Epoch 23/200\n",
            "18/18 [==============================] - 0s 9ms/step - loss: 0.4576 - nd: 0.3155 - val_loss: 0.2875 - val_nd: 0.2951\n",
            "Epoch 24/200\n",
            "18/18 [==============================] - 0s 9ms/step - loss: 0.4653 - nd: 0.3127 - val_loss: 0.2684 - val_nd: 0.2809\n",
            "Epoch 25/200\n",
            "18/18 [==============================] - 0s 11ms/step - loss: 0.4669 - nd: 0.3111 - val_loss: 0.2953 - val_nd: 0.2983\n",
            "Epoch 26/200\n",
            "18/18 [==============================] - 0s 11ms/step - loss: 0.4612 - nd: 0.3072 - val_loss: 0.2965 - val_nd: 0.2966\n",
            "Epoch 27/200\n",
            "18/18 [==============================] - 0s 11ms/step - loss: 0.4520 - nd: 0.3005 - val_loss: 0.2986 - val_nd: 0.2970\n",
            "Epoch 28/200\n",
            "18/18 [==============================] - 0s 9ms/step - loss: 0.4572 - nd: 0.3059 - val_loss: 0.3250 - val_nd: 0.3267\n",
            "Epoch 29/200\n",
            "18/18 [==============================] - 0s 10ms/step - loss: 0.4709 - nd: 0.3159 - val_loss: 0.3231 - val_nd: 0.3120\n",
            "Epoch 30/200\n",
            "18/18 [==============================] - 0s 9ms/step - loss: 0.4442 - nd: 0.3033 - val_loss: 0.2877 - val_nd: 0.2845\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 70801\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                   epoch 29\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                    loss 0.44419610500335693\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                   _step 30\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                val_loss 0.2876858711242676\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                      nd 0.30331629514694214\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              _timestamp 1650174427.7989035\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                _runtime 2852.1271634101868\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                  val_nd 0.284494549036026\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           best_val_loss 0.2652623951435089\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              best_epoch 19\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             best-nd_std 0.07434902377260735\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     best-nrmse_mean_std 0.07172863540759736\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       best-nrmse_maxmin 0.045592738240960634\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   best-nrmse_maxmin_std 0.0074848390875995015\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         best-nrmse_mean 0.43692387508209235\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              best-nd_95 0.00437194120035453\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      best-nrmse_mean_95 0.004217854659972305\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    best-nrmse_maxmin_95 0.0004401305454283128\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 best-nd 0.32746796680766793\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 7 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced expert-smoke-328: https://app.wandb.ai/matintavakoli/Interpreting_TS/runs/1nrk1bad\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.9.7\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Wandb version 0.12.14 is available!  To upgrade, please run:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in wandb/run-20220417_054712-1816py9v\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mdivine-waterfall-329\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://app.wandb.ai/matintavakoli/Interpreting_TS\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://app.wandb.ai/matintavakoli/Interpreting_TS/runs/1816py9v\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb off` to turn off syncing.\n",
            "\n",
            "/usr/local/envs/eval-metric/lib/python3.7/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  return f(**kwargs)\n",
            "/usr/local/envs/eval-metric/lib/python3.7/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  return f(**kwargs)\n",
            "  4%|█▍                                     | 101/2660 [00:00<00:11, 215.24it/s]\n",
            "(9090, 30, 10) (9090, 6) (1111, 30, 10) (1111, 6)\n",
            "(9090, 30, 10) (9090, 6) (1111, 30, 10) (1111, 6)\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 70947\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                   _step 2\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                    loss 0.003624900743641276\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                      nd 0.09275466252814012\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              _timestamp 1650174675.4287624\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                _runtime 3099.757022380829\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                val_loss 0.01937608910408121\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                  val_nd 0.0928127405006403\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             best-nd_std 0.009054930014232512\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     best-nrmse_mean_std 0.034002313198329304\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       best-nrmse_maxmin 0.01908872246388406\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   best-nrmse_maxmin_std 0.0035481205162966106\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         best-nrmse_mean 0.18293085502361522\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              best-nd_95 0.000532456508327892\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      best-nrmse_mean_95 0.001999435990360701\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    best-nrmse_maxmin_95 0.0002086399186149844\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 best-nd 0.109134120684586\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 8 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced divine-waterfall-329: https://app.wandb.ai/matintavakoli/Interpreting_TS/runs/1816py9v\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.9.7\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Wandb version 0.12.14 is available!  To upgrade, please run:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in wandb/run-20220417_055119-2uu09p15\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mvibrant-bee-330\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://app.wandb.ai/matintavakoli/Interpreting_TS\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://app.wandb.ai/matintavakoli/Interpreting_TS/runs/2uu09p15\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb off` to turn off syncing.\n",
            "\n",
            "/usr/local/envs/eval-metric/lib/python3.7/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  return f(**kwargs)\n",
            "/usr/local/envs/eval-metric/lib/python3.7/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  return f(**kwargs)\n",
            "  4%|█▍                                     | 101/2660 [00:00<00:12, 210.36it/s]\n",
            "(9090, 30, 10) (9090, 6) (1111, 30, 10) (1111, 6)\n",
            "(9090, 30, 10) (9090, 6) (1111, 30, 10) (1111, 6)\n",
            "Model: \"sequential_52\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "reshape_26 (Reshape)         (None, 300)               0         \n",
            "_________________________________________________________________\n",
            "dense_94 (Dense)             (None, 16)                4816      \n",
            "_________________________________________________________________\n",
            "dropout_68 (Dropout)         (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_95 (Dense)             (None, 16)                272       \n",
            "_________________________________________________________________\n",
            "dropout_69 (Dropout)         (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_96 (Dense)             (None, 16)                272       \n",
            "_________________________________________________________________\n",
            "dropout_70 (Dropout)         (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_97 (Dense)             (None, 6)                 102       \n",
            "=================================================================\n",
            "Total params: 5,462\n",
            "Trainable params: 5,462\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/200\n",
            "18/18 [==============================] - 0s 10ms/step - loss: 1.4414 - nd: 1.6070 - val_loss: 1.0596 - val_nd: 1.1991\n",
            "Epoch 2/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 1.3235 - nd: 1.4314 - val_loss: 0.9662 - val_nd: 1.0808\n",
            "Epoch 3/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 1.2428 - nd: 1.3111 - val_loss: 0.9088 - val_nd: 1.0223\n",
            "Epoch 4/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 1.1867 - nd: 1.2188 - val_loss: 0.8762 - val_nd: 0.9920\n",
            "Epoch 5/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 1.1465 - nd: 1.1580 - val_loss: 0.8570 - val_nd: 0.9730\n",
            "Epoch 6/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 1.1223 - nd: 1.1139 - val_loss: 0.8458 - val_nd: 0.9600\n",
            "Epoch 7/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 1.1014 - nd: 1.0776 - val_loss: 0.8392 - val_nd: 0.9514\n",
            "Epoch 8/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 1.0914 - nd: 1.0504 - val_loss: 0.8351 - val_nd: 0.9452\n",
            "Epoch 9/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 1.0721 - nd: 1.0276 - val_loss: 0.8317 - val_nd: 0.9397\n",
            "Epoch 10/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 1.0646 - nd: 1.0038 - val_loss: 0.8295 - val_nd: 0.9336\n",
            "Epoch 11/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 1.0478 - nd: 0.9890 - val_loss: 0.8282 - val_nd: 0.9307\n",
            "Epoch 12/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 1.0533 - nd: 0.9771 - val_loss: 0.8274 - val_nd: 0.9295\n",
            "Epoch 13/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 1.0453 - nd: 0.9670 - val_loss: 0.8270 - val_nd: 0.9292\n",
            "Epoch 14/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 1.0346 - nd: 0.9582 - val_loss: 0.8261 - val_nd: 0.9265\n",
            "Epoch 15/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 1.0321 - nd: 0.9525 - val_loss: 0.8262 - val_nd: 0.9278\n",
            "Epoch 16/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 1.0349 - nd: 0.9462 - val_loss: 0.8257 - val_nd: 0.9265\n",
            "Epoch 17/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 1.0236 - nd: 0.9405 - val_loss: 0.8253 - val_nd: 0.9251\n",
            "Epoch 18/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 1.0206 - nd: 0.9322 - val_loss: 0.8250 - val_nd: 0.9238\n",
            "Epoch 19/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 1.0205 - nd: 0.9328 - val_loss: 0.8250 - val_nd: 0.9241\n",
            "Epoch 20/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 1.0216 - nd: 0.9281 - val_loss: 0.8248 - val_nd: 0.9235\n",
            "Epoch 21/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 1.0199 - nd: 0.9264 - val_loss: 0.8248 - val_nd: 0.9237\n",
            "Epoch 22/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 1.0191 - nd: 0.9218 - val_loss: 0.8245 - val_nd: 0.9228\n",
            "Epoch 23/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 1.0163 - nd: 0.9211 - val_loss: 0.8243 - val_nd: 0.9220\n",
            "Epoch 24/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 1.0182 - nd: 0.9168 - val_loss: 0.8240 - val_nd: 0.9206\n",
            "Epoch 25/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 1.0135 - nd: 0.9140 - val_loss: 0.8238 - val_nd: 0.9197\n",
            "Epoch 26/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 1.0169 - nd: 0.9131 - val_loss: 0.8235 - val_nd: 0.9183\n",
            "Epoch 27/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 1.0089 - nd: 0.9097 - val_loss: 0.8235 - val_nd: 0.9185\n",
            "Epoch 28/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 1.0125 - nd: 0.9093 - val_loss: 0.8232 - val_nd: 0.9171\n",
            "Epoch 29/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 1.0139 - nd: 0.9061 - val_loss: 0.8231 - val_nd: 0.9170\n",
            "Epoch 30/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 1.0071 - nd: 0.9031 - val_loss: 0.8228 - val_nd: 0.9158\n",
            "Epoch 31/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 1.0120 - nd: 0.9017 - val_loss: 0.8224 - val_nd: 0.9140\n",
            "Epoch 32/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 1.0068 - nd: 0.9009 - val_loss: 0.8223 - val_nd: 0.9146\n",
            "Epoch 33/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 1.0031 - nd: 0.9001 - val_loss: 0.8218 - val_nd: 0.9131\n",
            "Epoch 34/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 1.0110 - nd: 0.8971 - val_loss: 0.8218 - val_nd: 0.9136\n",
            "Epoch 35/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 1.0066 - nd: 0.8951 - val_loss: 0.8214 - val_nd: 0.9128\n",
            "Epoch 36/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 1.0011 - nd: 0.8937 - val_loss: 0.8206 - val_nd: 0.9092\n",
            "Epoch 37/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 1.0058 - nd: 0.8925 - val_loss: 0.8203 - val_nd: 0.9089\n",
            "Epoch 38/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 1.0036 - nd: 0.8923 - val_loss: 0.8193 - val_nd: 0.9080\n",
            "Epoch 39/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.9995 - nd: 0.8892 - val_loss: 0.8182 - val_nd: 0.9046\n",
            "Epoch 40/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.9918 - nd: 0.8868 - val_loss: 0.8177 - val_nd: 0.9036\n",
            "Epoch 41/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.9995 - nd: 0.8848 - val_loss: 0.8167 - val_nd: 0.9014\n",
            "Epoch 42/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 1.0042 - nd: 0.8835 - val_loss: 0.8161 - val_nd: 0.9004\n",
            "Epoch 43/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 1.0003 - nd: 0.8810 - val_loss: 0.8154 - val_nd: 0.8996\n",
            "Epoch 44/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 1.0008 - nd: 0.8800 - val_loss: 0.8137 - val_nd: 0.8951\n",
            "Epoch 45/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 1.0030 - nd: 0.8792 - val_loss: 0.8131 - val_nd: 0.8944\n",
            "Epoch 46/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.9990 - nd: 0.8741 - val_loss: 0.8114 - val_nd: 0.8901\n",
            "Epoch 47/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 1.0042 - nd: 0.8699 - val_loss: 0.8098 - val_nd: 0.8869\n",
            "Epoch 48/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 1.0016 - nd: 0.8706 - val_loss: 0.8086 - val_nd: 0.8850\n",
            "Epoch 49/200\n",
            "18/18 [==============================] - 1s 35ms/step - loss: 0.9926 - nd: 0.8684 - val_loss: 0.8065 - val_nd: 0.8800\n",
            "Epoch 50/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.9965 - nd: 0.8690 - val_loss: 0.8050 - val_nd: 0.8779\n",
            "Epoch 51/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.9968 - nd: 0.8637 - val_loss: 0.8032 - val_nd: 0.8743\n",
            "Epoch 52/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.9919 - nd: 0.8627 - val_loss: 0.8009 - val_nd: 0.8698\n",
            "Epoch 53/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.9927 - nd: 0.8573 - val_loss: 0.7982 - val_nd: 0.8647\n",
            "Epoch 54/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.9944 - nd: 0.8554 - val_loss: 0.7949 - val_nd: 0.8577\n",
            "Epoch 55/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.9897 - nd: 0.8547 - val_loss: 0.7921 - val_nd: 0.8524\n",
            "Epoch 56/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.9926 - nd: 0.8518 - val_loss: 0.7905 - val_nd: 0.8530\n",
            "Epoch 57/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.9845 - nd: 0.8498 - val_loss: 0.7867 - val_nd: 0.8443\n",
            "Epoch 58/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.9847 - nd: 0.8406 - val_loss: 0.7834 - val_nd: 0.8408\n",
            "Epoch 59/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.9830 - nd: 0.8381 - val_loss: 0.7780 - val_nd: 0.8278\n",
            "Epoch 60/200\n",
            "18/18 [==============================] - 0s 8ms/step - loss: 0.9855 - nd: 0.8353 - val_loss: 0.7728 - val_nd: 0.8171\n",
            "Epoch 61/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.9765 - nd: 0.8262 - val_loss: 0.7683 - val_nd: 0.8120\n",
            "Epoch 62/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.9759 - nd: 0.8291 - val_loss: 0.7636 - val_nd: 0.8054\n",
            "Epoch 63/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.9778 - nd: 0.8287 - val_loss: 0.7581 - val_nd: 0.7985\n",
            "Epoch 64/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.9815 - nd: 0.8286 - val_loss: 0.7527 - val_nd: 0.7922\n",
            "Epoch 65/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.9725 - nd: 0.8265 - val_loss: 0.7464 - val_nd: 0.7841\n",
            "Epoch 66/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.9673 - nd: 0.8188 - val_loss: 0.7380 - val_nd: 0.7698\n",
            "Epoch 67/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.9552 - nd: 0.8152 - val_loss: 0.7288 - val_nd: 0.7538\n",
            "Epoch 68/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.9652 - nd: 0.8147 - val_loss: 0.7199 - val_nd: 0.7398\n",
            "Epoch 69/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.9618 - nd: 0.8121 - val_loss: 0.7112 - val_nd: 0.7279\n",
            "Epoch 70/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.9640 - nd: 0.8155 - val_loss: 0.7031 - val_nd: 0.7221\n",
            "Epoch 71/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.9558 - nd: 0.8135 - val_loss: 0.6965 - val_nd: 0.7197\n",
            "Epoch 72/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.9509 - nd: 0.8056 - val_loss: 0.6834 - val_nd: 0.6953\n",
            "Epoch 73/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.9435 - nd: 0.8022 - val_loss: 0.6723 - val_nd: 0.6773\n",
            "Epoch 74/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.9422 - nd: 0.7986 - val_loss: 0.6630 - val_nd: 0.6691\n",
            "Epoch 75/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.9310 - nd: 0.7958 - val_loss: 0.6578 - val_nd: 0.6752\n",
            "Epoch 76/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.9244 - nd: 0.7952 - val_loss: 0.6495 - val_nd: 0.6655\n",
            "Epoch 77/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.9363 - nd: 0.7965 - val_loss: 0.6407 - val_nd: 0.6582\n",
            "Epoch 78/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.9303 - nd: 0.7910 - val_loss: 0.6334 - val_nd: 0.6530\n",
            "Epoch 79/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.9268 - nd: 0.7880 - val_loss: 0.6211 - val_nd: 0.6274\n",
            "Epoch 80/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.9243 - nd: 0.7820 - val_loss: 0.6127 - val_nd: 0.6136\n",
            "Epoch 81/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.9144 - nd: 0.7744 - val_loss: 0.6058 - val_nd: 0.6080\n",
            "Epoch 82/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.9188 - nd: 0.7655 - val_loss: 0.5954 - val_nd: 0.5852\n",
            "Epoch 83/200\n",
            "18/18 [==============================] - 0s 8ms/step - loss: 0.9113 - nd: 0.7624 - val_loss: 0.5933 - val_nd: 0.5971\n",
            "Epoch 84/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.9122 - nd: 0.7703 - val_loss: 0.5863 - val_nd: 0.5884\n",
            "Epoch 85/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.9104 - nd: 0.7652 - val_loss: 0.5816 - val_nd: 0.5890\n",
            "Epoch 86/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.9072 - nd: 0.7578 - val_loss: 0.5710 - val_nd: 0.5727\n",
            "Epoch 87/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.9134 - nd: 0.7728 - val_loss: 0.5713 - val_nd: 0.5866\n",
            "Epoch 88/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.9098 - nd: 0.7597 - val_loss: 0.5637 - val_nd: 0.5737\n",
            "Epoch 89/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.8909 - nd: 0.7532 - val_loss: 0.5553 - val_nd: 0.5660\n",
            "Epoch 90/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.8873 - nd: 0.7523 - val_loss: 0.5472 - val_nd: 0.5558\n",
            "Epoch 91/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.8970 - nd: 0.7444 - val_loss: 0.5446 - val_nd: 0.5607\n",
            "Epoch 92/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.8950 - nd: 0.7450 - val_loss: 0.5393 - val_nd: 0.5526\n",
            "Epoch 93/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.8859 - nd: 0.7447 - val_loss: 0.5360 - val_nd: 0.5543\n",
            "Epoch 94/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.8741 - nd: 0.7410 - val_loss: 0.5285 - val_nd: 0.5482\n",
            "Epoch 95/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.8928 - nd: 0.7411 - val_loss: 0.5195 - val_nd: 0.5394\n",
            "Epoch 96/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.8772 - nd: 0.7331 - val_loss: 0.5076 - val_nd: 0.5247\n",
            "Epoch 97/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.8750 - nd: 0.7345 - val_loss: 0.5086 - val_nd: 0.5408\n",
            "Epoch 98/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.8838 - nd: 0.7372 - val_loss: 0.5074 - val_nd: 0.5338\n",
            "Epoch 99/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.8823 - nd: 0.7363 - val_loss: 0.5043 - val_nd: 0.5400\n",
            "Epoch 100/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.8781 - nd: 0.7332 - val_loss: 0.5036 - val_nd: 0.5504\n",
            "Epoch 101/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.8808 - nd: 0.7383 - val_loss: 0.4941 - val_nd: 0.5300\n",
            "Epoch 102/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.8813 - nd: 0.7306 - val_loss: 0.4914 - val_nd: 0.5328\n",
            "Epoch 103/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.8633 - nd: 0.7272 - val_loss: 0.4890 - val_nd: 0.5360\n",
            "Epoch 104/200\n",
            "18/18 [==============================] - 0s 8ms/step - loss: 0.8718 - nd: 0.7308 - val_loss: 0.4729 - val_nd: 0.4993\n",
            "Epoch 105/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.8742 - nd: 0.7173 - val_loss: 0.4773 - val_nd: 0.4947\n",
            "Epoch 106/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.8617 - nd: 0.7188 - val_loss: 0.4777 - val_nd: 0.5124\n",
            "Epoch 107/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.8593 - nd: 0.7184 - val_loss: 0.4697 - val_nd: 0.5012\n",
            "Epoch 108/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.8692 - nd: 0.7168 - val_loss: 0.4662 - val_nd: 0.5038\n",
            "Epoch 109/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.8641 - nd: 0.7187 - val_loss: 0.4589 - val_nd: 0.4953\n",
            "Epoch 110/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.8675 - nd: 0.7140 - val_loss: 0.4616 - val_nd: 0.5034\n",
            "Epoch 111/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.8663 - nd: 0.7203 - val_loss: 0.4587 - val_nd: 0.5031\n",
            "Epoch 112/200\n",
            "18/18 [==============================] - 0s 8ms/step - loss: 0.8523 - nd: 0.7039 - val_loss: 0.4556 - val_nd: 0.4997\n",
            "Epoch 113/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.8497 - nd: 0.7054 - val_loss: 0.4561 - val_nd: 0.5032\n",
            "Epoch 114/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.8737 - nd: 0.7218 - val_loss: 0.4543 - val_nd: 0.4920\n",
            "Epoch 115/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.8509 - nd: 0.7120 - val_loss: 0.4737 - val_nd: 0.5340\n",
            "Epoch 116/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.8445 - nd: 0.7099 - val_loss: 0.4478 - val_nd: 0.4846\n",
            "Epoch 117/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.8580 - nd: 0.7112 - val_loss: 0.4536 - val_nd: 0.5071\n",
            "Epoch 118/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.8488 - nd: 0.7076 - val_loss: 0.4457 - val_nd: 0.4821\n",
            "Epoch 119/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.8389 - nd: 0.6969 - val_loss: 0.4457 - val_nd: 0.4822\n",
            "Epoch 120/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.8532 - nd: 0.7094 - val_loss: 0.4544 - val_nd: 0.5142\n",
            "Epoch 121/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.8448 - nd: 0.7052 - val_loss: 0.4468 - val_nd: 0.5029\n",
            "Epoch 122/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.8483 - nd: 0.7079 - val_loss: 0.4388 - val_nd: 0.4913\n",
            "Epoch 123/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.8545 - nd: 0.7138 - val_loss: 0.4537 - val_nd: 0.5172\n",
            "Epoch 124/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.8586 - nd: 0.7093 - val_loss: 0.4326 - val_nd: 0.4615\n",
            "Epoch 125/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.8418 - nd: 0.6918 - val_loss: 0.4392 - val_nd: 0.4911\n",
            "Epoch 126/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.8423 - nd: 0.6964 - val_loss: 0.4402 - val_nd: 0.4984\n",
            "Epoch 127/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.8497 - nd: 0.6919 - val_loss: 0.4302 - val_nd: 0.4723\n",
            "Epoch 128/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.8393 - nd: 0.6720 - val_loss: 0.4186 - val_nd: 0.4563\n",
            "Epoch 129/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.8239 - nd: 0.6617 - val_loss: 0.4071 - val_nd: 0.4391\n",
            "Epoch 130/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.8322 - nd: 0.6745 - val_loss: 0.4046 - val_nd: 0.4464\n",
            "Epoch 131/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.8360 - nd: 0.6742 - val_loss: 0.3792 - val_nd: 0.3924\n",
            "Epoch 132/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.8299 - nd: 0.6637 - val_loss: 0.4028 - val_nd: 0.4339\n",
            "Epoch 133/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.8345 - nd: 0.6738 - val_loss: 0.3964 - val_nd: 0.4279\n",
            "Epoch 134/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.8036 - nd: 0.6534 - val_loss: 0.3734 - val_nd: 0.3839\n",
            "Epoch 135/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.8197 - nd: 0.6562 - val_loss: 0.3960 - val_nd: 0.4223\n",
            "Epoch 136/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.8305 - nd: 0.6671 - val_loss: 0.3701 - val_nd: 0.3866\n",
            "Epoch 137/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.8193 - nd: 0.6516 - val_loss: 0.3863 - val_nd: 0.4037\n",
            "Epoch 138/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.7991 - nd: 0.6475 - val_loss: 0.3750 - val_nd: 0.4062\n",
            "Epoch 139/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.8153 - nd: 0.6542 - val_loss: 0.3845 - val_nd: 0.3943\n",
            "Epoch 140/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.8123 - nd: 0.6618 - val_loss: 0.3598 - val_nd: 0.3807\n",
            "Epoch 141/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.7983 - nd: 0.6388 - val_loss: 0.3505 - val_nd: 0.3581\n",
            "Epoch 142/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.7996 - nd: 0.6410 - val_loss: 0.3634 - val_nd: 0.3695\n",
            "Epoch 143/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.7998 - nd: 0.6481 - val_loss: 0.3631 - val_nd: 0.3714\n",
            "Epoch 144/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.8118 - nd: 0.6517 - val_loss: 0.3631 - val_nd: 0.3771\n",
            "Epoch 145/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.8166 - nd: 0.6544 - val_loss: 0.3511 - val_nd: 0.3664\n",
            "Epoch 146/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.8016 - nd: 0.6519 - val_loss: 0.3492 - val_nd: 0.3521\n",
            "Epoch 147/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.8187 - nd: 0.6562 - val_loss: 0.3383 - val_nd: 0.3592\n",
            "Epoch 148/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.7944 - nd: 0.6476 - val_loss: 0.3520 - val_nd: 0.3556\n",
            "Epoch 149/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.7983 - nd: 0.6388 - val_loss: 0.3522 - val_nd: 0.3668\n",
            "Epoch 150/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.7892 - nd: 0.6417 - val_loss: 0.3320 - val_nd: 0.3439\n",
            "Epoch 151/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.7930 - nd: 0.6340 - val_loss: 0.3427 - val_nd: 0.3399\n",
            "Epoch 152/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.8062 - nd: 0.6389 - val_loss: 0.3475 - val_nd: 0.3496\n",
            "Epoch 153/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.8033 - nd: 0.6401 - val_loss: 0.3434 - val_nd: 0.3454\n",
            "Epoch 154/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.8043 - nd: 0.6412 - val_loss: 0.3346 - val_nd: 0.3401\n",
            "Epoch 155/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.7998 - nd: 0.6327 - val_loss: 0.3258 - val_nd: 0.3291\n",
            "Epoch 156/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.8069 - nd: 0.6363 - val_loss: 0.3425 - val_nd: 0.3559\n",
            "Epoch 157/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.7932 - nd: 0.6297 - val_loss: 0.3395 - val_nd: 0.3399\n",
            "Epoch 158/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.7930 - nd: 0.6335 - val_loss: 0.3371 - val_nd: 0.3413\n",
            "Epoch 159/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.7897 - nd: 0.6334 - val_loss: 0.3342 - val_nd: 0.3242\n",
            "Epoch 160/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.7914 - nd: 0.6288 - val_loss: 0.3351 - val_nd: 0.3338\n",
            "Epoch 161/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.7920 - nd: 0.6329 - val_loss: 0.3232 - val_nd: 0.3291\n",
            "Epoch 162/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.7915 - nd: 0.6289 - val_loss: 0.3271 - val_nd: 0.3243\n",
            "Epoch 163/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.7962 - nd: 0.6310 - val_loss: 0.3272 - val_nd: 0.3341\n",
            "Epoch 164/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.7860 - nd: 0.6312 - val_loss: 0.3391 - val_nd: 0.3267\n",
            "Epoch 165/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.7935 - nd: 0.6307 - val_loss: 0.3301 - val_nd: 0.3296\n",
            "Epoch 166/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.7832 - nd: 0.6270 - val_loss: 0.3269 - val_nd: 0.3282\n",
            "Epoch 167/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.7755 - nd: 0.6185 - val_loss: 0.3247 - val_nd: 0.3194\n",
            "Epoch 168/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.7779 - nd: 0.6239 - val_loss: 0.3126 - val_nd: 0.3083\n",
            "Epoch 169/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.8062 - nd: 0.6294 - val_loss: 0.3180 - val_nd: 0.3167\n",
            "Epoch 170/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.7807 - nd: 0.6205 - val_loss: 0.3087 - val_nd: 0.2951\n",
            "Epoch 171/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.7922 - nd: 0.6249 - val_loss: 0.3265 - val_nd: 0.3221\n",
            "Epoch 172/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.8052 - nd: 0.6355 - val_loss: 0.3225 - val_nd: 0.3209\n",
            "Epoch 173/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.7717 - nd: 0.6184 - val_loss: 0.3309 - val_nd: 0.3292\n",
            "Epoch 174/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.7910 - nd: 0.6186 - val_loss: 0.3271 - val_nd: 0.3226\n",
            "Epoch 175/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.7863 - nd: 0.6280 - val_loss: 0.3276 - val_nd: 0.3269\n",
            "Epoch 176/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.7971 - nd: 0.6298 - val_loss: 0.3274 - val_nd: 0.3256\n",
            "Epoch 177/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.7948 - nd: 0.6226 - val_loss: 0.3264 - val_nd: 0.3248\n",
            "Epoch 178/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.7902 - nd: 0.6222 - val_loss: 0.3335 - val_nd: 0.3274\n",
            "Epoch 179/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.7939 - nd: 0.6232 - val_loss: 0.3198 - val_nd: 0.3191\n",
            "Epoch 180/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.7957 - nd: 0.6315 - val_loss: 0.3236 - val_nd: 0.3268\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 71039\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                   epoch 179\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                    loss 0.7957180738449097\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                   _step 180\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                val_loss 0.3236059546470642\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                      nd 0.631544291973114\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              _timestamp 1650174702.4876668\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                _runtime 3126.8159267902374\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                  val_nd 0.32681509852409363\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           best_val_loss 0.3086625635623932\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              best_epoch 169\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             best-nd_std 0.06089470779616663\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     best-nrmse_mean_std 0.23302495433751205\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       best-nrmse_maxmin 0.05150970382028144\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   best-nrmse_maxmin_std 0.02431601098641229\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         best-nrmse_mean 0.49362728069859463\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              best-nd_95 0.0035807878622839207\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      best-nrmse_mean_95 0.013702552459797745\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    best-nrmse_maxmin_95 0.00142985294043547\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 best-nd 0.3246847741586184\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 7 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced vibrant-bee-330: https://app.wandb.ai/matintavakoli/Interpreting_TS/runs/2uu09p15\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.9.7\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Wandb version 0.12.14 is available!  To upgrade, please run:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in wandb/run-20220417_055146-27pw3uox\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mpeach-glade-331\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://app.wandb.ai/matintavakoli/Interpreting_TS\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://app.wandb.ai/matintavakoli/Interpreting_TS/runs/27pw3uox\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb off` to turn off syncing.\n",
            "\n",
            "/usr/local/envs/eval-metric/lib/python3.7/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  return f(**kwargs)\n",
            "/usr/local/envs/eval-metric/lib/python3.7/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  return f(**kwargs)\n",
            "  4%|█▍                                     | 101/2660 [00:00<00:11, 225.83it/s]\n",
            "(9090, 30, 10) (9090, 6) (1111, 30, 10) (1111, 6)\n",
            "(9090, 30, 10) (9090, 6) (1111, 30, 10) (1111, 6)\n",
            "Model: \"sequential_53\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm_42 (LSTM)               (None, 30, 16)            1728      \n",
            "_________________________________________________________________\n",
            "lstm_43 (LSTM)               (None, 30, 16)            2112      \n",
            "_________________________________________________________________\n",
            "lstm_44 (LSTM)               (None, 16)                2112      \n",
            "_________________________________________________________________\n",
            "dropout_71 (Dropout)         (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_98 (Dense)             (None, 6)                 102       \n",
            "=================================================================\n",
            "Total params: 6,054\n",
            "Trainable params: 6,054\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/200\n",
            "18/18 [==============================] - 1s 55ms/step - loss: 1.0116 - nd: 0.9906 - val_loss: 0.8282 - val_nd: 0.9470\n",
            "Epoch 2/200\n",
            "18/18 [==============================] - 0s 11ms/step - loss: 1.0009 - nd: 0.9165 - val_loss: 0.8023 - val_nd: 0.8654\n",
            "Epoch 3/200\n",
            "18/18 [==============================] - 0s 12ms/step - loss: 0.9636 - nd: 0.8559 - val_loss: 0.7738 - val_nd: 0.7889\n",
            "Epoch 4/200\n",
            "18/18 [==============================] - 0s 13ms/step - loss: 0.8692 - nd: 0.7344 - val_loss: 0.7339 - val_nd: 0.7115\n",
            "Epoch 5/200\n",
            "18/18 [==============================] - 0s 11ms/step - loss: 0.7664 - nd: 0.6245 - val_loss: 0.7269 - val_nd: 0.7509\n",
            "Epoch 6/200\n",
            "18/18 [==============================] - 0s 12ms/step - loss: 0.7243 - nd: 0.5792 - val_loss: 0.7011 - val_nd: 0.7051\n",
            "Epoch 7/200\n",
            "18/18 [==============================] - 0s 10ms/step - loss: 0.6950 - nd: 0.5536 - val_loss: 0.7119 - val_nd: 0.6932\n",
            "Epoch 8/200\n",
            "18/18 [==============================] - 0s 10ms/step - loss: 0.6816 - nd: 0.5349 - val_loss: 0.7216 - val_nd: 0.6864\n",
            "Epoch 9/200\n",
            "18/18 [==============================] - 0s 9ms/step - loss: 0.6612 - nd: 0.5208 - val_loss: 0.7125 - val_nd: 0.6985\n",
            "Epoch 10/200\n",
            "18/18 [==============================] - 0s 12ms/step - loss: 0.6441 - nd: 0.4956 - val_loss: 0.6906 - val_nd: 0.6803\n",
            "Epoch 11/200\n",
            "18/18 [==============================] - 0s 12ms/step - loss: 0.6323 - nd: 0.4823 - val_loss: 0.6794 - val_nd: 0.6898\n",
            "Epoch 12/200\n",
            "18/18 [==============================] - 0s 12ms/step - loss: 0.6205 - nd: 0.4701 - val_loss: 0.6454 - val_nd: 0.6597\n",
            "Epoch 13/200\n",
            "18/18 [==============================] - 0s 12ms/step - loss: 0.6012 - nd: 0.4547 - val_loss: 0.6237 - val_nd: 0.6491\n",
            "Epoch 14/200\n",
            "18/18 [==============================] - 0s 12ms/step - loss: 0.5956 - nd: 0.4490 - val_loss: 0.5823 - val_nd: 0.6118\n",
            "Epoch 15/200\n",
            "18/18 [==============================] - 0s 10ms/step - loss: 0.5929 - nd: 0.4364 - val_loss: 0.6208 - val_nd: 0.6243\n",
            "Epoch 16/200\n",
            "18/18 [==============================] - 0s 11ms/step - loss: 0.5784 - nd: 0.4322 - val_loss: 0.5495 - val_nd: 0.5798\n",
            "Epoch 17/200\n",
            "18/18 [==============================] - 0s 12ms/step - loss: 0.5683 - nd: 0.4296 - val_loss: 0.5365 - val_nd: 0.5708\n",
            "Epoch 18/200\n",
            "18/18 [==============================] - 0s 9ms/step - loss: 0.5550 - nd: 0.4208 - val_loss: 0.6190 - val_nd: 0.6335\n",
            "Epoch 19/200\n",
            "18/18 [==============================] - 0s 10ms/step - loss: 0.5505 - nd: 0.4244 - val_loss: 0.5822 - val_nd: 0.6350\n",
            "Epoch 20/200\n",
            "18/18 [==============================] - 0s 10ms/step - loss: 0.5514 - nd: 0.4217 - val_loss: 0.6016 - val_nd: 0.6046\n",
            "Epoch 21/200\n",
            "18/18 [==============================] - 0s 10ms/step - loss: 0.5411 - nd: 0.4010 - val_loss: 0.6300 - val_nd: 0.6377\n",
            "Epoch 22/200\n",
            "18/18 [==============================] - 0s 10ms/step - loss: 0.5598 - nd: 0.4199 - val_loss: 0.6161 - val_nd: 0.6432\n",
            "Epoch 23/200\n",
            "18/18 [==============================] - 0s 11ms/step - loss: 0.5415 - nd: 0.4034 - val_loss: 0.6052 - val_nd: 0.6296\n",
            "Epoch 24/200\n",
            "18/18 [==============================] - 0s 9ms/step - loss: 0.5446 - nd: 0.4084 - val_loss: 0.5507 - val_nd: 0.5855\n",
            "Epoch 25/200\n",
            "18/18 [==============================] - 0s 10ms/step - loss: 0.5518 - nd: 0.4127 - val_loss: 0.5817 - val_nd: 0.6002\n",
            "Epoch 26/200\n",
            "18/18 [==============================] - 0s 10ms/step - loss: 0.5416 - nd: 0.4008 - val_loss: 0.5992 - val_nd: 0.6055\n",
            "Epoch 27/200\n",
            "18/18 [==============================] - 0s 9ms/step - loss: 0.5321 - nd: 0.3957 - val_loss: 0.5966 - val_nd: 0.6180\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 71640\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                   epoch 26\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                    loss 0.5321106314659119\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                   _step 27\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                val_loss 0.5966209173202515\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                      nd 0.39566007256507874\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              _timestamp 1650174720.1227775\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                _runtime 3144.4510374069214\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                  val_nd 0.6180230975151062\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           best_val_loss 0.536462664604187\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              best_epoch 16\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             best-nd_std 0.1441382242228784\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     best-nrmse_mean_std 0.23250756488996766\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       best-nrmse_maxmin 0.09277031314101652\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   best-nrmse_maxmin_std 0.0242620217150636\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         best-nrmse_mean 0.8890355410532602\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              best-nd_95 0.008475751382469593\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      best-nrmse_mean_95 0.013672128439043066\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    best-nrmse_maxmin_95 0.0014266782125398037\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 best-nd 0.7085792944622253\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 7 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced peach-glade-331: https://app.wandb.ai/matintavakoli/Interpreting_TS/runs/27pw3uox\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.9.7\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Wandb version 0.12.14 is available!  To upgrade, please run:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in wandb/run-20220417_055205-3410d5o1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mlyric-capybara-332\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://app.wandb.ai/matintavakoli/Interpreting_TS\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://app.wandb.ai/matintavakoli/Interpreting_TS/runs/3410d5o1\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb off` to turn off syncing.\n",
            "\n",
            "/usr/local/envs/eval-metric/lib/python3.7/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  return f(**kwargs)\n",
            "/usr/local/envs/eval-metric/lib/python3.7/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  return f(**kwargs)\n",
            "  4%|█▍                                     | 101/2660 [00:00<00:11, 224.16it/s]\n",
            "(9090, 30, 10) (9090, 6) (1111, 30, 10) (1111, 6)\n",
            "(9090, 30, 10) (9090, 6) (1111, 30, 10) (1111, 6)\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 71779\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                   _step 2\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                    loss 0.003666146878716739\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                      nd 0.09356251361918454\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              _timestamp 1650174965.2931285\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                _runtime 3389.6213884353638\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                val_loss 0.01937698876431722\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                  val_nd 0.09234824850671366\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             best-nd_std 0.009392466633634109\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     best-nrmse_mean_std 0.03459457257412998\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       best-nrmse_maxmin 0.019076882930691767\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   best-nrmse_maxmin_std 0.0036099224187139626\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         best-nrmse_mean 0.18281739452702914\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              best-nd_95 0.0005523046539808\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      best-nrmse_mean_95 0.0020342625830309497\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    best-nrmse_maxmin_95 0.00021227405218834626\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 best-nd 0.1085277216886806\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 8 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced lyric-capybara-332: https://app.wandb.ai/matintavakoli/Interpreting_TS/runs/3410d5o1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.9.7\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Wandb version 0.12.14 is available!  To upgrade, please run:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in wandb/run-20220417_055608-13zqm40n\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mflowing-haze-333\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://app.wandb.ai/matintavakoli/Interpreting_TS\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://app.wandb.ai/matintavakoli/Interpreting_TS/runs/13zqm40n\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb off` to turn off syncing.\n",
            "\n",
            "/usr/local/envs/eval-metric/lib/python3.7/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  return f(**kwargs)\n",
            "/usr/local/envs/eval-metric/lib/python3.7/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  return f(**kwargs)\n",
            "  4%|█▍                                     | 101/2660 [00:00<00:10, 238.82it/s]\n",
            "(9090, 30, 10) (9090, 6) (1111, 30, 10) (1111, 6)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Error uploading \"config.yaml\": CommError, /tmp/tmp8skuou3bwandb/9yi3aqln-config.yaml is an empty file\n",
            "(9090, 30, 10) (9090, 6) (1111, 30, 10) (1111, 6)\n",
            "Model: \"sequential_54\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "reshape_27 (Reshape)         (None, 300)               0         \n",
            "_________________________________________________________________\n",
            "dense_99 (Dense)             (None, 32)                9632      \n",
            "_________________________________________________________________\n",
            "dropout_72 (Dropout)         (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense_100 (Dense)            (None, 32)                1056      \n",
            "_________________________________________________________________\n",
            "dropout_73 (Dropout)         (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense_101 (Dense)            (None, 32)                1056      \n",
            "_________________________________________________________________\n",
            "dropout_74 (Dropout)         (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense_102 (Dense)            (None, 6)                 198       \n",
            "=================================================================\n",
            "Total params: 11,942\n",
            "Trainable params: 11,942\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/200\n",
            "18/18 [==============================] - 0s 10ms/step - loss: 1.0157 - nd: 0.8941 - val_loss: 0.8209 - val_nd: 0.9141\n",
            "Epoch 2/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.9946 - nd: 0.8847 - val_loss: 0.8098 - val_nd: 0.8997\n",
            "Epoch 3/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.9718 - nd: 0.8579 - val_loss: 0.7883 - val_nd: 0.8652\n",
            "Epoch 4/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.9486 - nd: 0.8224 - val_loss: 0.7501 - val_nd: 0.8236\n",
            "Epoch 5/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.9117 - nd: 0.7714 - val_loss: 0.6793 - val_nd: 0.7363\n",
            "Epoch 6/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.8387 - nd: 0.6703 - val_loss: 0.5697 - val_nd: 0.6043\n",
            "Epoch 7/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.7454 - nd: 0.5362 - val_loss: 0.4338 - val_nd: 0.4086\n",
            "Epoch 8/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.6369 - nd: 0.4035 - val_loss: 0.3234 - val_nd: 0.2821\n",
            "Epoch 9/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.5868 - nd: 0.3287 - val_loss: 0.2713 - val_nd: 0.2499\n",
            "Epoch 10/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.5617 - nd: 0.2966 - val_loss: 0.2536 - val_nd: 0.2404\n",
            "Epoch 11/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.5381 - nd: 0.2719 - val_loss: 0.2045 - val_nd: 0.1768\n",
            "Epoch 12/200\n",
            "18/18 [==============================] - 0s 8ms/step - loss: 0.5087 - nd: 0.2497 - val_loss: 0.1999 - val_nd: 0.1844\n",
            "Epoch 13/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4995 - nd: 0.2557 - val_loss: 0.2130 - val_nd: 0.2083\n",
            "Epoch 14/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.4728 - nd: 0.2412 - val_loss: 0.1735 - val_nd: 0.1653\n",
            "Epoch 15/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.4694 - nd: 0.2240 - val_loss: 0.1683 - val_nd: 0.1644\n",
            "Epoch 16/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.4542 - nd: 0.2191 - val_loss: 0.1449 - val_nd: 0.1401\n",
            "Epoch 17/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4500 - nd: 0.2131 - val_loss: 0.1632 - val_nd: 0.1722\n",
            "Epoch 18/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4298 - nd: 0.2152 - val_loss: 0.1526 - val_nd: 0.1583\n",
            "Epoch 19/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4284 - nd: 0.2074 - val_loss: 0.1393 - val_nd: 0.1375\n",
            "Epoch 20/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.4415 - nd: 0.2054 - val_loss: 0.1353 - val_nd: 0.1392\n",
            "Epoch 21/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4191 - nd: 0.1988 - val_loss: 0.1405 - val_nd: 0.1381\n",
            "Epoch 22/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4340 - nd: 0.1993 - val_loss: 0.1385 - val_nd: 0.1326\n",
            "Epoch 23/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4206 - nd: 0.2055 - val_loss: 0.1670 - val_nd: 0.1665\n",
            "Epoch 24/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4185 - nd: 0.2115 - val_loss: 0.1434 - val_nd: 0.1343\n",
            "Epoch 25/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4196 - nd: 0.1978 - val_loss: 0.1421 - val_nd: 0.1324\n",
            "Epoch 26/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.3940 - nd: 0.1919 - val_loss: 0.1468 - val_nd: 0.1335\n",
            "Epoch 27/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4116 - nd: 0.1886 - val_loss: 0.1470 - val_nd: 0.1298\n",
            "Epoch 28/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.3980 - nd: 0.1901 - val_loss: 0.1457 - val_nd: 0.1268\n",
            "Epoch 29/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4073 - nd: 0.1885 - val_loss: 0.1519 - val_nd: 0.1337\n",
            "Epoch 30/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.4024 - nd: 0.1867 - val_loss: 0.1596 - val_nd: 0.1479\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 71871\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                   epoch 29\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                    loss 0.402353972196579\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                   _step 30\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                val_loss 0.15960656106472015\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                      nd 0.1866627186536789\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              _timestamp 1650174974.5852442\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                _runtime 3398.9135041236877\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                  val_nd 0.14792026579380035\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           best_val_loss 0.13530823588371277\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              best_epoch 19\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             best-nd_std 0.025207285644313585\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     best-nrmse_mean_std 0.02011998641673203\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       best-nrmse_maxmin 0.024049497762135757\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   best-nrmse_maxmin_std 0.002099508235702142\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         best-nrmse_mean 0.23047090746065846\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              best-nd_95 0.0014822625108641028\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      best-nrmse_mean_95 0.0011831143585007355\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    best-nrmse_maxmin_95 0.00012345725727647909\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 best-nd 0.17082828212566747\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 7 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced flowing-haze-333: https://app.wandb.ai/matintavakoli/Interpreting_TS/runs/13zqm40n\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.9.7\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Wandb version 0.12.14 is available!  To upgrade, please run:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in wandb/run-20220417_055619-1bydcc9x\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33meager-universe-334\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://app.wandb.ai/matintavakoli/Interpreting_TS\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://app.wandb.ai/matintavakoli/Interpreting_TS/runs/1bydcc9x\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb off` to turn off syncing.\n",
            "\n",
            "/usr/local/envs/eval-metric/lib/python3.7/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  return f(**kwargs)\n",
            "/usr/local/envs/eval-metric/lib/python3.7/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  return f(**kwargs)\n",
            "  4%|█▍                                     | 101/2660 [00:00<00:11, 228.50it/s]\n",
            "(9090, 30, 10) (9090, 6) (1111, 30, 10) (1111, 6)\n",
            "(9090, 30, 10) (9090, 6) (1111, 30, 10) (1111, 6)\n",
            "Model: \"sequential_55\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm_45 (LSTM)               (None, 30, 32)            5504      \n",
            "_________________________________________________________________\n",
            "lstm_46 (LSTM)               (None, 30, 32)            8320      \n",
            "_________________________________________________________________\n",
            "lstm_47 (LSTM)               (None, 32)                8320      \n",
            "_________________________________________________________________\n",
            "dropout_75 (Dropout)         (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense_103 (Dense)            (None, 6)                 198       \n",
            "=================================================================\n",
            "Total params: 22,342\n",
            "Trainable params: 22,342\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/200\n",
            "18/18 [==============================] - 2s 88ms/step - loss: 0.9697 - nd: 0.8519 - val_loss: 0.7184 - val_nd: 0.7471\n",
            "Epoch 2/200\n",
            "18/18 [==============================] - 0s 15ms/step - loss: 0.8108 - nd: 0.6271 - val_loss: 0.5649 - val_nd: 0.5453\n",
            "Epoch 3/200\n",
            "18/18 [==============================] - 0s 13ms/step - loss: 0.6546 - nd: 0.4760 - val_loss: 0.4272 - val_nd: 0.4501\n",
            "Epoch 4/200\n",
            "18/18 [==============================] - 0s 13ms/step - loss: 0.5488 - nd: 0.3891 - val_loss: 0.2964 - val_nd: 0.3423\n",
            "Epoch 5/200\n",
            "18/18 [==============================] - 0s 12ms/step - loss: 0.5036 - nd: 0.3460 - val_loss: 0.2616 - val_nd: 0.2924\n",
            "Epoch 6/200\n",
            "18/18 [==============================] - 0s 12ms/step - loss: 0.4608 - nd: 0.3184 - val_loss: 0.2406 - val_nd: 0.2681\n",
            "Epoch 7/200\n",
            "18/18 [==============================] - 0s 12ms/step - loss: 0.4459 - nd: 0.2975 - val_loss: 0.2228 - val_nd: 0.2388\n",
            "Epoch 8/200\n",
            "18/18 [==============================] - 0s 10ms/step - loss: 0.4428 - nd: 0.2775 - val_loss: 0.2263 - val_nd: 0.2398\n",
            "Epoch 9/200\n",
            "18/18 [==============================] - 0s 11ms/step - loss: 0.4280 - nd: 0.2618 - val_loss: 0.2238 - val_nd: 0.2237\n",
            "Epoch 10/200\n",
            "18/18 [==============================] - 0s 12ms/step - loss: 0.4198 - nd: 0.2568 - val_loss: 0.2049 - val_nd: 0.1923\n",
            "Epoch 11/200\n",
            "18/18 [==============================] - 0s 10ms/step - loss: 0.4076 - nd: 0.2350 - val_loss: 0.2091 - val_nd: 0.1821\n",
            "Epoch 12/200\n",
            "18/18 [==============================] - 0s 10ms/step - loss: 0.4099 - nd: 0.2389 - val_loss: 0.2074 - val_nd: 0.1780\n",
            "Epoch 13/200\n",
            "18/18 [==============================] - 0s 11ms/step - loss: 0.4210 - nd: 0.2739 - val_loss: 0.2181 - val_nd: 0.2087\n",
            "Epoch 14/200\n",
            "18/18 [==============================] - 0s 9ms/step - loss: 0.4193 - nd: 0.2416 - val_loss: 0.2109 - val_nd: 0.1722\n",
            "Epoch 15/200\n",
            "18/18 [==============================] - 0s 10ms/step - loss: 0.4048 - nd: 0.2203 - val_loss: 0.2065 - val_nd: 0.1672\n",
            "Epoch 16/200\n",
            "18/18 [==============================] - 0s 12ms/step - loss: 0.3928 - nd: 0.2093 - val_loss: 0.2005 - val_nd: 0.1656\n",
            "Epoch 17/200\n",
            "18/18 [==============================] - 0s 10ms/step - loss: 0.3961 - nd: 0.2094 - val_loss: 0.2069 - val_nd: 0.1705\n",
            "Epoch 18/200\n",
            "18/18 [==============================] - 0s 9ms/step - loss: 0.3978 - nd: 0.2118 - val_loss: 0.2172 - val_nd: 0.1990\n",
            "Epoch 19/200\n",
            "18/18 [==============================] - 0s 9ms/step - loss: 0.4013 - nd: 0.2225 - val_loss: 0.2043 - val_nd: 0.1629\n",
            "Epoch 20/200\n",
            "18/18 [==============================] - 0s 12ms/step - loss: 0.3924 - nd: 0.2039 - val_loss: 0.1938 - val_nd: 0.1561\n",
            "Epoch 21/200\n",
            "18/18 [==============================] - 0s 9ms/step - loss: 0.3809 - nd: 0.1938 - val_loss: 0.1960 - val_nd: 0.1616\n",
            "Epoch 22/200\n",
            "18/18 [==============================] - 0s 9ms/step - loss: 0.3995 - nd: 0.1936 - val_loss: 0.1992 - val_nd: 0.1644\n",
            "Epoch 23/200\n",
            "18/18 [==============================] - 0s 9ms/step - loss: 0.3862 - nd: 0.1900 - val_loss: 0.1948 - val_nd: 0.1604\n",
            "Epoch 24/200\n",
            "18/18 [==============================] - 0s 12ms/step - loss: 0.3848 - nd: 0.1851 - val_loss: 0.1911 - val_nd: 0.1580\n",
            "Epoch 25/200\n",
            "18/18 [==============================] - 0s 10ms/step - loss: 0.3914 - nd: 0.1860 - val_loss: 0.1928 - val_nd: 0.1601\n",
            "Epoch 26/200\n",
            "18/18 [==============================] - 0s 9ms/step - loss: 0.3834 - nd: 0.1878 - val_loss: 0.1968 - val_nd: 0.1767\n",
            "Epoch 27/200\n",
            "18/18 [==============================] - 0s 12ms/step - loss: 0.3805 - nd: 0.1852 - val_loss: 0.1874 - val_nd: 0.1556\n",
            "Epoch 28/200\n",
            "18/18 [==============================] - 0s 12ms/step - loss: 0.3842 - nd: 0.1787 - val_loss: 0.1864 - val_nd: 0.1527\n",
            "Epoch 29/200\n",
            "18/18 [==============================] - 0s 10ms/step - loss: 0.3927 - nd: 0.1781 - val_loss: 0.1869 - val_nd: 0.1578\n",
            "Epoch 30/200\n",
            "18/18 [==============================] - 0s 9ms/step - loss: 0.3981 - nd: 0.1906 - val_loss: 0.1924 - val_nd: 0.1705\n",
            "Epoch 31/200\n",
            "18/18 [==============================] - 0s 12ms/step - loss: 0.3998 - nd: 0.1908 - val_loss: 0.1829 - val_nd: 0.1499\n",
            "Epoch 32/200\n",
            "18/18 [==============================] - 0s 9ms/step - loss: 0.3844 - nd: 0.2049 - val_loss: 0.1925 - val_nd: 0.1610\n",
            "Epoch 33/200\n",
            "18/18 [==============================] - 0s 11ms/step - loss: 0.4347 - nd: 0.2252 - val_loss: 0.1624 - val_nd: 0.1708\n",
            "Epoch 34/200\n",
            "18/18 [==============================] - 0s 9ms/step - loss: 0.3978 - nd: 0.2132 - val_loss: 0.1990 - val_nd: 0.1966\n",
            "Epoch 35/200\n",
            "18/18 [==============================] - 0s 10ms/step - loss: 0.4053 - nd: 0.2129 - val_loss: 0.1870 - val_nd: 0.1536\n",
            "Epoch 36/200\n",
            "18/18 [==============================] - 0s 10ms/step - loss: 0.3933 - nd: 0.1856 - val_loss: 0.1887 - val_nd: 0.1527\n",
            "Epoch 37/200\n",
            "18/18 [==============================] - 0s 10ms/step - loss: 0.3830 - nd: 0.1756 - val_loss: 0.1952 - val_nd: 0.1628\n",
            "Epoch 38/200\n",
            "18/18 [==============================] - 0s 10ms/step - loss: 0.3923 - nd: 0.1746 - val_loss: 0.1861 - val_nd: 0.1424\n",
            "Epoch 39/200\n",
            "18/18 [==============================] - 0s 9ms/step - loss: 0.3947 - nd: 0.1842 - val_loss: 0.1895 - val_nd: 0.1471\n",
            "Epoch 40/200\n",
            "18/18 [==============================] - 0s 9ms/step - loss: 0.3806 - nd: 0.1772 - val_loss: 0.1795 - val_nd: 0.1477\n",
            "Epoch 41/200\n",
            "18/18 [==============================] - 0s 10ms/step - loss: 0.3772 - nd: 0.1788 - val_loss: 0.1735 - val_nd: 0.1373\n",
            "Epoch 42/200\n",
            "18/18 [==============================] - 0s 11ms/step - loss: 0.3908 - nd: 0.1769 - val_loss: 0.1761 - val_nd: 0.1320\n",
            "Epoch 43/200\n",
            "18/18 [==============================] - 0s 11ms/step - loss: 0.3734 - nd: 0.1710 - val_loss: 0.1374 - val_nd: 0.1251\n",
            "Epoch 44/200\n",
            "18/18 [==============================] - 0s 11ms/step - loss: 0.3831 - nd: 0.1774 - val_loss: 0.1236 - val_nd: 0.1293\n",
            "Epoch 45/200\n",
            "18/18 [==============================] - 0s 9ms/step - loss: 0.3768 - nd: 0.1735 - val_loss: 0.1281 - val_nd: 0.1339\n",
            "Epoch 46/200\n",
            "18/18 [==============================] - 0s 10ms/step - loss: 0.3636 - nd: 0.1695 - val_loss: 0.1248 - val_nd: 0.1246\n",
            "Epoch 47/200\n",
            "18/18 [==============================] - 0s 10ms/step - loss: 0.3644 - nd: 0.1663 - val_loss: 0.1368 - val_nd: 0.1436\n",
            "Epoch 48/200\n",
            "18/18 [==============================] - 0s 10ms/step - loss: 0.3735 - nd: 0.1837 - val_loss: 0.2026 - val_nd: 0.1700\n",
            "Epoch 49/200\n",
            "18/18 [==============================] - 0s 11ms/step - loss: 0.3701 - nd: 0.1854 - val_loss: 0.1232 - val_nd: 0.1259\n",
            "Epoch 50/200\n",
            "18/18 [==============================] - 0s 11ms/step - loss: 0.3577 - nd: 0.1757 - val_loss: 0.1180 - val_nd: 0.1236\n",
            "Epoch 51/200\n",
            "18/18 [==============================] - 0s 10ms/step - loss: 0.3738 - nd: 0.1734 - val_loss: 0.1241 - val_nd: 0.1283\n",
            "Epoch 52/200\n",
            "18/18 [==============================] - 0s 9ms/step - loss: 0.3649 - nd: 0.1676 - val_loss: 0.1216 - val_nd: 0.1283\n",
            "Epoch 53/200\n",
            "18/18 [==============================] - 0s 10ms/step - loss: 0.3659 - nd: 0.1633 - val_loss: 0.1227 - val_nd: 0.1321\n",
            "Epoch 54/200\n",
            "18/18 [==============================] - 0s 10ms/step - loss: 0.3641 - nd: 0.1628 - val_loss: 0.1352 - val_nd: 0.1536\n",
            "Epoch 55/200\n",
            "18/18 [==============================] - 0s 10ms/step - loss: 0.3638 - nd: 0.1753 - val_loss: 0.1759 - val_nd: 0.2037\n",
            "Epoch 56/200\n",
            "18/18 [==============================] - 0s 10ms/step - loss: 0.3486 - nd: 0.1805 - val_loss: 0.1532 - val_nd: 0.1673\n",
            "Epoch 57/200\n",
            "18/18 [==============================] - 0s 10ms/step - loss: 0.3449 - nd: 0.1742 - val_loss: 0.1378 - val_nd: 0.1373\n",
            "Epoch 58/200\n",
            "18/18 [==============================] - 0s 9ms/step - loss: 0.3771 - nd: 0.1875 - val_loss: 0.1365 - val_nd: 0.1463\n",
            "Epoch 59/200\n",
            "18/18 [==============================] - 0s 12ms/step - loss: 0.3498 - nd: 0.1770 - val_loss: 0.1161 - val_nd: 0.1192\n",
            "Epoch 60/200\n",
            "18/18 [==============================] - 0s 10ms/step - loss: 0.3503 - nd: 0.1609 - val_loss: 0.1183 - val_nd: 0.1216\n",
            "Epoch 61/200\n",
            "18/18 [==============================] - 0s 10ms/step - loss: 0.3581 - nd: 0.1716 - val_loss: 0.1285 - val_nd: 0.1305\n",
            "Epoch 62/200\n",
            "18/18 [==============================] - 0s 11ms/step - loss: 0.3586 - nd: 0.1704 - val_loss: 0.1130 - val_nd: 0.1188\n",
            "Epoch 63/200\n",
            "18/18 [==============================] - 0s 9ms/step - loss: 0.3459 - nd: 0.1610 - val_loss: 0.1219 - val_nd: 0.1284\n",
            "Epoch 64/200\n",
            "18/18 [==============================] - 0s 11ms/step - loss: 0.3285 - nd: 0.1569 - val_loss: 0.1192 - val_nd: 0.1249\n",
            "Epoch 65/200\n",
            "18/18 [==============================] - 0s 9ms/step - loss: 0.3513 - nd: 0.1629 - val_loss: 0.1291 - val_nd: 0.1439\n",
            "Epoch 66/200\n",
            "18/18 [==============================] - 0s 12ms/step - loss: 0.3517 - nd: 0.1613 - val_loss: 0.1115 - val_nd: 0.1191\n",
            "Epoch 67/200\n",
            "18/18 [==============================] - 0s 10ms/step - loss: 0.3699 - nd: 0.1764 - val_loss: 0.1376 - val_nd: 0.1388\n",
            "Epoch 68/200\n",
            "18/18 [==============================] - 0s 9ms/step - loss: 0.3517 - nd: 0.1647 - val_loss: 0.1191 - val_nd: 0.1330\n",
            "Epoch 69/200\n",
            "18/18 [==============================] - 0s 10ms/step - loss: 0.3485 - nd: 0.1591 - val_loss: 0.1191 - val_nd: 0.1240\n",
            "Epoch 70/200\n",
            "18/18 [==============================] - 0s 9ms/step - loss: 0.3462 - nd: 0.1599 - val_loss: 0.1325 - val_nd: 0.1450\n",
            "Epoch 71/200\n",
            "18/18 [==============================] - 0s 10ms/step - loss: 0.3534 - nd: 0.1740 - val_loss: 0.1198 - val_nd: 0.1294\n",
            "Epoch 72/200\n",
            "18/18 [==============================] - 0s 10ms/step - loss: 0.3553 - nd: 0.1598 - val_loss: 0.1162 - val_nd: 0.1240\n",
            "Epoch 73/200\n",
            "18/18 [==============================] - 0s 10ms/step - loss: 0.3636 - nd: 0.1620 - val_loss: 0.1379 - val_nd: 0.1352\n",
            "Epoch 74/200\n",
            "18/18 [==============================] - 0s 9ms/step - loss: 0.3565 - nd: 0.1663 - val_loss: 0.1191 - val_nd: 0.1253\n",
            "Epoch 75/200\n",
            "18/18 [==============================] - 0s 9ms/step - loss: 0.3356 - nd: 0.1601 - val_loss: 0.1153 - val_nd: 0.1196\n",
            "Epoch 76/200\n",
            "18/18 [==============================] - 0s 10ms/step - loss: 0.3543 - nd: 0.1626 - val_loss: 0.1326 - val_nd: 0.1393\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 72019\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                   epoch 75\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                    loss 0.35427066683769226\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                   _step 76\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                val_loss 0.13260552287101746\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                      nd 0.16261965036392212\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              _timestamp 1650175002.0977235\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                _runtime 3426.425983428955\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                  val_nd 0.139287069439888\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           best_val_loss 0.11151241511106491\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              best_epoch 65\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             best-nd_std 0.019287137137736738\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     best-nrmse_mean_std 0.022337948937606062\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       best-nrmse_maxmin 0.019592686118510116\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   best-nrmse_maxmin_std 0.0023309512636746255\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         best-nrmse_mean 0.18776043449997865\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              best-nd_95 0.0011341403721352734\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      best-nrmse_mean_95 0.0013135370760270413\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    best-nrmse_maxmin_95 0.0001370667878148008\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 best-nd 0.140728493171896\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 7 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced eager-universe-334: https://app.wandb.ai/matintavakoli/Interpreting_TS/runs/1bydcc9x\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.9.7\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Wandb version 0.12.14 is available!  To upgrade, please run:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in wandb/run-20220417_055646-1w6n3o3e\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mworldly-thunder-335\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://app.wandb.ai/matintavakoli/Interpreting_TS\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://app.wandb.ai/matintavakoli/Interpreting_TS/runs/1w6n3o3e\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb off` to turn off syncing.\n",
            "\n",
            "/usr/local/envs/eval-metric/lib/python3.7/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  return f(**kwargs)\n",
            "/usr/local/envs/eval-metric/lib/python3.7/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  return f(**kwargs)\n",
            "  4%|█▍                                     | 101/2660 [00:00<00:11, 224.91it/s]\n",
            "(9090, 30, 10) (9090, 6) (1111, 30, 10) (1111, 6)\n",
            "(9090, 30, 10) (9090, 6) (1111, 30, 10) (1111, 6)\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 72305\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                   _step 2\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                    loss 0.0037362807933717327\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                      nd 0.09468122845680943\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              _timestamp 1650175247.2338927\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                _runtime 3671.5621526241302\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                val_loss 0.019115139868401458\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                  val_nd 0.09216676003169734\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             best-nd_std 0.009915857756396702\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     best-nrmse_mean_std 0.03171811955100907\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       best-nrmse_maxmin 0.018869920117915107\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   best-nrmse_maxmin_std 0.0033097663109230317\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         best-nrmse_mean 0.18083403056063707\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              best-nd_95 0.0005830815908845588\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      best-nrmse_mean_95 0.0018651186878652416\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    best-nrmse_maxmin_95 0.00019462399052509266\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 best-nd 0.10853471164238929\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 8 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced worldly-thunder-335: https://app.wandb.ai/matintavakoli/Interpreting_TS/runs/1w6n3o3e\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.9.7\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Wandb version 0.12.14 is available!  To upgrade, please run:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in wandb/run-20220417_060051-1udjeeq8\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33msage-field-336\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://app.wandb.ai/matintavakoli/Interpreting_TS\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://app.wandb.ai/matintavakoli/Interpreting_TS/runs/1udjeeq8\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb off` to turn off syncing.\n",
            "\n",
            "/usr/local/envs/eval-metric/lib/python3.7/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  return f(**kwargs)\n",
            "/usr/local/envs/eval-metric/lib/python3.7/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  return f(**kwargs)\n",
            "  4%|█▍                                     | 101/2660 [00:00<00:10, 253.21it/s]\n",
            "(9090, 30, 10) (9090, 6) (1111, 30, 10) (1111, 6)\n",
            "(9090, 30, 10) (9090, 6) (1111, 30, 10) (1111, 6)\n",
            "Model: \"sequential_56\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "reshape_28 (Reshape)         (None, 300)               0         \n",
            "_________________________________________________________________\n",
            "dense_104 (Dense)            (None, 32)                9632      \n",
            "_________________________________________________________________\n",
            "dropout_76 (Dropout)         (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense_105 (Dense)            (None, 32)                1056      \n",
            "_________________________________________________________________\n",
            "dropout_77 (Dropout)         (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense_106 (Dense)            (None, 32)                1056      \n",
            "_________________________________________________________________\n",
            "dropout_78 (Dropout)         (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense_107 (Dense)            (None, 6)                 198       \n",
            "=================================================================\n",
            "Total params: 11,942\n",
            "Trainable params: 11,942\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/200\n",
            "18/18 [==============================] - 0s 10ms/step - loss: 1.1557 - nd: 1.1266 - val_loss: 0.8603 - val_nd: 0.8986\n",
            "Epoch 2/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 1.0581 - nd: 0.9384 - val_loss: 0.8263 - val_nd: 0.8943\n",
            "Epoch 3/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 1.0378 - nd: 0.9453 - val_loss: 0.8264 - val_nd: 0.9130\n",
            "Epoch 4/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 1.0349 - nd: 0.9342 - val_loss: 0.8244 - val_nd: 0.9062\n",
            "Epoch 5/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 1.0264 - nd: 0.9259 - val_loss: 0.8229 - val_nd: 0.9069\n",
            "Epoch 6/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 1.0216 - nd: 0.9165 - val_loss: 0.8206 - val_nd: 0.9047\n",
            "Epoch 7/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 1.0164 - nd: 0.9082 - val_loss: 0.8168 - val_nd: 0.8974\n",
            "Epoch 8/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 1.0082 - nd: 0.8986 - val_loss: 0.8118 - val_nd: 0.8896\n",
            "Epoch 9/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 1.0057 - nd: 0.8914 - val_loss: 0.8052 - val_nd: 0.8830\n",
            "Epoch 10/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.9894 - nd: 0.8785 - val_loss: 0.7950 - val_nd: 0.8619\n",
            "Epoch 11/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.9903 - nd: 0.8566 - val_loss: 0.7801 - val_nd: 0.8380\n",
            "Epoch 12/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.9767 - nd: 0.8481 - val_loss: 0.7553 - val_nd: 0.7944\n",
            "Epoch 13/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.9458 - nd: 0.7947 - val_loss: 0.7201 - val_nd: 0.7392\n",
            "Epoch 14/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.9242 - nd: 0.7648 - val_loss: 0.6704 - val_nd: 0.6671\n",
            "Epoch 15/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.8960 - nd: 0.7194 - val_loss: 0.6107 - val_nd: 0.5901\n",
            "Epoch 16/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.8465 - nd: 0.6849 - val_loss: 0.5431 - val_nd: 0.5034\n",
            "Epoch 17/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.8123 - nd: 0.6501 - val_loss: 0.4812 - val_nd: 0.4568\n",
            "Epoch 18/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.7763 - nd: 0.6314 - val_loss: 0.4239 - val_nd: 0.3939\n",
            "Epoch 19/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.7350 - nd: 0.5986 - val_loss: 0.3723 - val_nd: 0.3399\n",
            "Epoch 20/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.7226 - nd: 0.5702 - val_loss: 0.3349 - val_nd: 0.3138\n",
            "Epoch 21/200\n",
            "18/18 [==============================] - 0s 8ms/step - loss: 0.6898 - nd: 0.5525 - val_loss: 0.3100 - val_nd: 0.3027\n",
            "Epoch 22/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.6856 - nd: 0.5302 - val_loss: 0.2882 - val_nd: 0.2939\n",
            "Epoch 23/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.6697 - nd: 0.5252 - val_loss: 0.2762 - val_nd: 0.2719\n",
            "Epoch 24/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.6473 - nd: 0.5109 - val_loss: 0.2723 - val_nd: 0.2761\n",
            "Epoch 25/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.6455 - nd: 0.5036 - val_loss: 0.2784 - val_nd: 0.2876\n",
            "Epoch 26/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.6441 - nd: 0.4866 - val_loss: 0.2493 - val_nd: 0.2598\n",
            "Epoch 27/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.6324 - nd: 0.4853 - val_loss: 0.2639 - val_nd: 0.2751\n",
            "Epoch 28/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.6280 - nd: 0.4826 - val_loss: 0.2258 - val_nd: 0.2233\n",
            "Epoch 29/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.6215 - nd: 0.4696 - val_loss: 0.2538 - val_nd: 0.2708\n",
            "Epoch 30/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.6179 - nd: 0.4732 - val_loss: 0.2507 - val_nd: 0.2726\n",
            "Epoch 31/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.6149 - nd: 0.4726 - val_loss: 0.2264 - val_nd: 0.2279\n",
            "Epoch 32/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.6080 - nd: 0.4572 - val_loss: 0.2819 - val_nd: 0.2902\n",
            "Epoch 33/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.6090 - nd: 0.4749 - val_loss: 0.2495 - val_nd: 0.2547\n",
            "Epoch 34/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.6046 - nd: 0.4594 - val_loss: 0.2180 - val_nd: 0.2316\n",
            "Epoch 35/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.5988 - nd: 0.4401 - val_loss: 0.2178 - val_nd: 0.2260\n",
            "Epoch 36/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.5924 - nd: 0.4395 - val_loss: 0.2140 - val_nd: 0.2199\n",
            "Epoch 37/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5906 - nd: 0.4363 - val_loss: 0.2313 - val_nd: 0.2271\n",
            "Epoch 38/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.5837 - nd: 0.4301 - val_loss: 0.2114 - val_nd: 0.2252\n",
            "Epoch 39/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.5889 - nd: 0.4268 - val_loss: 0.2107 - val_nd: 0.2299\n",
            "Epoch 40/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.5842 - nd: 0.4281 - val_loss: 0.1999 - val_nd: 0.2204\n",
            "Epoch 41/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5691 - nd: 0.4208 - val_loss: 0.2018 - val_nd: 0.2272\n",
            "Epoch 42/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.5664 - nd: 0.4194 - val_loss: 0.1958 - val_nd: 0.2253\n",
            "Epoch 43/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.5673 - nd: 0.4145 - val_loss: 0.1780 - val_nd: 0.2017\n",
            "Epoch 44/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.5579 - nd: 0.4060 - val_loss: 0.1851 - val_nd: 0.2094\n",
            "Epoch 45/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.5529 - nd: 0.4061 - val_loss: 0.1760 - val_nd: 0.1993\n",
            "Epoch 46/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.5615 - nd: 0.4080 - val_loss: 0.1849 - val_nd: 0.1950\n",
            "Epoch 47/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.5541 - nd: 0.4036 - val_loss: 0.1660 - val_nd: 0.1801\n",
            "Epoch 48/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.5467 - nd: 0.3936 - val_loss: 0.1580 - val_nd: 0.1853\n",
            "Epoch 49/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5466 - nd: 0.3957 - val_loss: 0.1659 - val_nd: 0.1891\n",
            "Epoch 50/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.5487 - nd: 0.3932 - val_loss: 0.1812 - val_nd: 0.2120\n",
            "Epoch 51/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5340 - nd: 0.3927 - val_loss: 0.1958 - val_nd: 0.2060\n",
            "Epoch 52/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.5367 - nd: 0.3890 - val_loss: 0.1554 - val_nd: 0.1776\n",
            "Epoch 53/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.5367 - nd: 0.3898 - val_loss: 0.1532 - val_nd: 0.1778\n",
            "Epoch 54/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5474 - nd: 0.3909 - val_loss: 0.1656 - val_nd: 0.1773\n",
            "Epoch 55/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.5294 - nd: 0.3824 - val_loss: 0.1570 - val_nd: 0.1861\n",
            "Epoch 56/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5219 - nd: 0.3819 - val_loss: 0.1833 - val_nd: 0.2072\n",
            "Epoch 57/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.5201 - nd: 0.3818 - val_loss: 0.1667 - val_nd: 0.1817\n",
            "Epoch 58/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.5312 - nd: 0.3779 - val_loss: 0.1642 - val_nd: 0.1820\n",
            "Epoch 59/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5259 - nd: 0.3777 - val_loss: 0.1638 - val_nd: 0.1819\n",
            "Epoch 60/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5194 - nd: 0.3776 - val_loss: 0.1763 - val_nd: 0.1970\n",
            "Epoch 61/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.5212 - nd: 0.3729 - val_loss: 0.1644 - val_nd: 0.1802\n",
            "Epoch 62/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5092 - nd: 0.3642 - val_loss: 0.1765 - val_nd: 0.1984\n",
            "Epoch 63/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.5277 - nd: 0.3775 - val_loss: 0.1768 - val_nd: 0.2000\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 72396\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                   epoch 62\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                    loss 0.5276897549629211\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                   _step 63\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                val_loss 0.17683395743370056\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                      nd 0.37748464941978455\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              _timestamp 1650175260.3552146\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                _runtime 3684.6834745407104\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                  val_nd 0.19999468326568604\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           best_val_loss 0.1531984657049179\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              best_epoch 52\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             best-nd_std 0.020841144715901268\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     best-nrmse_mean_std 0.01639989904165287\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       best-nrmse_maxmin 0.026397526053957614\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   best-nrmse_maxmin_std 0.0017113194009912454\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         best-nrmse_mean 0.25297250880434596\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              best-nd_95 0.0012255205868563148\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      best-nrmse_mean_95 0.0009643622829688505\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    best-nrmse_maxmin_95 0.00010063061243470152\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 best-nd 0.19495168124946158\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 7 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced sage-field-336: https://app.wandb.ai/matintavakoli/Interpreting_TS/runs/1udjeeq8\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.9.7\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Wandb version 0.12.14 is available!  To upgrade, please run:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in wandb/run-20220417_060105-1r8litvw\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mprime-feather-337\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://app.wandb.ai/matintavakoli/Interpreting_TS\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://app.wandb.ai/matintavakoli/Interpreting_TS/runs/1r8litvw\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb off` to turn off syncing.\n",
            "\n",
            "/usr/local/envs/eval-metric/lib/python3.7/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  return f(**kwargs)\n",
            "/usr/local/envs/eval-metric/lib/python3.7/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  return f(**kwargs)\n",
            "  4%|█▍                                     | 101/2660 [00:00<00:11, 229.44it/s]\n",
            "(9090, 30, 10) (9090, 6) (1111, 30, 10) (1111, 6)\n",
            "(9090, 30, 10) (9090, 6) (1111, 30, 10) (1111, 6)\n",
            "Model: \"sequential_57\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm_48 (LSTM)               (None, 30, 32)            5504      \n",
            "_________________________________________________________________\n",
            "lstm_49 (LSTM)               (None, 30, 32)            8320      \n",
            "_________________________________________________________________\n",
            "lstm_50 (LSTM)               (None, 32)                8320      \n",
            "_________________________________________________________________\n",
            "dropout_79 (Dropout)         (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense_108 (Dense)            (None, 6)                 198       \n",
            "=================================================================\n",
            "Total params: 22,342\n",
            "Trainable params: 22,342\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/200\n",
            "18/18 [==============================] - 1s 59ms/step - loss: 0.9930 - nd: 0.8957 - val_loss: 0.7934 - val_nd: 0.8521\n",
            "Epoch 2/200\n",
            "18/18 [==============================] - 0s 12ms/step - loss: 0.9392 - nd: 0.8035 - val_loss: 0.7520 - val_nd: 0.7848\n",
            "Epoch 3/200\n",
            "18/18 [==============================] - 0s 13ms/step - loss: 0.7940 - nd: 0.6432 - val_loss: 0.6081 - val_nd: 0.6211\n",
            "Epoch 4/200\n",
            "18/18 [==============================] - 0s 12ms/step - loss: 0.6567 - nd: 0.5172 - val_loss: 0.4580 - val_nd: 0.5106\n",
            "Epoch 5/200\n",
            "18/18 [==============================] - 0s 11ms/step - loss: 0.6089 - nd: 0.4731 - val_loss: 0.4602 - val_nd: 0.4871\n",
            "Epoch 6/200\n",
            "18/18 [==============================] - 0s 13ms/step - loss: 0.5578 - nd: 0.4319 - val_loss: 0.4101 - val_nd: 0.4733\n",
            "Epoch 7/200\n",
            "18/18 [==============================] - 0s 12ms/step - loss: 0.5487 - nd: 0.3967 - val_loss: 0.3381 - val_nd: 0.3844\n",
            "Epoch 8/200\n",
            "18/18 [==============================] - 0s 12ms/step - loss: 0.5057 - nd: 0.3690 - val_loss: 0.3241 - val_nd: 0.3722\n",
            "Epoch 9/200\n",
            "18/18 [==============================] - 0s 11ms/step - loss: 0.5057 - nd: 0.3564 - val_loss: 0.3714 - val_nd: 0.4133\n",
            "Epoch 10/200\n",
            "18/18 [==============================] - 0s 12ms/step - loss: 0.4880 - nd: 0.3447 - val_loss: 0.3144 - val_nd: 0.3659\n",
            "Epoch 11/200\n",
            "18/18 [==============================] - 0s 12ms/step - loss: 0.4695 - nd: 0.3268 - val_loss: 0.2576 - val_nd: 0.2843\n",
            "Epoch 12/200\n",
            "18/18 [==============================] - 0s 11ms/step - loss: 0.4683 - nd: 0.3186 - val_loss: 0.2323 - val_nd: 0.2615\n",
            "Epoch 13/200\n",
            "18/18 [==============================] - 0s 9ms/step - loss: 0.4587 - nd: 0.3109 - val_loss: 0.2606 - val_nd: 0.2707\n",
            "Epoch 14/200\n",
            "18/18 [==============================] - 0s 10ms/step - loss: 0.4468 - nd: 0.3009 - val_loss: 0.2888 - val_nd: 0.3095\n",
            "Epoch 15/200\n",
            "18/18 [==============================] - 0s 8ms/step - loss: 0.4506 - nd: 0.3066 - val_loss: 0.2750 - val_nd: 0.3135\n",
            "Epoch 16/200\n",
            "18/18 [==============================] - 0s 9ms/step - loss: 0.4387 - nd: 0.2983 - val_loss: 0.2395 - val_nd: 0.2553\n",
            "Epoch 17/200\n",
            "18/18 [==============================] - 0s 10ms/step - loss: 0.4400 - nd: 0.2960 - val_loss: 0.2342 - val_nd: 0.2648\n",
            "Epoch 18/200\n",
            "18/18 [==============================] - 0s 9ms/step - loss: 0.4520 - nd: 0.2915 - val_loss: 0.2513 - val_nd: 0.2642\n",
            "Epoch 19/200\n",
            "18/18 [==============================] - 0s 11ms/step - loss: 0.4488 - nd: 0.2879 - val_loss: 0.2262 - val_nd: 0.2626\n",
            "Epoch 20/200\n",
            "18/18 [==============================] - 0s 12ms/step - loss: 0.4414 - nd: 0.2815 - val_loss: 0.2127 - val_nd: 0.2334\n",
            "Epoch 21/200\n",
            "18/18 [==============================] - 0s 10ms/step - loss: 0.4393 - nd: 0.2812 - val_loss: 0.2372 - val_nd: 0.2667\n",
            "Epoch 22/200\n",
            "18/18 [==============================] - 0s 10ms/step - loss: 0.4307 - nd: 0.2755 - val_loss: 0.2160 - val_nd: 0.2495\n",
            "Epoch 23/200\n",
            "18/18 [==============================] - 0s 10ms/step - loss: 0.4258 - nd: 0.2727 - val_loss: 0.2209 - val_nd: 0.2482\n",
            "Epoch 24/200\n",
            "18/18 [==============================] - 0s 9ms/step - loss: 0.4326 - nd: 0.2739 - val_loss: 0.2392 - val_nd: 0.2690\n",
            "Epoch 25/200\n",
            "18/18 [==============================] - 0s 9ms/step - loss: 0.4220 - nd: 0.2727 - val_loss: 0.2205 - val_nd: 0.2550\n",
            "Epoch 26/200\n",
            "18/18 [==============================] - 0s 10ms/step - loss: 0.4245 - nd: 0.2753 - val_loss: 0.2381 - val_nd: 0.2752\n",
            "Epoch 27/200\n",
            "18/18 [==============================] - 0s 10ms/step - loss: 0.4287 - nd: 0.2727 - val_loss: 0.2323 - val_nd: 0.2701\n",
            "Epoch 28/200\n",
            "18/18 [==============================] - 0s 12ms/step - loss: 0.4066 - nd: 0.2612 - val_loss: 0.2040 - val_nd: 0.2197\n",
            "Epoch 29/200\n",
            "18/18 [==============================] - 0s 10ms/step - loss: 0.4213 - nd: 0.2610 - val_loss: 0.2089 - val_nd: 0.2423\n",
            "Epoch 30/200\n",
            "18/18 [==============================] - 0s 10ms/step - loss: 0.4250 - nd: 0.2586 - val_loss: 0.2240 - val_nd: 0.2567\n",
            "Epoch 31/200\n",
            "18/18 [==============================] - 0s 10ms/step - loss: 0.4131 - nd: 0.2543 - val_loss: 0.2050 - val_nd: 0.2330\n",
            "Epoch 32/200\n",
            "18/18 [==============================] - 0s 9ms/step - loss: 0.4165 - nd: 0.2626 - val_loss: 0.2092 - val_nd: 0.2580\n",
            "Epoch 33/200\n",
            "18/18 [==============================] - 0s 12ms/step - loss: 0.4059 - nd: 0.2588 - val_loss: 0.1991 - val_nd: 0.2278\n",
            "Epoch 34/200\n",
            "18/18 [==============================] - 0s 12ms/step - loss: 0.4115 - nd: 0.2548 - val_loss: 0.1811 - val_nd: 0.1977\n",
            "Epoch 35/200\n",
            "18/18 [==============================] - 0s 9ms/step - loss: 0.4050 - nd: 0.2556 - val_loss: 0.2080 - val_nd: 0.2545\n",
            "Epoch 36/200\n",
            "18/18 [==============================] - 0s 10ms/step - loss: 0.4072 - nd: 0.2572 - val_loss: 0.1978 - val_nd: 0.2314\n",
            "Epoch 37/200\n",
            "18/18 [==============================] - 0s 10ms/step - loss: 0.4043 - nd: 0.2506 - val_loss: 0.1864 - val_nd: 0.2016\n",
            "Epoch 38/200\n",
            "18/18 [==============================] - 0s 9ms/step - loss: 0.4217 - nd: 0.2551 - val_loss: 0.2166 - val_nd: 0.2399\n",
            "Epoch 39/200\n",
            "18/18 [==============================] - 0s 13ms/step - loss: 0.4124 - nd: 0.2554 - val_loss: 0.1761 - val_nd: 0.1902\n",
            "Epoch 40/200\n",
            "18/18 [==============================] - 0s 9ms/step - loss: 0.4089 - nd: 0.2544 - val_loss: 0.2194 - val_nd: 0.2580\n",
            "Epoch 41/200\n",
            "18/18 [==============================] - 0s 12ms/step - loss: 0.3942 - nd: 0.2496 - val_loss: 0.1756 - val_nd: 0.1912\n",
            "Epoch 42/200\n",
            "18/18 [==============================] - 0s 9ms/step - loss: 0.4042 - nd: 0.2527 - val_loss: 0.2147 - val_nd: 0.2341\n",
            "Epoch 43/200\n",
            "18/18 [==============================] - 0s 10ms/step - loss: 0.4050 - nd: 0.2478 - val_loss: 0.1868 - val_nd: 0.2125\n",
            "Epoch 44/200\n",
            "18/18 [==============================] - 0s 10ms/step - loss: 0.3930 - nd: 0.2449 - val_loss: 0.1874 - val_nd: 0.2064\n",
            "Epoch 45/200\n",
            "18/18 [==============================] - 0s 10ms/step - loss: 0.4151 - nd: 0.2480 - val_loss: 0.2046 - val_nd: 0.2205\n",
            "Epoch 46/200\n",
            "18/18 [==============================] - 0s 9ms/step - loss: 0.4049 - nd: 0.2485 - val_loss: 0.2054 - val_nd: 0.2256\n",
            "Epoch 47/200\n",
            "18/18 [==============================] - 0s 9ms/step - loss: 0.4053 - nd: 0.2461 - val_loss: 0.1905 - val_nd: 0.2063\n",
            "Epoch 48/200\n",
            "18/18 [==============================] - 0s 13ms/step - loss: 0.4051 - nd: 0.2398 - val_loss: 0.1712 - val_nd: 0.1870\n",
            "Epoch 49/200\n",
            "18/18 [==============================] - 0s 10ms/step - loss: 0.3895 - nd: 0.2413 - val_loss: 0.1738 - val_nd: 0.1832\n",
            "Epoch 50/200\n",
            "18/18 [==============================] - 0s 10ms/step - loss: 0.3971 - nd: 0.2421 - val_loss: 0.1977 - val_nd: 0.2318\n",
            "Epoch 51/200\n",
            "18/18 [==============================] - 0s 9ms/step - loss: 0.3934 - nd: 0.2406 - val_loss: 0.1954 - val_nd: 0.2204\n",
            "Epoch 52/200\n",
            "18/18 [==============================] - 0s 9ms/step - loss: 0.3956 - nd: 0.2417 - val_loss: 0.1717 - val_nd: 0.1924\n",
            "Epoch 53/200\n",
            "18/18 [==============================] - 0s 9ms/step - loss: 0.3978 - nd: 0.2390 - val_loss: 0.1856 - val_nd: 0.2081\n",
            "Epoch 54/200\n",
            "18/18 [==============================] - 0s 10ms/step - loss: 0.3798 - nd: 0.2330 - val_loss: 0.1865 - val_nd: 0.2012\n",
            "Epoch 55/200\n",
            "18/18 [==============================] - 0s 10ms/step - loss: 0.3952 - nd: 0.2405 - val_loss: 0.1905 - val_nd: 0.2041\n",
            "Epoch 56/200\n",
            "18/18 [==============================] - 0s 9ms/step - loss: 0.4109 - nd: 0.2355 - val_loss: 0.1980 - val_nd: 0.2098\n",
            "Epoch 57/200\n",
            "18/18 [==============================] - 0s 9ms/step - loss: 0.3795 - nd: 0.2359 - val_loss: 0.2137 - val_nd: 0.2456\n",
            "Epoch 58/200\n",
            "18/18 [==============================] - 0s 9ms/step - loss: 0.3964 - nd: 0.2392 - val_loss: 0.2096 - val_nd: 0.2334\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 72643\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                   epoch 57\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                    loss 0.39643070101737976\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                   _step 58\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                val_loss 0.2095780372619629\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                      nd 0.23917418718338013\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              _timestamp 1650175286.3883123\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                _runtime 3710.7165722846985\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                  val_nd 0.23342563211917877\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           best_val_loss 0.17115800082683563\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              best_epoch 47\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             best-nd_std 0.038756799529400685\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     best-nrmse_mean_std 0.06894650036258458\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       best-nrmse_maxmin 0.030108489932254817\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   best-nrmse_maxmin_std 0.0071945250029447376\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         best-nrmse_mean 0.28853538088774733\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              best-nd_95 0.002279013765865963\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      best-nrmse_mean_95 0.004054256939235009\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    best-nrmse_maxmin_95 0.0004230592236631842\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 best-nd 0.2236859345633656\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 7 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced prime-feather-337: https://app.wandb.ai/matintavakoli/Interpreting_TS/runs/1r8litvw\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.9.7\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Wandb version 0.12.14 is available!  To upgrade, please run:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in wandb/run-20220417_060130-2cykc4f6\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mdevout-puddle-338\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://app.wandb.ai/matintavakoli/Interpreting_TS\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://app.wandb.ai/matintavakoli/Interpreting_TS/runs/2cykc4f6\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb off` to turn off syncing.\n",
            "\n",
            "/usr/local/envs/eval-metric/lib/python3.7/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  return f(**kwargs)\n",
            "/usr/local/envs/eval-metric/lib/python3.7/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  return f(**kwargs)\n",
            "  4%|█▍                                     | 101/2660 [00:00<00:10, 241.40it/s]\n",
            "(9090, 30, 10) (9090, 6) (1111, 30, 10) (1111, 6)\n",
            "(9090, 30, 10) (9090, 6) (1111, 30, 10) (1111, 6)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Error uploading \"config.yaml\": CommError, /tmp/tmp6vq8r6yiwandb/166i365y-config.yaml is an empty file\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 72876\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                   _step 2\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                    loss 0.002720379702567755\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                      nd 0.07544690359797104\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              _timestamp 1650175595.6256926\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                _runtime 4019.953952550888\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                val_loss 0.019830662184851765\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                  val_nd 0.09166507071653048\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             best-nd_std 0.008426741671778202\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     best-nrmse_mean_std 0.0396192286823119\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       best-nrmse_maxmin 0.019181816037456195\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   best-nrmse_maxmin_std 0.004134242200159046\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         best-nrmse_mean 0.1838229884308129\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              best-nd_95 0.0004955171867793247\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      best-nrmse_mean_95 0.002329727135789034\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    best-nrmse_maxmin_95 0.00024310559695309676\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 best-nd 0.10713432480409568\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 8 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced devout-puddle-338: https://app.wandb.ai/matintavakoli/Interpreting_TS/runs/2cykc4f6\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.9.7\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Wandb version 0.12.14 is available!  To upgrade, please run:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in wandb/run-20220417_060639-39xf3uze\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mplayful-cloud-339\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://app.wandb.ai/matintavakoli/Interpreting_TS\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://app.wandb.ai/matintavakoli/Interpreting_TS/runs/39xf3uze\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb off` to turn off syncing.\n",
            "\n",
            "/usr/local/envs/eval-metric/lib/python3.7/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  return f(**kwargs)\n",
            "/usr/local/envs/eval-metric/lib/python3.7/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  return f(**kwargs)\n",
            "  4%|█▍                                     | 101/2660 [00:00<00:10, 242.84it/s]\n",
            "(9090, 30, 10) (9090, 6) (1111, 30, 10) (1111, 6)\n",
            "(9090, 30, 10) (9090, 6) (1111, 30, 10) (1111, 6)\n",
            "Model: \"sequential_58\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "reshape_29 (Reshape)         (None, 300)               0         \n",
            "_________________________________________________________________\n",
            "dense_109 (Dense)            (None, 32)                9632      \n",
            "_________________________________________________________________\n",
            "dropout_80 (Dropout)         (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense_110 (Dense)            (None, 32)                1056      \n",
            "_________________________________________________________________\n",
            "dropout_81 (Dropout)         (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense_111 (Dense)            (None, 32)                1056      \n",
            "_________________________________________________________________\n",
            "dropout_82 (Dropout)         (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense_112 (Dense)            (None, 6)                 198       \n",
            "=================================================================\n",
            "Total params: 11,942\n",
            "Trainable params: 11,942\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/200\n",
            "18/18 [==============================] - 0s 10ms/step - loss: 1.1697 - nd: 1.1974 - val_loss: 0.8278 - val_nd: 0.9270\n",
            "Epoch 2/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 1.1142 - nd: 1.0838 - val_loss: 0.8235 - val_nd: 0.9203\n",
            "Epoch 3/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 1.0761 - nd: 1.0269 - val_loss: 0.8207 - val_nd: 0.9138\n",
            "Epoch 4/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 1.0558 - nd: 0.9861 - val_loss: 0.8192 - val_nd: 0.9137\n",
            "Epoch 5/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 1.0393 - nd: 0.9591 - val_loss: 0.8172 - val_nd: 0.9113\n",
            "Epoch 6/200\n",
            "18/18 [==============================] - 0s 8ms/step - loss: 1.0150 - nd: 0.9380 - val_loss: 0.8136 - val_nd: 0.9051\n",
            "Epoch 7/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 1.0099 - nd: 0.9216 - val_loss: 0.8089 - val_nd: 0.8959\n",
            "Epoch 8/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 1.0120 - nd: 0.9121 - val_loss: 0.8025 - val_nd: 0.8894\n",
            "Epoch 9/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.9998 - nd: 0.8901 - val_loss: 0.7920 - val_nd: 0.8713\n",
            "Epoch 10/200\n",
            "18/18 [==============================] - 0s 8ms/step - loss: 0.9917 - nd: 0.8794 - val_loss: 0.7774 - val_nd: 0.8477\n",
            "Epoch 11/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.9795 - nd: 0.8620 - val_loss: 0.7559 - val_nd: 0.8143\n",
            "Epoch 12/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.9553 - nd: 0.8440 - val_loss: 0.7234 - val_nd: 0.7643\n",
            "Epoch 13/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.9442 - nd: 0.8136 - val_loss: 0.6810 - val_nd: 0.6900\n",
            "Epoch 14/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.9231 - nd: 0.7833 - val_loss: 0.6283 - val_nd: 0.6117\n",
            "Epoch 15/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.8974 - nd: 0.7611 - val_loss: 0.5760 - val_nd: 0.5378\n",
            "Epoch 16/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.8712 - nd: 0.7362 - val_loss: 0.5265 - val_nd: 0.5013\n",
            "Epoch 17/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.8420 - nd: 0.7125 - val_loss: 0.4658 - val_nd: 0.4279\n",
            "Epoch 18/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.8189 - nd: 0.6875 - val_loss: 0.4189 - val_nd: 0.3904\n",
            "Epoch 19/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.7929 - nd: 0.6668 - val_loss: 0.3759 - val_nd: 0.3548\n",
            "Epoch 20/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.7711 - nd: 0.6424 - val_loss: 0.3629 - val_nd: 0.3519\n",
            "Epoch 21/200\n",
            "18/18 [==============================] - 0s 8ms/step - loss: 0.7468 - nd: 0.6138 - val_loss: 0.3324 - val_nd: 0.3381\n",
            "Epoch 22/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.7517 - nd: 0.6114 - val_loss: 0.3144 - val_nd: 0.3381\n",
            "Epoch 23/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.7251 - nd: 0.5820 - val_loss: 0.3052 - val_nd: 0.3187\n",
            "Epoch 24/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.7090 - nd: 0.5803 - val_loss: 0.2795 - val_nd: 0.2840\n",
            "Epoch 25/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.7105 - nd: 0.5776 - val_loss: 0.2910 - val_nd: 0.3166\n",
            "Epoch 26/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.7018 - nd: 0.5650 - val_loss: 0.2576 - val_nd: 0.2650\n",
            "Epoch 27/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.6895 - nd: 0.5580 - val_loss: 0.2679 - val_nd: 0.2872\n",
            "Epoch 28/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.6998 - nd: 0.5534 - val_loss: 0.2950 - val_nd: 0.3305\n",
            "Epoch 29/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.6858 - nd: 0.5432 - val_loss: 0.2506 - val_nd: 0.2659\n",
            "Epoch 30/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.6903 - nd: 0.5439 - val_loss: 0.2834 - val_nd: 0.3012\n",
            "Epoch 31/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.6625 - nd: 0.5184 - val_loss: 0.2608 - val_nd: 0.2723\n",
            "Epoch 32/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.6730 - nd: 0.5247 - val_loss: 0.2541 - val_nd: 0.2544\n",
            "Epoch 33/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.6908 - nd: 0.5269 - val_loss: 0.2822 - val_nd: 0.3130\n",
            "Epoch 34/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.6628 - nd: 0.5335 - val_loss: 0.2874 - val_nd: 0.3068\n",
            "Epoch 35/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.6746 - nd: 0.5111 - val_loss: 0.2938 - val_nd: 0.3034\n",
            "Epoch 36/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.6750 - nd: 0.5150 - val_loss: 0.2800 - val_nd: 0.3047\n",
            "Epoch 37/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.6624 - nd: 0.5027 - val_loss: 0.2724 - val_nd: 0.2804\n",
            "Epoch 38/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.6589 - nd: 0.4995 - val_loss: 0.2308 - val_nd: 0.2494\n",
            "Epoch 39/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.6520 - nd: 0.4957 - val_loss: 0.2520 - val_nd: 0.2640\n",
            "Epoch 40/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.6618 - nd: 0.4983 - val_loss: 0.2042 - val_nd: 0.2150\n",
            "Epoch 41/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.6478 - nd: 0.4943 - val_loss: 0.2583 - val_nd: 0.2902\n",
            "Epoch 42/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.6467 - nd: 0.4939 - val_loss: 0.2441 - val_nd: 0.2636\n",
            "Epoch 43/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.6518 - nd: 0.4969 - val_loss: 0.2559 - val_nd: 0.2728\n",
            "Epoch 44/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.6540 - nd: 0.4939 - val_loss: 0.2203 - val_nd: 0.2426\n",
            "Epoch 45/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.6566 - nd: 0.4958 - val_loss: 0.2381 - val_nd: 0.2595\n",
            "Epoch 46/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.6462 - nd: 0.4859 - val_loss: 0.2496 - val_nd: 0.2883\n",
            "Epoch 47/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.6474 - nd: 0.4930 - val_loss: 0.2674 - val_nd: 0.2886\n",
            "Epoch 48/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.6414 - nd: 0.4781 - val_loss: 0.2294 - val_nd: 0.2636\n",
            "Epoch 49/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.6471 - nd: 0.4860 - val_loss: 0.2351 - val_nd: 0.2534\n",
            "Epoch 50/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.6403 - nd: 0.4766 - val_loss: 0.2320 - val_nd: 0.2542\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 72981\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                   epoch 49\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                    loss 0.640285074710846\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                   _step 50\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                val_loss 0.23195277154445648\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                      nd 0.47662612795829773\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              _timestamp 1650175607.8769474\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                _runtime 4032.20520734787\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                  val_nd 0.25418519973754883\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           best_val_loss 0.20415949821472168\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              best_epoch 39\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             best-nd_std 0.029323390004852116\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     best-nrmse_mean_std 0.10073095458349145\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       best-nrmse_maxmin 0.03544019430329019\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   best-nrmse_maxmin_std 0.010511213295964494\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         best-nrmse_mean 0.3396301171212477\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              best-nd_95 0.001724301549518265\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      best-nrmse_mean_95 0.005923276300728799\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    best-nrmse_maxmin_95 0.0006180902470877181\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 best-nd 0.2565623523038964\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 7 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced playful-cloud-339: https://app.wandb.ai/matintavakoli/Interpreting_TS/runs/39xf3uze\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.9.7\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Wandb version 0.12.14 is available!  To upgrade, please run:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in wandb/run-20220417_060652-14l9iwfj\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mfeasible-blaze-340\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://app.wandb.ai/matintavakoli/Interpreting_TS\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://app.wandb.ai/matintavakoli/Interpreting_TS/runs/14l9iwfj\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb off` to turn off syncing.\n",
            "\n",
            "/usr/local/envs/eval-metric/lib/python3.7/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  return f(**kwargs)\n",
            "/usr/local/envs/eval-metric/lib/python3.7/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  return f(**kwargs)\n",
            "  4%|█▍                                     | 101/2660 [00:00<00:10, 233.32it/s]\n",
            "(9090, 30, 10) (9090, 6) (1111, 30, 10) (1111, 6)\n",
            "(9090, 30, 10) (9090, 6) (1111, 30, 10) (1111, 6)\n",
            "Model: \"sequential_59\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm_51 (LSTM)               (None, 30, 32)            5504      \n",
            "_________________________________________________________________\n",
            "lstm_52 (LSTM)               (None, 30, 32)            8320      \n",
            "_________________________________________________________________\n",
            "lstm_53 (LSTM)               (None, 32)                8320      \n",
            "_________________________________________________________________\n",
            "dropout_83 (Dropout)         (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense_113 (Dense)            (None, 6)                 198       \n",
            "=================================================================\n",
            "Total params: 22,342\n",
            "Trainable params: 22,342\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/200\n",
            "18/18 [==============================] - 1s 59ms/step - loss: 1.0005 - nd: 0.9070 - val_loss: 0.8021 - val_nd: 0.8638\n",
            "Epoch 2/200\n",
            "18/18 [==============================] - 0s 11ms/step - loss: 0.9620 - nd: 0.8539 - val_loss: 0.8692 - val_nd: 0.8269\n",
            "Epoch 3/200\n",
            "18/18 [==============================] - 0s 11ms/step - loss: 0.8370 - nd: 0.7136 - val_loss: 0.8097 - val_nd: 0.7214\n",
            "Epoch 4/200\n",
            "18/18 [==============================] - 0s 10ms/step - loss: 0.7436 - nd: 0.6307 - val_loss: 0.8345 - val_nd: 0.7476\n",
            "Epoch 5/200\n",
            "18/18 [==============================] - 0s 10ms/step - loss: 0.6761 - nd: 0.5645 - val_loss: 0.8327 - val_nd: 0.7214\n",
            "Epoch 6/200\n",
            "18/18 [==============================] - 0s 12ms/step - loss: 0.6440 - nd: 0.5201 - val_loss: 0.6997 - val_nd: 0.6586\n",
            "Epoch 7/200\n",
            "18/18 [==============================] - 0s 11ms/step - loss: 0.6191 - nd: 0.4952 - val_loss: 0.6230 - val_nd: 0.6065\n",
            "Epoch 8/200\n",
            "18/18 [==============================] - 0s 12ms/step - loss: 0.6006 - nd: 0.4681 - val_loss: 0.6170 - val_nd: 0.5864\n",
            "Epoch 9/200\n",
            "18/18 [==============================] - 0s 12ms/step - loss: 0.5846 - nd: 0.4570 - val_loss: 0.5539 - val_nd: 0.5298\n",
            "Epoch 10/200\n",
            "18/18 [==============================] - 0s 13ms/step - loss: 0.5676 - nd: 0.4431 - val_loss: 0.5529 - val_nd: 0.5073\n",
            "Epoch 11/200\n",
            "18/18 [==============================] - 0s 11ms/step - loss: 0.5469 - nd: 0.4235 - val_loss: 0.4940 - val_nd: 0.4764\n",
            "Epoch 12/200\n",
            "18/18 [==============================] - 0s 12ms/step - loss: 0.5231 - nd: 0.4089 - val_loss: 0.4220 - val_nd: 0.4126\n",
            "Epoch 13/200\n",
            "18/18 [==============================] - 0s 12ms/step - loss: 0.5163 - nd: 0.3972 - val_loss: 0.4219 - val_nd: 0.4056\n",
            "Epoch 14/200\n",
            "18/18 [==============================] - 0s 13ms/step - loss: 0.5106 - nd: 0.3907 - val_loss: 0.4075 - val_nd: 0.4031\n",
            "Epoch 15/200\n",
            "18/18 [==============================] - 0s 12ms/step - loss: 0.5177 - nd: 0.3844 - val_loss: 0.3577 - val_nd: 0.3578\n",
            "Epoch 16/200\n",
            "18/18 [==============================] - 0s 12ms/step - loss: 0.5164 - nd: 0.3805 - val_loss: 0.3512 - val_nd: 0.3518\n",
            "Epoch 17/200\n",
            "18/18 [==============================] - 0s 9ms/step - loss: 0.5057 - nd: 0.3689 - val_loss: 0.3722 - val_nd: 0.3611\n",
            "Epoch 18/200\n",
            "18/18 [==============================] - 0s 10ms/step - loss: 0.5021 - nd: 0.3725 - val_loss: 0.3872 - val_nd: 0.3795\n",
            "Epoch 19/200\n",
            "18/18 [==============================] - 0s 12ms/step - loss: 0.4944 - nd: 0.3658 - val_loss: 0.3378 - val_nd: 0.3156\n",
            "Epoch 20/200\n",
            "18/18 [==============================] - 0s 12ms/step - loss: 0.5174 - nd: 0.3672 - val_loss: 0.3268 - val_nd: 0.3103\n",
            "Epoch 21/200\n",
            "18/18 [==============================] - 0s 13ms/step - loss: 0.5074 - nd: 0.3617 - val_loss: 0.3207 - val_nd: 0.3109\n",
            "Epoch 22/200\n",
            "18/18 [==============================] - 0s 9ms/step - loss: 0.4935 - nd: 0.3522 - val_loss: 0.3354 - val_nd: 0.3160\n",
            "Epoch 23/200\n",
            "18/18 [==============================] - 0s 9ms/step - loss: 0.4928 - nd: 0.3539 - val_loss: 0.3487 - val_nd: 0.3515\n",
            "Epoch 24/200\n",
            "18/18 [==============================] - 0s 10ms/step - loss: 0.4911 - nd: 0.3503 - val_loss: 0.3222 - val_nd: 0.3139\n",
            "Epoch 25/200\n",
            "18/18 [==============================] - 0s 9ms/step - loss: 0.4905 - nd: 0.3406 - val_loss: 0.3470 - val_nd: 0.3283\n",
            "Epoch 26/200\n",
            "18/18 [==============================] - 0s 9ms/step - loss: 0.4870 - nd: 0.3447 - val_loss: 0.3425 - val_nd: 0.3310\n",
            "Epoch 27/200\n",
            "18/18 [==============================] - 0s 9ms/step - loss: 0.4880 - nd: 0.3399 - val_loss: 0.3456 - val_nd: 0.3290\n",
            "Epoch 28/200\n",
            "18/18 [==============================] - 0s 9ms/step - loss: 0.4845 - nd: 0.3351 - val_loss: 0.3268 - val_nd: 0.2933\n",
            "Epoch 29/200\n",
            "18/18 [==============================] - 0s 10ms/step - loss: 0.4651 - nd: 0.3286 - val_loss: 0.3461 - val_nd: 0.3210\n",
            "Epoch 30/200\n",
            "18/18 [==============================] - 0s 12ms/step - loss: 0.4775 - nd: 0.3321 - val_loss: 0.2939 - val_nd: 0.2866\n",
            "Epoch 31/200\n",
            "18/18 [==============================] - 0s 10ms/step - loss: 0.4884 - nd: 0.3345 - val_loss: 0.3164 - val_nd: 0.2984\n",
            "Epoch 32/200\n",
            "18/18 [==============================] - 0s 9ms/step - loss: 0.4827 - nd: 0.3389 - val_loss: 0.3091 - val_nd: 0.2867\n",
            "Epoch 33/200\n",
            "18/18 [==============================] - 0s 10ms/step - loss: 0.4794 - nd: 0.3301 - val_loss: 0.3012 - val_nd: 0.2803\n",
            "Epoch 34/200\n",
            "18/18 [==============================] - 0s 9ms/step - loss: 0.4710 - nd: 0.3278 - val_loss: 0.3337 - val_nd: 0.3027\n",
            "Epoch 35/200\n",
            "18/18 [==============================] - 0s 14ms/step - loss: 0.4648 - nd: 0.3285 - val_loss: 0.2774 - val_nd: 0.2738\n",
            "Epoch 36/200\n",
            "18/18 [==============================] - 0s 10ms/step - loss: 0.4689 - nd: 0.3246 - val_loss: 0.3127 - val_nd: 0.3037\n",
            "Epoch 37/200\n",
            "18/18 [==============================] - 0s 9ms/step - loss: 0.4768 - nd: 0.3277 - val_loss: 0.3089 - val_nd: 0.2908\n",
            "Epoch 38/200\n",
            "18/18 [==============================] - 0s 10ms/step - loss: 0.4745 - nd: 0.3263 - val_loss: 0.3146 - val_nd: 0.2977\n",
            "Epoch 39/200\n",
            "18/18 [==============================] - 0s 10ms/step - loss: 0.4720 - nd: 0.3185 - val_loss: 0.3142 - val_nd: 0.2902\n",
            "Epoch 40/200\n",
            "18/18 [==============================] - 0s 9ms/step - loss: 0.4653 - nd: 0.3212 - val_loss: 0.3135 - val_nd: 0.3004\n",
            "Epoch 41/200\n",
            "18/18 [==============================] - 0s 10ms/step - loss: 0.4718 - nd: 0.3226 - val_loss: 0.3051 - val_nd: 0.2920\n",
            "Epoch 42/200\n",
            "18/18 [==============================] - 0s 10ms/step - loss: 0.4706 - nd: 0.3272 - val_loss: 0.3228 - val_nd: 0.2999\n",
            "Epoch 43/200\n",
            "18/18 [==============================] - 0s 9ms/step - loss: 0.4751 - nd: 0.3185 - val_loss: 0.3106 - val_nd: 0.2894\n",
            "Epoch 44/200\n",
            "18/18 [==============================] - 0s 9ms/step - loss: 0.4728 - nd: 0.3205 - val_loss: 0.3148 - val_nd: 0.2885\n",
            "Epoch 45/200\n",
            "18/18 [==============================] - 0s 10ms/step - loss: 0.4602 - nd: 0.3127 - val_loss: 0.2805 - val_nd: 0.2739\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 73188\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                   epoch 44\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                    loss 0.4602411091327667\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                   _step 45\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                val_loss 0.2805252969264984\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                      nd 0.31265318393707275\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              _timestamp 1650175629.327508\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                _runtime 4053.655767917633\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                  val_nd 0.2739156186580658\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           best_val_loss 0.27737510204315186\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              best_epoch 34\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             best-nd_std 0.08807114997817408\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     best-nrmse_mean_std 0.18161869580449502\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       best-nrmse_maxmin 0.043526717074468785\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   best-nrmse_maxmin_std 0.018951799454591942\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         best-nrmse_mean 0.4171248016135403\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              best-nd_95 0.005178842567318879\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      best-nrmse_mean_95 0.010679713312320234\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    best-nrmse_maxmin_95 0.0011144215303996229\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 best-nd 0.28818480080598485\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 7 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced feasible-blaze-340: https://app.wandb.ai/matintavakoli/Interpreting_TS/runs/14l9iwfj\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.9.7\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Wandb version 0.12.14 is available!  To upgrade, please run:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in wandb/run-20220417_060714-2qfak9r0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mroyal-pyramid-341\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://app.wandb.ai/matintavakoli/Interpreting_TS\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://app.wandb.ai/matintavakoli/Interpreting_TS/runs/2qfak9r0\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb off` to turn off syncing.\n",
            "\n",
            "/usr/local/envs/eval-metric/lib/python3.7/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  return f(**kwargs)\n",
            "/usr/local/envs/eval-metric/lib/python3.7/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  return f(**kwargs)\n",
            "  4%|█▍                                     | 101/2660 [00:00<00:10, 234.03it/s]\n",
            "(9090, 30, 10) (9090, 6) (1111, 30, 10) (1111, 6)\n",
            "(9090, 30, 10) (9090, 6) (1111, 30, 10) (1111, 6)\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 73384\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                   _step 2\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                    loss 0.0027661548497454076\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                      nd 0.0760813607217118\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              _timestamp 1650175934.5550406\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                _runtime 4358.883300542831\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                val_loss 0.020379206944868965\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                  val_nd 0.09190883940905849\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             best-nd_std 0.00790170501006975\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     best-nrmse_mean_std 0.048209953872697005\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       best-nrmse_maxmin 0.01963221249357597\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   best-nrmse_maxmin_std 0.00503067910196869\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         best-nrmse_mean 0.188139223263892\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              best-nd_95 0.0004646434873473021\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      best-nrmse_mean_95 0.0028348870356101506\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    best-nrmse_maxmin_95 0.000295818722501676\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 best-nd 0.1072874640153926\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 8 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced royal-pyramid-341: https://app.wandb.ai/matintavakoli/Interpreting_TS/runs/2qfak9r0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.9.7\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Wandb version 0.12.14 is available!  To upgrade, please run:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in wandb/run-20220417_061218-2ze2mgdb\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33meffortless-darkness-342\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://app.wandb.ai/matintavakoli/Interpreting_TS\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://app.wandb.ai/matintavakoli/Interpreting_TS/runs/2ze2mgdb\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb off` to turn off syncing.\n",
            "\n",
            "/usr/local/envs/eval-metric/lib/python3.7/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  return f(**kwargs)\n",
            "/usr/local/envs/eval-metric/lib/python3.7/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  return f(**kwargs)\n",
            "  4%|█▍                                     | 101/2660 [00:00<00:11, 224.87it/s]\n",
            "(9090, 30, 10) (9090, 6) (1111, 30, 10) (1111, 6)\n",
            "(9090, 30, 10) (9090, 6) (1111, 30, 10) (1111, 6)\n",
            "Model: \"sequential_60\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "reshape_30 (Reshape)         (None, 300)               0         \n",
            "_________________________________________________________________\n",
            "dense_114 (Dense)            (None, 64)                19264     \n",
            "_________________________________________________________________\n",
            "dropout_84 (Dropout)         (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_115 (Dense)            (None, 64)                4160      \n",
            "_________________________________________________________________\n",
            "dropout_85 (Dropout)         (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_116 (Dense)            (None, 64)                4160      \n",
            "_________________________________________________________________\n",
            "dropout_86 (Dropout)         (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_117 (Dense)            (None, 6)                 390       \n",
            "=================================================================\n",
            "Total params: 27,974\n",
            "Trainable params: 27,974\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/200\n",
            "18/18 [==============================] - 0s 10ms/step - loss: 1.0477 - nd: 1.0138 - val_loss: 0.8311 - val_nd: 0.8729\n",
            "Epoch 2/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 1.0004 - nd: 0.8681 - val_loss: 0.8163 - val_nd: 0.9014\n",
            "Epoch 3/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.9864 - nd: 0.8649 - val_loss: 0.7970 - val_nd: 0.8770\n",
            "Epoch 4/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.9626 - nd: 0.8320 - val_loss: 0.7528 - val_nd: 0.8223\n",
            "Epoch 5/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.9019 - nd: 0.7672 - val_loss: 0.6486 - val_nd: 0.7088\n",
            "Epoch 6/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.7853 - nd: 0.6079 - val_loss: 0.4647 - val_nd: 0.4737\n",
            "Epoch 7/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.6462 - nd: 0.4148 - val_loss: 0.2924 - val_nd: 0.2650\n",
            "Epoch 8/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.5543 - nd: 0.3141 - val_loss: 0.2373 - val_nd: 0.2276\n",
            "Epoch 9/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.5099 - nd: 0.3038 - val_loss: 0.2183 - val_nd: 0.2158\n",
            "Epoch 10/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.4980 - nd: 0.3066 - val_loss: 0.1799 - val_nd: 0.1792\n",
            "Epoch 11/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.4755 - nd: 0.2584 - val_loss: 0.1692 - val_nd: 0.1800\n",
            "Epoch 12/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.4565 - nd: 0.2442 - val_loss: 0.1820 - val_nd: 0.2097\n",
            "Epoch 13/200\n",
            "18/18 [==============================] - 0s 8ms/step - loss: 0.4513 - nd: 0.2525 - val_loss: 0.1608 - val_nd: 0.1641\n",
            "Epoch 14/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4376 - nd: 0.2339 - val_loss: 0.1640 - val_nd: 0.1839\n",
            "Epoch 15/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.4279 - nd: 0.2201 - val_loss: 0.1460 - val_nd: 0.1464\n",
            "Epoch 16/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4163 - nd: 0.2247 - val_loss: 0.1615 - val_nd: 0.1687\n",
            "Epoch 17/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4289 - nd: 0.2766 - val_loss: 0.1873 - val_nd: 0.1870\n",
            "Epoch 18/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4182 - nd: 0.2399 - val_loss: 0.1576 - val_nd: 0.1534\n",
            "Epoch 19/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.4138 - nd: 0.2167 - val_loss: 0.1549 - val_nd: 0.1464\n",
            "Epoch 20/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4055 - nd: 0.2023 - val_loss: 0.1754 - val_nd: 0.1718\n",
            "Epoch 21/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4061 - nd: 0.1979 - val_loss: 0.1665 - val_nd: 0.1471\n",
            "Epoch 22/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.3957 - nd: 0.1993 - val_loss: 0.1758 - val_nd: 0.1535\n",
            "Epoch 23/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.3937 - nd: 0.1968 - val_loss: 0.1707 - val_nd: 0.1459\n",
            "Epoch 24/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.3978 - nd: 0.1935 - val_loss: 0.1790 - val_nd: 0.1504\n",
            "Epoch 25/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.3924 - nd: 0.1913 - val_loss: 0.1756 - val_nd: 0.1470\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 73486\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                   epoch 24\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                    loss 0.39244621992111206\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                   _step 25\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                val_loss 0.175557479262352\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                      nd 0.19126181304454803\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              _timestamp 1650175946.01047\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                _runtime 4370.338729858398\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                  val_nd 0.1470150500535965\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           best_val_loss 0.1459588259458542\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              best_epoch 14\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             best-nd_std 0.024399174848703412\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     best-nrmse_mean_std 0.033467002543058295\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       best-nrmse_maxmin 0.02593590138495995\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   best-nrmse_maxmin_std 0.0034922611779192258\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         best-nrmse_mean 0.24854867187343\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              best-nd_95 0.0014347432200582822\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      best-nrmse_mean_95 0.0019679581498993934\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    best-nrmse_maxmin_95 0.00020535522527962132\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 best-nd 0.18597262037139606\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 7 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced effortless-darkness-342: https://app.wandb.ai/matintavakoli/Interpreting_TS/runs/2ze2mgdb\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.9.7\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Wandb version 0.12.14 is available!  To upgrade, please run:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in wandb/run-20220417_061231-2mncs7yh\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33msolar-smoke-343\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://app.wandb.ai/matintavakoli/Interpreting_TS\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://app.wandb.ai/matintavakoli/Interpreting_TS/runs/2mncs7yh\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb off` to turn off syncing.\n",
            "\n",
            "/usr/local/envs/eval-metric/lib/python3.7/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  return f(**kwargs)\n",
            "/usr/local/envs/eval-metric/lib/python3.7/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  return f(**kwargs)\n",
            "  4%|█▍                                     | 101/2660 [00:00<00:10, 235.22it/s]\n",
            "(9090, 30, 10) (9090, 6) (1111, 30, 10) (1111, 6)\n",
            "(9090, 30, 10) (9090, 6) (1111, 30, 10) (1111, 6)\n",
            "Model: \"sequential_61\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm_54 (LSTM)               (None, 30, 64)            19200     \n",
            "_________________________________________________________________\n",
            "lstm_55 (LSTM)               (None, 30, 64)            33024     \n",
            "_________________________________________________________________\n",
            "lstm_56 (LSTM)               (None, 64)                33024     \n",
            "_________________________________________________________________\n",
            "dropout_87 (Dropout)         (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_118 (Dense)            (None, 6)                 390       \n",
            "=================================================================\n",
            "Total params: 85,638\n",
            "Trainable params: 85,638\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/200\n",
            "18/18 [==============================] - 1s 63ms/step - loss: 0.9497 - nd: 0.8302 - val_loss: 0.6151 - val_nd: 0.6740\n",
            "Epoch 2/200\n",
            "18/18 [==============================] - 0s 14ms/step - loss: 0.6848 - nd: 0.5363 - val_loss: 0.3491 - val_nd: 0.3788\n",
            "Epoch 3/200\n",
            "18/18 [==============================] - 0s 13ms/step - loss: 0.5476 - nd: 0.4000 - val_loss: 0.3137 - val_nd: 0.3598\n",
            "Epoch 4/200\n",
            "18/18 [==============================] - 0s 14ms/step - loss: 0.4803 - nd: 0.3233 - val_loss: 0.2377 - val_nd: 0.2478\n",
            "Epoch 5/200\n",
            "18/18 [==============================] - 0s 12ms/step - loss: 0.4306 - nd: 0.2783 - val_loss: 0.2280 - val_nd: 0.2260\n",
            "Epoch 6/200\n",
            "18/18 [==============================] - 0s 11ms/step - loss: 0.4222 - nd: 0.2576 - val_loss: 0.2649 - val_nd: 0.2688\n",
            "Epoch 7/200\n",
            "18/18 [==============================] - 0s 11ms/step - loss: 0.4615 - nd: 0.3309 - val_loss: 0.2303 - val_nd: 0.2202\n",
            "Epoch 8/200\n",
            "18/18 [==============================] - 0s 13ms/step - loss: 0.4354 - nd: 0.2822 - val_loss: 0.2276 - val_nd: 0.2187\n",
            "Epoch 9/200\n",
            "18/18 [==============================] - 0s 13ms/step - loss: 0.4078 - nd: 0.2565 - val_loss: 0.2124 - val_nd: 0.2066\n",
            "Epoch 10/200\n",
            "18/18 [==============================] - 0s 11ms/step - loss: 0.4038 - nd: 0.2400 - val_loss: 0.2396 - val_nd: 0.2440\n",
            "Epoch 11/200\n",
            "18/18 [==============================] - 0s 13ms/step - loss: 0.3990 - nd: 0.2290 - val_loss: 0.2028 - val_nd: 0.1845\n",
            "Epoch 12/200\n",
            "18/18 [==============================] - 0s 12ms/step - loss: 0.3989 - nd: 0.2302 - val_loss: 0.2119 - val_nd: 0.2018\n",
            "Epoch 13/200\n",
            "18/18 [==============================] - 0s 11ms/step - loss: 0.3905 - nd: 0.2146 - val_loss: 0.2415 - val_nd: 0.2545\n",
            "Epoch 14/200\n",
            "18/18 [==============================] - 0s 10ms/step - loss: 0.3928 - nd: 0.2281 - val_loss: 0.2352 - val_nd: 0.2400\n",
            "Epoch 15/200\n",
            "18/18 [==============================] - 0s 13ms/step - loss: 0.3904 - nd: 0.2164 - val_loss: 0.1923 - val_nd: 0.1810\n",
            "Epoch 16/200\n",
            "18/18 [==============================] - 0s 14ms/step - loss: 0.3880 - nd: 0.1978 - val_loss: 0.1878 - val_nd: 0.1671\n",
            "Epoch 17/200\n",
            "18/18 [==============================] - 0s 11ms/step - loss: 0.4001 - nd: 0.1919 - val_loss: 0.1965 - val_nd: 0.1685\n",
            "Epoch 18/200\n",
            "18/18 [==============================] - 0s 13ms/step - loss: 0.3798 - nd: 0.1890 - val_loss: 0.1808 - val_nd: 0.1673\n",
            "Epoch 19/200\n",
            "18/18 [==============================] - 0s 12ms/step - loss: 0.3827 - nd: 0.1881 - val_loss: 0.1529 - val_nd: 0.1618\n",
            "Epoch 20/200\n",
            "18/18 [==============================] - 0s 14ms/step - loss: 0.3825 - nd: 0.1887 - val_loss: 0.1527 - val_nd: 0.1781\n",
            "Epoch 21/200\n",
            "18/18 [==============================] - 0s 13ms/step - loss: 0.3714 - nd: 0.1858 - val_loss: 0.1364 - val_nd: 0.1463\n",
            "Epoch 22/200\n",
            "18/18 [==============================] - 0s 11ms/step - loss: 0.3619 - nd: 0.1965 - val_loss: 0.1689 - val_nd: 0.1757\n",
            "Epoch 23/200\n",
            "18/18 [==============================] - 0s 11ms/step - loss: 0.3740 - nd: 0.1847 - val_loss: 0.1996 - val_nd: 0.1919\n",
            "Epoch 24/200\n",
            "18/18 [==============================] - 0s 10ms/step - loss: 0.3673 - nd: 0.1824 - val_loss: 0.1859 - val_nd: 0.1788\n",
            "Epoch 25/200\n",
            "18/18 [==============================] - 0s 12ms/step - loss: 0.3660 - nd: 0.1831 - val_loss: 0.1916 - val_nd: 0.1551\n",
            "Epoch 26/200\n",
            "18/18 [==============================] - 0s 11ms/step - loss: 0.3665 - nd: 0.2305 - val_loss: 0.1512 - val_nd: 0.1667\n",
            "Epoch 27/200\n",
            "18/18 [==============================] - 0s 11ms/step - loss: 0.3508 - nd: 0.1968 - val_loss: 0.1424 - val_nd: 0.1423\n",
            "Epoch 28/200\n",
            "18/18 [==============================] - 0s 11ms/step - loss: 0.3701 - nd: 0.2113 - val_loss: 0.1966 - val_nd: 0.2272\n",
            "Epoch 29/200\n",
            "18/18 [==============================] - 0s 13ms/step - loss: 0.3690 - nd: 0.1784 - val_loss: 0.1280 - val_nd: 0.1233\n",
            "Epoch 30/200\n",
            "18/18 [==============================] - 0s 11ms/step - loss: 0.3599 - nd: 0.1685 - val_loss: 0.1498 - val_nd: 0.1600\n",
            "Epoch 31/200\n",
            "18/18 [==============================] - 0s 11ms/step - loss: 0.3526 - nd: 0.1653 - val_loss: 0.1391 - val_nd: 0.1418\n",
            "Epoch 32/200\n",
            "18/18 [==============================] - 0s 10ms/step - loss: 0.3548 - nd: 0.1638 - val_loss: 0.1634 - val_nd: 0.1415\n",
            "Epoch 33/200\n",
            "18/18 [==============================] - 0s 11ms/step - loss: 0.3594 - nd: 0.1688 - val_loss: 0.1389 - val_nd: 0.1215\n",
            "Epoch 34/200\n",
            "18/18 [==============================] - 0s 15ms/step - loss: 0.3622 - nd: 0.1879 - val_loss: 0.1276 - val_nd: 0.1225\n",
            "Epoch 35/200\n",
            "18/18 [==============================] - 0s 11ms/step - loss: 0.3500 - nd: 0.1714 - val_loss: 0.1453 - val_nd: 0.1468\n",
            "Epoch 36/200\n",
            "18/18 [==============================] - 0s 11ms/step - loss: 0.3487 - nd: 0.1670 - val_loss: 0.1779 - val_nd: 0.2027\n",
            "Epoch 37/200\n",
            "18/18 [==============================] - 0s 11ms/step - loss: 0.3477 - nd: 0.1726 - val_loss: 0.1686 - val_nd: 0.1982\n",
            "Epoch 38/200\n",
            "18/18 [==============================] - 0s 11ms/step - loss: 0.3386 - nd: 0.1703 - val_loss: 0.1423 - val_nd: 0.1275\n",
            "Epoch 39/200\n",
            "18/18 [==============================] - 0s 13ms/step - loss: 0.3446 - nd: 0.1569 - val_loss: 0.1509 - val_nd: 0.1358\n",
            "Epoch 40/200\n",
            "18/18 [==============================] - 0s 10ms/step - loss: 0.3401 - nd: 0.1546 - val_loss: 0.1277 - val_nd: 0.1190\n",
            "Epoch 41/200\n",
            "18/18 [==============================] - 0s 14ms/step - loss: 0.3468 - nd: 0.1561 - val_loss: 0.1209 - val_nd: 0.1229\n",
            "Epoch 42/200\n",
            "18/18 [==============================] - 0s 12ms/step - loss: 0.3420 - nd: 0.1518 - val_loss: 0.1411 - val_nd: 0.1351\n",
            "Epoch 43/200\n",
            "18/18 [==============================] - 0s 11ms/step - loss: 0.3310 - nd: 0.1501 - val_loss: 0.1215 - val_nd: 0.1096\n",
            "Epoch 44/200\n",
            "18/18 [==============================] - 0s 12ms/step - loss: 0.3377 - nd: 0.1800 - val_loss: 0.1374 - val_nd: 0.1234\n",
            "Epoch 45/200\n",
            "18/18 [==============================] - 0s 11ms/step - loss: 0.3406 - nd: 0.1641 - val_loss: 0.1320 - val_nd: 0.1452\n",
            "Epoch 46/200\n",
            "18/18 [==============================] - 0s 11ms/step - loss: 0.3457 - nd: 0.1550 - val_loss: 0.1489 - val_nd: 0.1578\n",
            "Epoch 47/200\n",
            "18/18 [==============================] - 0s 11ms/step - loss: 0.3465 - nd: 0.1653 - val_loss: 0.1778 - val_nd: 0.1874\n",
            "Epoch 48/200\n",
            "18/18 [==============================] - 0s 13ms/step - loss: 0.3545 - nd: 0.1645 - val_loss: 0.1146 - val_nd: 0.1065\n",
            "Epoch 49/200\n",
            "18/18 [==============================] - 0s 10ms/step - loss: 0.3457 - nd: 0.1493 - val_loss: 0.1321 - val_nd: 0.1320\n",
            "Epoch 50/200\n",
            "18/18 [==============================] - 0s 11ms/step - loss: 0.3138 - nd: 0.1439 - val_loss: 0.1250 - val_nd: 0.1256\n",
            "Epoch 51/200\n",
            "18/18 [==============================] - 0s 11ms/step - loss: 0.3532 - nd: 0.1505 - val_loss: 0.1407 - val_nd: 0.1297\n",
            "Epoch 52/200\n",
            "18/18 [==============================] - 0s 11ms/step - loss: 0.3566 - nd: 0.1624 - val_loss: 0.1283 - val_nd: 0.1088\n",
            "Epoch 53/200\n",
            "18/18 [==============================] - 0s 12ms/step - loss: 0.3527 - nd: 0.1723 - val_loss: 0.1537 - val_nd: 0.1576\n",
            "Epoch 54/200\n",
            "18/18 [==============================] - 0s 12ms/step - loss: 0.3528 - nd: 0.1651 - val_loss: 0.1299 - val_nd: 0.1303\n",
            "Epoch 55/200\n",
            "18/18 [==============================] - 0s 11ms/step - loss: 0.3342 - nd: 0.1595 - val_loss: 0.1174 - val_nd: 0.1087\n",
            "Epoch 56/200\n",
            "18/18 [==============================] - 0s 11ms/step - loss: 0.3341 - nd: 0.1547 - val_loss: 0.1333 - val_nd: 0.1372\n",
            "Epoch 57/200\n",
            "18/18 [==============================] - 0s 11ms/step - loss: 0.3269 - nd: 0.1518 - val_loss: 0.1508 - val_nd: 0.1660\n",
            "Epoch 58/200\n",
            "18/18 [==============================] - 0s 12ms/step - loss: 0.3222 - nd: 0.1619 - val_loss: 0.1777 - val_nd: 0.1938\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 73618\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                   epoch 57\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                    loss 0.3222326934337616\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                   _step 58\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                val_loss 0.1776561141014099\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                      nd 0.1618722826242447\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              _timestamp 1650175972.2014089\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                _runtime 4396.529668807983\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                  val_nd 0.1937842220067978\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           best_val_loss 0.11464344710111618\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              best_epoch 47\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             best-nd_std 0.010838316910872031\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     best-nrmse_mean_std 0.02935917006896388\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       best-nrmse_maxmin 0.020425815492174065\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   best-nrmse_maxmin_std 0.0030636113800707673\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         best-nrmse_mean 0.19574447161707678\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              best-nd_95 0.0006373249014010415\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      best-nrmse_mean_95 0.0017264055224893154\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    best-nrmse_maxmin_95 0.00018014935684120115\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 best-nd 0.13233863518038966\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 7 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced solar-smoke-343: https://app.wandb.ai/matintavakoli/Interpreting_TS/runs/2mncs7yh\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.9.7\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Wandb version 0.12.14 is available!  To upgrade, please run:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in wandb/run-20220417_061257-1fz1j5cs\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mmild-monkey-344\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://app.wandb.ai/matintavakoli/Interpreting_TS\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://app.wandb.ai/matintavakoli/Interpreting_TS/runs/1fz1j5cs\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb off` to turn off syncing.\n",
            "\n",
            "/usr/local/envs/eval-metric/lib/python3.7/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  return f(**kwargs)\n",
            "/usr/local/envs/eval-metric/lib/python3.7/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  return f(**kwargs)\n",
            "  4%|█▍                                     | 101/2660 [00:00<00:11, 217.01it/s]\n",
            "(9090, 30, 10) (9090, 6) (1111, 30, 10) (1111, 6)\n",
            "(9090, 30, 10) (9090, 6) (1111, 30, 10) (1111, 6)\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 73853\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                   _step 2\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                    loss 0.0028587911979192737\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                      nd 0.07778946388568019\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              _timestamp 1650176282.103623\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                _runtime 4706.431882858276\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                val_loss 0.020962948229913868\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                  val_nd 0.09232544271450832\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             best-nd_std 0.007726125786846625\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     best-nrmse_mean_std 0.05191684125907204\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       best-nrmse_maxmin 0.020131807666148444\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   best-nrmse_maxmin_std 0.005417490525958651\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         best-nrmse_mean 0.19292693874654238\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              best-nd_95 0.0004543189127801476\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      best-nrmse_mean_95 0.003052862912995355\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    best-nrmse_maxmin_95 0.00031856437154317125\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 best-nd 0.1077634602461229\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 8 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced mild-monkey-344: https://app.wandb.ai/matintavakoli/Interpreting_TS/runs/1fz1j5cs\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.9.7\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Wandb version 0.12.14 is available!  To upgrade, please run:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in wandb/run-20220417_061806-1hmr9aep\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mproud-fog-345\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://app.wandb.ai/matintavakoli/Interpreting_TS\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://app.wandb.ai/matintavakoli/Interpreting_TS/runs/1hmr9aep\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb off` to turn off syncing.\n",
            "\n",
            "/usr/local/envs/eval-metric/lib/python3.7/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  return f(**kwargs)\n",
            "/usr/local/envs/eval-metric/lib/python3.7/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  return f(**kwargs)\n",
            "  4%|█▍                                     | 101/2660 [00:00<00:12, 206.41it/s]\n",
            "(9090, 30, 10) (9090, 6) (1111, 30, 10) (1111, 6)\n",
            "(9090, 30, 10) (9090, 6) (1111, 30, 10) (1111, 6)\n",
            "Model: \"sequential_62\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "reshape_31 (Reshape)         (None, 300)               0         \n",
            "_________________________________________________________________\n",
            "dense_119 (Dense)            (None, 64)                19264     \n",
            "_________________________________________________________________\n",
            "dropout_88 (Dropout)         (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_120 (Dense)            (None, 64)                4160      \n",
            "_________________________________________________________________\n",
            "dropout_89 (Dropout)         (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_121 (Dense)            (None, 64)                4160      \n",
            "_________________________________________________________________\n",
            "dropout_90 (Dropout)         (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_122 (Dense)            (None, 6)                 390       \n",
            "=================================================================\n",
            "Total params: 27,974\n",
            "Trainable params: 27,974\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/200\n",
            "18/18 [==============================] - 0s 10ms/step - loss: 1.1520 - nd: 1.1723 - val_loss: 0.8260 - val_nd: 0.8915\n",
            "Epoch 2/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 1.0558 - nd: 0.9607 - val_loss: 0.8239 - val_nd: 0.9140\n",
            "Epoch 3/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 1.0387 - nd: 0.9485 - val_loss: 0.8162 - val_nd: 0.9011\n",
            "Epoch 4/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 1.0249 - nd: 0.9333 - val_loss: 0.8044 - val_nd: 0.8838\n",
            "Epoch 5/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 1.0062 - nd: 0.8931 - val_loss: 0.7833 - val_nd: 0.8624\n",
            "Epoch 6/200\n",
            "18/18 [==============================] - 0s 8ms/step - loss: 0.9775 - nd: 0.8641 - val_loss: 0.7410 - val_nd: 0.8194\n",
            "Epoch 7/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.9318 - nd: 0.8081 - val_loss: 0.6545 - val_nd: 0.6959\n",
            "Epoch 8/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.8582 - nd: 0.7081 - val_loss: 0.5269 - val_nd: 0.5237\n",
            "Epoch 9/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.7732 - nd: 0.6122 - val_loss: 0.3946 - val_nd: 0.3557\n",
            "Epoch 10/200\n",
            "18/18 [==============================] - 0s 8ms/step - loss: 0.7082 - nd: 0.5671 - val_loss: 0.3039 - val_nd: 0.2808\n",
            "Epoch 11/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.6701 - nd: 0.5249 - val_loss: 0.2587 - val_nd: 0.2457\n",
            "Epoch 12/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.6268 - nd: 0.4843 - val_loss: 0.2296 - val_nd: 0.2044\n",
            "Epoch 13/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.6050 - nd: 0.4546 - val_loss: 0.2058 - val_nd: 0.1947\n",
            "Epoch 14/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.5781 - nd: 0.4319 - val_loss: 0.1949 - val_nd: 0.1814\n",
            "Epoch 15/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5657 - nd: 0.4155 - val_loss: 0.2030 - val_nd: 0.1961\n",
            "Epoch 16/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.5540 - nd: 0.4068 - val_loss: 0.1740 - val_nd: 0.1811\n",
            "Epoch 17/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.5488 - nd: 0.3973 - val_loss: 0.1724 - val_nd: 0.1812\n",
            "Epoch 18/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5364 - nd: 0.3907 - val_loss: 0.1759 - val_nd: 0.1943\n",
            "Epoch 19/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.5186 - nd: 0.3800 - val_loss: 0.1592 - val_nd: 0.1818\n",
            "Epoch 20/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.5063 - nd: 0.3708 - val_loss: 0.1658 - val_nd: 0.1817\n",
            "Epoch 21/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.5036 - nd: 0.3630 - val_loss: 0.1727 - val_nd: 0.1849\n",
            "Epoch 22/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4964 - nd: 0.3535 - val_loss: 0.1611 - val_nd: 0.1752\n",
            "Epoch 23/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.5016 - nd: 0.3524 - val_loss: 0.1654 - val_nd: 0.1788\n",
            "Epoch 24/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4871 - nd: 0.3408 - val_loss: 0.1972 - val_nd: 0.2029\n",
            "Epoch 25/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4868 - nd: 0.3430 - val_loss: 0.1725 - val_nd: 0.1811\n",
            "Epoch 26/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.4801 - nd: 0.3401 - val_loss: 0.1683 - val_nd: 0.1576\n",
            "Epoch 27/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4748 - nd: 0.3357 - val_loss: 0.1819 - val_nd: 0.1858\n",
            "Epoch 28/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4834 - nd: 0.3346 - val_loss: 0.1784 - val_nd: 0.1778\n",
            "Epoch 29/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4656 - nd: 0.3251 - val_loss: 0.1880 - val_nd: 0.1860\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 73957\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                   epoch 28\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                    loss 0.4655853509902954\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                   _step 29\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                val_loss 0.18800656497478485\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                      nd 0.3251308798789978\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              _timestamp 1650176292.4087107\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                _runtime 4716.736970663071\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                  val_nd 0.1860206127166748\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           best_val_loss 0.15915647149085999\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              best_epoch 18\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             best-nd_std 0.01986579111507791\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     best-nrmse_mean_std 0.03499226719631414\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       best-nrmse_maxmin 0.02771621438385051\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   best-nrmse_maxmin_std 0.0036514216084885492\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         best-nrmse_mean 0.2656097496754088\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              best-nd_95 0.0011681669273732314\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      best-nrmse_mean_95 0.0020576481961252667\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    best-nrmse_maxmin_95 0.00021471432656386123\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 best-nd 0.21253425808878249\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 7 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced proud-fog-345: https://app.wandb.ai/matintavakoli/Interpreting_TS/runs/1hmr9aep\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.9.7\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Wandb version 0.12.14 is available!  To upgrade, please run:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in wandb/run-20220417_061816-a30o0utm\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mgentle-gorge-346\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://app.wandb.ai/matintavakoli/Interpreting_TS\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://app.wandb.ai/matintavakoli/Interpreting_TS/runs/a30o0utm\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb off` to turn off syncing.\n",
            "\n",
            "/usr/local/envs/eval-metric/lib/python3.7/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  return f(**kwargs)\n",
            "/usr/local/envs/eval-metric/lib/python3.7/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  return f(**kwargs)\n",
            "  4%|█▍                                     | 101/2660 [00:00<00:11, 214.90it/s]\n",
            "(9090, 30, 10) (9090, 6) (1111, 30, 10) (1111, 6)\n",
            "(9090, 30, 10) (9090, 6) (1111, 30, 10) (1111, 6)\n",
            "Model: \"sequential_63\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm_57 (LSTM)               (None, 30, 64)            19200     \n",
            "_________________________________________________________________\n",
            "lstm_58 (LSTM)               (None, 30, 64)            33024     \n",
            "_________________________________________________________________\n",
            "lstm_59 (LSTM)               (None, 64)                33024     \n",
            "_________________________________________________________________\n",
            "dropout_91 (Dropout)         (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_123 (Dense)            (None, 6)                 390       \n",
            "=================================================================\n",
            "Total params: 85,638\n",
            "Trainable params: 85,638\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/200\n",
            "18/18 [==============================] - 1s 63ms/step - loss: 0.9785 - nd: 0.8654 - val_loss: 0.8021 - val_nd: 0.7665\n",
            "Epoch 2/200\n",
            "18/18 [==============================] - 0s 15ms/step - loss: 0.8054 - nd: 0.6642 - val_loss: 0.4980 - val_nd: 0.5151\n",
            "Epoch 3/200\n",
            "18/18 [==============================] - 0s 14ms/step - loss: 0.6065 - nd: 0.4837 - val_loss: 0.4538 - val_nd: 0.4809\n",
            "Epoch 4/200\n",
            "18/18 [==============================] - 0s 14ms/step - loss: 0.5356 - nd: 0.4027 - val_loss: 0.3645 - val_nd: 0.3973\n",
            "Epoch 5/200\n",
            "18/18 [==============================] - 0s 14ms/step - loss: 0.4970 - nd: 0.3731 - val_loss: 0.3181 - val_nd: 0.3523\n",
            "Epoch 6/200\n",
            "18/18 [==============================] - 0s 14ms/step - loss: 0.4745 - nd: 0.3459 - val_loss: 0.3128 - val_nd: 0.3315\n",
            "Epoch 7/200\n",
            "18/18 [==============================] - 0s 15ms/step - loss: 0.4711 - nd: 0.3442 - val_loss: 0.2602 - val_nd: 0.2835\n",
            "Epoch 8/200\n",
            "18/18 [==============================] - 0s 11ms/step - loss: 0.4696 - nd: 0.3307 - val_loss: 0.2833 - val_nd: 0.3005\n",
            "Epoch 9/200\n",
            "18/18 [==============================] - 0s 11ms/step - loss: 0.4622 - nd: 0.3228 - val_loss: 0.2860 - val_nd: 0.3010\n",
            "Epoch 10/200\n",
            "18/18 [==============================] - 0s 14ms/step - loss: 0.4633 - nd: 0.3077 - val_loss: 0.2599 - val_nd: 0.2662\n",
            "Epoch 11/200\n",
            "18/18 [==============================] - 0s 13ms/step - loss: 0.4387 - nd: 0.2939 - val_loss: 0.2587 - val_nd: 0.2761\n",
            "Epoch 12/200\n",
            "18/18 [==============================] - 0s 11ms/step - loss: 0.4469 - nd: 0.2924 - val_loss: 0.2721 - val_nd: 0.2777\n",
            "Epoch 13/200\n",
            "18/18 [==============================] - 0s 15ms/step - loss: 0.4507 - nd: 0.2866 - val_loss: 0.2579 - val_nd: 0.2689\n",
            "Epoch 14/200\n",
            "18/18 [==============================] - 0s 11ms/step - loss: 0.4330 - nd: 0.2767 - val_loss: 0.2659 - val_nd: 0.2707\n",
            "Epoch 15/200\n",
            "18/18 [==============================] - 0s 11ms/step - loss: 0.4322 - nd: 0.2748 - val_loss: 0.2731 - val_nd: 0.2807\n",
            "Epoch 16/200\n",
            "18/18 [==============================] - 0s 13ms/step - loss: 0.4387 - nd: 0.2749 - val_loss: 0.2362 - val_nd: 0.2502\n",
            "Epoch 17/200\n",
            "18/18 [==============================] - 0s 14ms/step - loss: 0.4413 - nd: 0.2660 - val_loss: 0.2065 - val_nd: 0.2316\n",
            "Epoch 18/200\n",
            "18/18 [==============================] - 0s 12ms/step - loss: 0.4193 - nd: 0.2633 - val_loss: 0.2253 - val_nd: 0.2642\n",
            "Epoch 19/200\n",
            "18/18 [==============================] - 0s 12ms/step - loss: 0.4216 - nd: 0.2628 - val_loss: 0.2395 - val_nd: 0.2707\n",
            "Epoch 20/200\n",
            "18/18 [==============================] - 0s 13ms/step - loss: 0.4218 - nd: 0.2639 - val_loss: 0.2053 - val_nd: 0.2175\n",
            "Epoch 21/200\n",
            "18/18 [==============================] - 0s 12ms/step - loss: 0.4088 - nd: 0.2594 - val_loss: 0.3025 - val_nd: 0.2631\n",
            "Epoch 22/200\n",
            "18/18 [==============================] - 0s 12ms/step - loss: 0.4280 - nd: 0.2714 - val_loss: 0.2713 - val_nd: 0.2738\n",
            "Epoch 23/200\n",
            "18/18 [==============================] - 0s 14ms/step - loss: 0.4012 - nd: 0.2565 - val_loss: 0.1990 - val_nd: 0.2098\n",
            "Epoch 24/200\n",
            "18/18 [==============================] - 0s 12ms/step - loss: 0.4160 - nd: 0.2534 - val_loss: 0.2657 - val_nd: 0.2289\n",
            "Epoch 25/200\n",
            "18/18 [==============================] - 0s 14ms/step - loss: 0.3999 - nd: 0.2569 - val_loss: 0.1826 - val_nd: 0.1954\n",
            "Epoch 26/200\n",
            "18/18 [==============================] - 0s 14ms/step - loss: 0.4131 - nd: 0.2540 - val_loss: 0.1694 - val_nd: 0.1818\n",
            "Epoch 27/200\n",
            "18/18 [==============================] - 0s 12ms/step - loss: 0.4020 - nd: 0.2513 - val_loss: 0.2224 - val_nd: 0.2119\n",
            "Epoch 28/200\n",
            "18/18 [==============================] - 0s 12ms/step - loss: 0.4006 - nd: 0.2517 - val_loss: 0.2512 - val_nd: 0.2504\n",
            "Epoch 29/200\n",
            "18/18 [==============================] - 0s 11ms/step - loss: 0.4162 - nd: 0.2549 - val_loss: 0.2246 - val_nd: 0.2232\n",
            "Epoch 30/200\n",
            "18/18 [==============================] - 0s 11ms/step - loss: 0.4009 - nd: 0.2465 - val_loss: 0.1875 - val_nd: 0.2023\n",
            "Epoch 31/200\n",
            "18/18 [==============================] - 0s 13ms/step - loss: 0.3997 - nd: 0.2446 - val_loss: 0.1983 - val_nd: 0.2233\n",
            "Epoch 32/200\n",
            "18/18 [==============================] - 0s 14ms/step - loss: 0.4028 - nd: 0.2571 - val_loss: 0.1601 - val_nd: 0.1653\n",
            "Epoch 33/200\n",
            "18/18 [==============================] - 0s 12ms/step - loss: 0.3952 - nd: 0.2422 - val_loss: 0.2017 - val_nd: 0.2101\n",
            "Epoch 34/200\n",
            "18/18 [==============================] - 0s 12ms/step - loss: 0.4001 - nd: 0.2368 - val_loss: 0.1779 - val_nd: 0.1833\n",
            "Epoch 35/200\n",
            "18/18 [==============================] - 0s 12ms/step - loss: 0.3966 - nd: 0.2351 - val_loss: 0.1772 - val_nd: 0.1837\n",
            "Epoch 36/200\n",
            "18/18 [==============================] - 0s 12ms/step - loss: 0.4021 - nd: 0.2336 - val_loss: 0.1850 - val_nd: 0.1977\n",
            "Epoch 37/200\n",
            "18/18 [==============================] - 0s 12ms/step - loss: 0.3923 - nd: 0.2295 - val_loss: 0.2031 - val_nd: 0.2218\n",
            "Epoch 38/200\n",
            "18/18 [==============================] - 0s 12ms/step - loss: 0.3968 - nd: 0.2419 - val_loss: 0.1731 - val_nd: 0.1743\n",
            "Epoch 39/200\n",
            "18/18 [==============================] - 0s 11ms/step - loss: 0.3941 - nd: 0.2306 - val_loss: 0.2330 - val_nd: 0.1920\n",
            "Epoch 40/200\n",
            "18/18 [==============================] - 0s 12ms/step - loss: 0.3952 - nd: 0.2341 - val_loss: 0.1637 - val_nd: 0.1682\n",
            "Epoch 41/200\n",
            "18/18 [==============================] - 0s 12ms/step - loss: 0.3848 - nd: 0.2247 - val_loss: 0.2249 - val_nd: 0.2349\n",
            "Epoch 42/200\n",
            "18/18 [==============================] - 0s 14ms/step - loss: 0.3783 - nd: 0.2239 - val_loss: 0.1588 - val_nd: 0.1597\n",
            "Epoch 43/200\n",
            "18/18 [==============================] - 0s 12ms/step - loss: 0.3792 - nd: 0.2200 - val_loss: 0.1620 - val_nd: 0.1649\n",
            "Epoch 44/200\n",
            "18/18 [==============================] - 0s 12ms/step - loss: 0.3671 - nd: 0.2173 - val_loss: 0.1943 - val_nd: 0.1930\n",
            "Epoch 45/200\n",
            "18/18 [==============================] - 0s 14ms/step - loss: 0.3796 - nd: 0.2283 - val_loss: 0.1510 - val_nd: 0.1454\n",
            "Epoch 46/200\n",
            "18/18 [==============================] - 0s 12ms/step - loss: 0.4070 - nd: 0.2336 - val_loss: 0.2100 - val_nd: 0.2218\n",
            "Epoch 47/200\n",
            "18/18 [==============================] - 0s 12ms/step - loss: 0.3901 - nd: 0.2291 - val_loss: 0.1868 - val_nd: 0.1779\n",
            "Epoch 48/200\n",
            "18/18 [==============================] - 0s 12ms/step - loss: 0.3759 - nd: 0.2189 - val_loss: 0.1616 - val_nd: 0.1593\n",
            "Epoch 49/200\n",
            "18/18 [==============================] - 0s 13ms/step - loss: 0.3730 - nd: 0.2122 - val_loss: 0.1746 - val_nd: 0.1692\n",
            "Epoch 50/200\n",
            "18/18 [==============================] - 0s 12ms/step - loss: 0.3953 - nd: 0.2131 - val_loss: 0.1735 - val_nd: 0.1715\n",
            "Epoch 51/200\n",
            "18/18 [==============================] - 0s 12ms/step - loss: 0.3776 - nd: 0.2138 - val_loss: 0.1707 - val_nd: 0.1761\n",
            "Epoch 52/200\n",
            "18/18 [==============================] - 0s 11ms/step - loss: 0.3891 - nd: 0.2141 - val_loss: 0.1571 - val_nd: 0.1606\n",
            "Epoch 53/200\n",
            "18/18 [==============================] - 0s 12ms/step - loss: 0.3814 - nd: 0.2101 - val_loss: 0.1590 - val_nd: 0.1555\n",
            "Epoch 54/200\n",
            "18/18 [==============================] - 0s 13ms/step - loss: 0.3800 - nd: 0.2132 - val_loss: 0.2232 - val_nd: 0.2024\n",
            "Epoch 55/200\n",
            "18/18 [==============================] - 0s 12ms/step - loss: 0.3800 - nd: 0.2306 - val_loss: 0.1695 - val_nd: 0.1815\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 74101\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                   epoch 54\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                    loss 0.37999454140663147\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                   _step 55\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                val_loss 0.16952845454216003\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                      nd 0.23063413798809052\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              _timestamp 1650176318.2445087\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                _runtime 4742.572768688202\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                  val_nd 0.18154071271419525\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           best_val_loss 0.15102161467075348\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              best_epoch 44\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             best-nd_std 0.020502817760696335\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     best-nrmse_mean_std 0.03110911927910659\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       best-nrmse_maxmin 0.027254656792856657\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   best-nrmse_maxmin_std 0.0032462175062707176\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         best-nrmse_mean 0.261186555565751\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              best-nd_95 0.0012056259671344091\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      best-nrmse_mean_95 0.0018293076812822807\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    best-nrmse_maxmin_95 0.00019088713396403725\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 best-nd 0.19150238001602718\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 7 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced gentle-gorge-346: https://app.wandb.ai/matintavakoli/Interpreting_TS/runs/a30o0utm\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.9.7\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Wandb version 0.12.14 is available!  To upgrade, please run:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in wandb/run-20220417_061843-eaml8lgm\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mcomfy-yogurt-347\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://app.wandb.ai/matintavakoli/Interpreting_TS\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://app.wandb.ai/matintavakoli/Interpreting_TS/runs/eaml8lgm\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb off` to turn off syncing.\n",
            "\n",
            "/usr/local/envs/eval-metric/lib/python3.7/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  return f(**kwargs)\n",
            "/usr/local/envs/eval-metric/lib/python3.7/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  return f(**kwargs)\n",
            "  4%|█▍                                     | 101/2660 [00:00<00:11, 222.62it/s]\n",
            "(9090, 30, 10) (9090, 6) (1111, 30, 10) (1111, 6)\n",
            "(9090, 30, 10) (9090, 6) (1111, 30, 10) (1111, 6)\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 74327\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                   _step 2\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                    loss 0.002912865533685925\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                      nd 0.07878182334795146\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              _timestamp 1650176628.080212\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                _runtime 5052.408472061157\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                val_loss 0.021120071847411447\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                  val_nd 0.09383533509670239\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             best-nd_std 0.007780071770883361\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     best-nrmse_mean_std 0.05272669451585196\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       best-nrmse_maxmin 0.02031019903807913\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   best-nrmse_maxmin_std 0.005501998216326955\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         best-nrmse_mean 0.19463649716554388\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              best-nd_95 0.00045749109525459683\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      best-nrmse_mean_95 0.0031004846656411773\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    best-nrmse_maxmin_95 0.00032353367220807266\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 best-nd 0.10962014544981226\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 8 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced comfy-yogurt-347: https://app.wandb.ai/matintavakoli/Interpreting_TS/runs/eaml8lgm\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.9.7\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Wandb version 0.12.14 is available!  To upgrade, please run:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in wandb/run-20220417_062351-koac1b9d\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mglamorous-paper-348\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://app.wandb.ai/matintavakoli/Interpreting_TS\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://app.wandb.ai/matintavakoli/Interpreting_TS/runs/koac1b9d\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb off` to turn off syncing.\n",
            "\n",
            "/usr/local/envs/eval-metric/lib/python3.7/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  return f(**kwargs)\n",
            "/usr/local/envs/eval-metric/lib/python3.7/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  return f(**kwargs)\n",
            "  4%|█▍                                     | 101/2660 [00:00<00:10, 237.77it/s]\n",
            "(9090, 30, 10) (9090, 6) (1111, 30, 10) (1111, 6)\n",
            "(9090, 30, 10) (9090, 6) (1111, 30, 10) (1111, 6)\n",
            "Model: \"sequential_64\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "reshape_32 (Reshape)         (None, 300)               0         \n",
            "_________________________________________________________________\n",
            "dense_124 (Dense)            (None, 64)                19264     \n",
            "_________________________________________________________________\n",
            "dropout_92 (Dropout)         (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_125 (Dense)            (None, 64)                4160      \n",
            "_________________________________________________________________\n",
            "dropout_93 (Dropout)         (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_126 (Dense)            (None, 64)                4160      \n",
            "_________________________________________________________________\n",
            "dropout_94 (Dropout)         (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_127 (Dense)            (None, 6)                 390       \n",
            "=================================================================\n",
            "Total params: 27,974\n",
            "Trainable params: 27,974\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/200\n",
            "18/18 [==============================] - 0s 10ms/step - loss: 1.3144 - nd: 1.4230 - val_loss: 0.8274 - val_nd: 0.9163\n",
            "Epoch 2/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 1.1527 - nd: 1.1489 - val_loss: 0.8250 - val_nd: 0.9002\n",
            "Epoch 3/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 1.1010 - nd: 1.0619 - val_loss: 0.8233 - val_nd: 0.9141\n",
            "Epoch 4/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 1.0635 - nd: 1.0012 - val_loss: 0.8213 - val_nd: 0.9145\n",
            "Epoch 5/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 1.0348 - nd: 0.9592 - val_loss: 0.8196 - val_nd: 0.9137\n",
            "Epoch 6/200\n",
            "18/18 [==============================] - 0s 8ms/step - loss: 1.0232 - nd: 0.9372 - val_loss: 0.8165 - val_nd: 0.9078\n",
            "Epoch 7/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 1.0144 - nd: 0.9233 - val_loss: 0.8140 - val_nd: 0.9127\n",
            "Epoch 8/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 1.0097 - nd: 0.9153 - val_loss: 0.8082 - val_nd: 0.8987\n",
            "Epoch 9/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 1.0024 - nd: 0.9021 - val_loss: 0.7990 - val_nd: 0.8920\n",
            "Epoch 10/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.9896 - nd: 0.8843 - val_loss: 0.7781 - val_nd: 0.8622\n",
            "Epoch 11/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.9655 - nd: 0.8573 - val_loss: 0.7335 - val_nd: 0.7997\n",
            "Epoch 12/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.9282 - nd: 0.8145 - val_loss: 0.6375 - val_nd: 0.6629\n",
            "Epoch 13/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.8799 - nd: 0.7546 - val_loss: 0.5119 - val_nd: 0.4906\n",
            "Epoch 14/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.8093 - nd: 0.6847 - val_loss: 0.4021 - val_nd: 0.3695\n",
            "Epoch 15/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.7717 - nd: 0.6450 - val_loss: 0.3399 - val_nd: 0.3332\n",
            "Epoch 16/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.7315 - nd: 0.6025 - val_loss: 0.2946 - val_nd: 0.2942\n",
            "Epoch 17/200\n",
            "18/18 [==============================] - 0s 8ms/step - loss: 0.7098 - nd: 0.5777 - val_loss: 0.2740 - val_nd: 0.2770\n",
            "Epoch 18/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.6835 - nd: 0.5584 - val_loss: 0.2629 - val_nd: 0.2819\n",
            "Epoch 19/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.6776 - nd: 0.5436 - val_loss: 0.2710 - val_nd: 0.2924\n",
            "Epoch 20/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.6538 - nd: 0.5264 - val_loss: 0.2347 - val_nd: 0.2538\n",
            "Epoch 21/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.6508 - nd: 0.5184 - val_loss: 0.2441 - val_nd: 0.2735\n",
            "Epoch 22/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.6589 - nd: 0.5189 - val_loss: 0.2432 - val_nd: 0.2701\n",
            "Epoch 23/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.6440 - nd: 0.5003 - val_loss: 0.2694 - val_nd: 0.2916\n",
            "Epoch 24/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.6512 - nd: 0.5102 - val_loss: 0.2244 - val_nd: 0.2607\n",
            "Epoch 25/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.6387 - nd: 0.4986 - val_loss: 0.2260 - val_nd: 0.2682\n",
            "Epoch 26/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.6147 - nd: 0.4893 - val_loss: 0.2310 - val_nd: 0.2812\n",
            "Epoch 27/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.6177 - nd: 0.4902 - val_loss: 0.2121 - val_nd: 0.2474\n",
            "Epoch 28/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.6147 - nd: 0.4888 - val_loss: 0.2142 - val_nd: 0.2473\n",
            "Epoch 29/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.6194 - nd: 0.4890 - val_loss: 0.2162 - val_nd: 0.2504\n",
            "Epoch 30/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.6123 - nd: 0.4746 - val_loss: 0.2021 - val_nd: 0.2330\n",
            "Epoch 31/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.6095 - nd: 0.4693 - val_loss: 0.2254 - val_nd: 0.2709\n",
            "Epoch 32/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.6040 - nd: 0.4698 - val_loss: 0.1906 - val_nd: 0.2319\n",
            "Epoch 33/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.6002 - nd: 0.4692 - val_loss: 0.1971 - val_nd: 0.2540\n",
            "Epoch 34/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.6056 - nd: 0.4622 - val_loss: 0.2317 - val_nd: 0.2608\n",
            "Epoch 35/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5937 - nd: 0.4664 - val_loss: 0.2723 - val_nd: 0.3271\n",
            "Epoch 36/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.5994 - nd: 0.4739 - val_loss: 0.2889 - val_nd: 0.3518\n",
            "Epoch 37/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.6065 - nd: 0.4749 - val_loss: 0.2136 - val_nd: 0.2706\n",
            "Epoch 38/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5973 - nd: 0.4629 - val_loss: 0.2244 - val_nd: 0.2534\n",
            "Epoch 39/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5810 - nd: 0.4507 - val_loss: 0.2178 - val_nd: 0.2447\n",
            "Epoch 40/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.5864 - nd: 0.4530 - val_loss: 0.2194 - val_nd: 0.2612\n",
            "Epoch 41/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5871 - nd: 0.4508 - val_loss: 0.2130 - val_nd: 0.2414\n",
            "Epoch 42/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.5988 - nd: 0.4515 - val_loss: 0.2226 - val_nd: 0.2799\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 74431\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                   epoch 41\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                    loss 0.5988456606864929\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                   _step 42\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                val_loss 0.22258460521697998\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                      nd 0.45149415731430054\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              _timestamp 1650176638.853729\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                _runtime 5063.181988954544\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                  val_nd 0.27988770604133606\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           best_val_loss 0.19060374796390533\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              best_epoch 31\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             best-nd_std 0.020227993639544892\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     best-nrmse_mean_std 0.02481771053975441\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       best-nrmse_maxmin 0.033741674056575036\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   best-nrmse_maxmin_std 0.002589712865121773\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         best-nrmse_mean 0.3233528748073949\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              best-nd_95 0.0011894655007671842\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      best-nrmse_mean_95 0.0014593543492793077\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    best-nrmse_maxmin_95 0.000152282730796063\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 best-nd 0.27847097340368493\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 7 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced glamorous-paper-348: https://app.wandb.ai/matintavakoli/Interpreting_TS/runs/koac1b9d\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.9.7\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Wandb version 0.12.14 is available!  To upgrade, please run:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in wandb/run-20220417_062404-m8ls9qs1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mfearless-leaf-349\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://app.wandb.ai/matintavakoli/Interpreting_TS\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://app.wandb.ai/matintavakoli/Interpreting_TS/runs/m8ls9qs1\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb off` to turn off syncing.\n",
            "\n",
            "/usr/local/envs/eval-metric/lib/python3.7/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  return f(**kwargs)\n",
            "/usr/local/envs/eval-metric/lib/python3.7/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  return f(**kwargs)\n",
            "  4%|█▍                                     | 101/2660 [00:00<00:10, 233.84it/s]\n",
            "(9090, 30, 10) (9090, 6) (1111, 30, 10) (1111, 6)\n",
            "(9090, 30, 10) (9090, 6) (1111, 30, 10) (1111, 6)\n",
            "Model: \"sequential_65\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm_60 (LSTM)               (None, 30, 64)            19200     \n",
            "_________________________________________________________________\n",
            "lstm_61 (LSTM)               (None, 30, 64)            33024     \n",
            "_________________________________________________________________\n",
            "lstm_62 (LSTM)               (None, 64)                33024     \n",
            "_________________________________________________________________\n",
            "dropout_95 (Dropout)         (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_128 (Dense)            (None, 6)                 390       \n",
            "=================================================================\n",
            "Total params: 85,638\n",
            "Trainable params: 85,638\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/200\n",
            "18/18 [==============================] - 1s 63ms/step - loss: 1.0062 - nd: 0.9087 - val_loss: 0.8168 - val_nd: 0.7864\n",
            "Epoch 2/200\n",
            "18/18 [==============================] - 0s 16ms/step - loss: 0.8311 - nd: 0.7096 - val_loss: 0.7244 - val_nd: 0.7022\n",
            "Epoch 3/200\n",
            "18/18 [==============================] - 0s 14ms/step - loss: 0.6734 - nd: 0.5809 - val_loss: 0.6947 - val_nd: 0.7660\n",
            "Epoch 4/200\n",
            "18/18 [==============================] - 0s 13ms/step - loss: 0.6413 - nd: 0.5263 - val_loss: 0.5869 - val_nd: 0.5941\n",
            "Epoch 5/200\n",
            "18/18 [==============================] - 0s 14ms/step - loss: 0.6024 - nd: 0.4832 - val_loss: 0.4748 - val_nd: 0.5043\n",
            "Epoch 6/200\n",
            "18/18 [==============================] - 0s 14ms/step - loss: 0.5926 - nd: 0.4776 - val_loss: 0.4738 - val_nd: 0.4914\n",
            "Epoch 7/200\n",
            "18/18 [==============================] - 0s 14ms/step - loss: 0.5752 - nd: 0.4386 - val_loss: 0.4080 - val_nd: 0.4197\n",
            "Epoch 8/200\n",
            "18/18 [==============================] - 0s 14ms/step - loss: 0.5397 - nd: 0.4172 - val_loss: 0.3980 - val_nd: 0.4282\n",
            "Epoch 9/200\n",
            "18/18 [==============================] - 0s 15ms/step - loss: 0.5239 - nd: 0.4035 - val_loss: 0.3916 - val_nd: 0.4245\n",
            "Epoch 10/200\n",
            "18/18 [==============================] - 0s 13ms/step - loss: 0.5265 - nd: 0.3898 - val_loss: 0.3874 - val_nd: 0.4406\n",
            "Epoch 11/200\n",
            "18/18 [==============================] - 0s 14ms/step - loss: 0.5170 - nd: 0.3792 - val_loss: 0.3443 - val_nd: 0.3792\n",
            "Epoch 12/200\n",
            "18/18 [==============================] - 0s 11ms/step - loss: 0.4976 - nd: 0.3769 - val_loss: 0.3450 - val_nd: 0.3966\n",
            "Epoch 13/200\n",
            "18/18 [==============================] - 0s 12ms/step - loss: 0.5027 - nd: 0.3761 - val_loss: 0.3562 - val_nd: 0.4069\n",
            "Epoch 14/200\n",
            "18/18 [==============================] - 0s 14ms/step - loss: 0.4964 - nd: 0.3584 - val_loss: 0.2975 - val_nd: 0.3434\n",
            "Epoch 15/200\n",
            "18/18 [==============================] - 0s 11ms/step - loss: 0.4809 - nd: 0.3454 - val_loss: 0.3426 - val_nd: 0.4019\n",
            "Epoch 16/200\n",
            "18/18 [==============================] - 0s 11ms/step - loss: 0.4875 - nd: 0.3445 - val_loss: 0.2991 - val_nd: 0.3709\n",
            "Epoch 17/200\n",
            "18/18 [==============================] - 0s 12ms/step - loss: 0.4853 - nd: 0.3424 - val_loss: 0.3291 - val_nd: 0.4134\n",
            "Epoch 18/200\n",
            "18/18 [==============================] - 0s 12ms/step - loss: 0.4799 - nd: 0.3327 - val_loss: 0.3671 - val_nd: 0.4220\n",
            "Epoch 19/200\n",
            "18/18 [==============================] - 0s 11ms/step - loss: 0.4629 - nd: 0.3263 - val_loss: 0.3504 - val_nd: 0.4316\n",
            "Epoch 20/200\n",
            "18/18 [==============================] - 0s 12ms/step - loss: 0.4781 - nd: 0.3323 - val_loss: 0.3258 - val_nd: 0.3945\n",
            "Epoch 21/200\n",
            "18/18 [==============================] - 0s 12ms/step - loss: 0.4663 - nd: 0.3271 - val_loss: 0.3132 - val_nd: 0.3762\n",
            "Epoch 22/200\n",
            "18/18 [==============================] - 0s 11ms/step - loss: 0.4620 - nd: 0.3179 - val_loss: 0.3192 - val_nd: 0.3795\n",
            "Epoch 23/200\n",
            "18/18 [==============================] - 0s 14ms/step - loss: 0.4715 - nd: 0.3157 - val_loss: 0.2956 - val_nd: 0.3623\n",
            "Epoch 24/200\n",
            "18/18 [==============================] - 0s 12ms/step - loss: 0.4579 - nd: 0.3134 - val_loss: 0.2964 - val_nd: 0.3511\n",
            "Epoch 25/200\n",
            "18/18 [==============================] - 0s 12ms/step - loss: 0.4591 - nd: 0.3098 - val_loss: 0.3212 - val_nd: 0.3691\n",
            "Epoch 26/200\n",
            "18/18 [==============================] - 0s 12ms/step - loss: 0.4544 - nd: 0.3124 - val_loss: 0.3311 - val_nd: 0.3644\n",
            "Epoch 27/200\n",
            "18/18 [==============================] - 0s 11ms/step - loss: 0.4434 - nd: 0.3162 - val_loss: 0.2988 - val_nd: 0.3373\n",
            "Epoch 28/200\n",
            "18/18 [==============================] - 0s 11ms/step - loss: 0.4610 - nd: 0.3181 - val_loss: 0.3093 - val_nd: 0.3545\n",
            "Epoch 29/200\n",
            "18/18 [==============================] - 0s 11ms/step - loss: 0.4464 - nd: 0.3150 - val_loss: 0.3044 - val_nd: 0.3305\n",
            "Epoch 30/200\n",
            "18/18 [==============================] - 0s 12ms/step - loss: 0.4501 - nd: 0.3070 - val_loss: 0.3005 - val_nd: 0.3296\n",
            "Epoch 31/200\n",
            "18/18 [==============================] - 0s 12ms/step - loss: 0.4379 - nd: 0.2971 - val_loss: 0.3176 - val_nd: 0.3517\n",
            "Epoch 32/200\n",
            "18/18 [==============================] - 0s 12ms/step - loss: 0.4462 - nd: 0.2951 - val_loss: 0.3115 - val_nd: 0.3567\n",
            "Epoch 33/200\n",
            "18/18 [==============================] - 0s 12ms/step - loss: 0.4326 - nd: 0.2997 - val_loss: 0.3004 - val_nd: 0.3252\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 74614\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                   epoch 32\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                    loss 0.4325607717037201\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                   _step 33\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                val_loss 0.300403892993927\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                      nd 0.2996672987937927\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              _timestamp 1650176660.0612416\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                _runtime 5084.389501571655\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                  val_nd 0.3251659572124481\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           best_val_loss 0.2956188917160034\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              best_epoch 22\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             best-nd_std 0.043959294711375135\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     best-nrmse_mean_std 0.03578713934545163\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       best-nrmse_maxmin 0.052013995778375885\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   best-nrmse_maxmin_std 0.003734366029467711\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         best-nrmse_mean 0.49846000636948684\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              best-nd_95 0.002584935779049142\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      best-nrmse_mean_95 0.002104389015593953\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    best-nrmse_maxmin_95 0.00021959170239232436\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 best-nd 0.42875713985258646\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 7 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced fearless-leaf-349: https://app.wandb.ai/matintavakoli/Interpreting_TS/runs/m8ls9qs1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.9.7\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Wandb version 0.12.14 is available!  To upgrade, please run:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in wandb/run-20220417_062424-7ak1p6p7\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33msplendid-blaze-350\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://app.wandb.ai/matintavakoli/Interpreting_TS\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://app.wandb.ai/matintavakoli/Interpreting_TS/runs/7ak1p6p7\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb off` to turn off syncing.\n",
            "\n",
            "/usr/local/envs/eval-metric/lib/python3.7/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  return f(**kwargs)\n",
            "/usr/local/envs/eval-metric/lib/python3.7/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  return f(**kwargs)\n",
            "  4%|█▍                                     | 101/2660 [00:00<00:11, 226.75it/s]\n",
            "(9090, 30, 10) (9090, 6) (1111, 30, 10) (1111, 6)\n",
            "(9090, 30, 10) (9090, 6) (1111, 30, 10) (1111, 6)\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 74771\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                   _step 2\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                    loss 0.005111530080317694\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                      nd 0.12037438854469278\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              _timestamp 1650176916.168782\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                _runtime 5340.497041940689\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                val_loss 0.02019852487416217\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                  val_nd 0.0993370269808837\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             best-nd_std 0.013808232786836681\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     best-nrmse_mean_std 0.03866530782761103\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       best-nrmse_maxmin 0.01977546374234121\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   best-nrmse_maxmin_std 0.004034701144356596\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         best-nrmse_mean 0.18951202720451224\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              best-nd_95 0.0008119646871153592\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      best-nrmse_mean_95 0.0022736337847949527\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    best-nrmse_maxmin_95 0.000237252290199258\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 best-nd 0.11699093495320378\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 8 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced splendid-blaze-350: https://app.wandb.ai/matintavakoli/Interpreting_TS/runs/7ak1p6p7\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.9.7\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Wandb version 0.12.14 is available!  To upgrade, please run:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in wandb/run-20220417_062839-1i0vnwm8\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mdevoted-river-351\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://app.wandb.ai/matintavakoli/Interpreting_TS\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://app.wandb.ai/matintavakoli/Interpreting_TS/runs/1i0vnwm8\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb off` to turn off syncing.\n",
            "\n",
            "/usr/local/envs/eval-metric/lib/python3.7/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  return f(**kwargs)\n",
            "/usr/local/envs/eval-metric/lib/python3.7/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  return f(**kwargs)\n",
            "  4%|█▍                                     | 101/2660 [00:00<00:11, 228.56it/s]\n",
            "(9090, 30, 10) (9090, 6) (1111, 30, 10) (1111, 6)\n",
            "(9090, 30, 10) (9090, 6) (1111, 30, 10) (1111, 6)\n",
            "Model: \"sequential_66\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "reshape_33 (Reshape)         (None, 300)               0         \n",
            "_________________________________________________________________\n",
            "dense_129 (Dense)            (None, 128)               38528     \n",
            "_________________________________________________________________\n",
            "dropout_96 (Dropout)         (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_130 (Dense)            (None, 128)               16512     \n",
            "_________________________________________________________________\n",
            "dropout_97 (Dropout)         (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_131 (Dense)            (None, 128)               16512     \n",
            "_________________________________________________________________\n",
            "dropout_98 (Dropout)         (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_132 (Dense)            (None, 6)                 774       \n",
            "=================================================================\n",
            "Total params: 72,326\n",
            "Trainable params: 72,326\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/200\n",
            "18/18 [==============================] - 0s 10ms/step - loss: 1.0325 - nd: 0.9314 - val_loss: 0.8189 - val_nd: 0.8957\n",
            "Epoch 2/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.9842 - nd: 0.8790 - val_loss: 0.7897 - val_nd: 0.8602\n",
            "Epoch 3/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.9451 - nd: 0.8103 - val_loss: 0.6971 - val_nd: 0.7798\n",
            "Epoch 4/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.7999 - nd: 0.6344 - val_loss: 0.4185 - val_nd: 0.4355\n",
            "Epoch 5/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.5812 - nd: 0.3780 - val_loss: 0.2251 - val_nd: 0.2178\n",
            "Epoch 6/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.4963 - nd: 0.3046 - val_loss: 0.1891 - val_nd: 0.1779\n",
            "Epoch 7/200\n",
            "18/18 [==============================] - 0s 9ms/step - loss: 0.4532 - nd: 0.2682 - val_loss: 0.1668 - val_nd: 0.1683\n",
            "Epoch 8/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.4500 - nd: 0.2709 - val_loss: 0.1502 - val_nd: 0.1474\n",
            "Epoch 9/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.4510 - nd: 0.3064 - val_loss: 0.2120 - val_nd: 0.2293\n",
            "Epoch 10/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4450 - nd: 0.2816 - val_loss: 0.1686 - val_nd: 0.1627\n",
            "Epoch 11/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4319 - nd: 0.2423 - val_loss: 0.1913 - val_nd: 0.2023\n",
            "Epoch 12/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4183 - nd: 0.2228 - val_loss: 0.1665 - val_nd: 0.1444\n",
            "Epoch 13/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4006 - nd: 0.2061 - val_loss: 0.1655 - val_nd: 0.1384\n",
            "Epoch 14/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.4026 - nd: 0.1998 - val_loss: 0.1677 - val_nd: 0.1393\n",
            "Epoch 15/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.3981 - nd: 0.2150 - val_loss: 0.2222 - val_nd: 0.2274\n",
            "Epoch 16/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.4165 - nd: 0.2354 - val_loss: 0.1660 - val_nd: 0.1423\n",
            "Epoch 17/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4155 - nd: 0.1967 - val_loss: 0.1725 - val_nd: 0.1372\n",
            "Epoch 18/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.3954 - nd: 0.1938 - val_loss: 0.1705 - val_nd: 0.1321\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 74865\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                   epoch 17\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                    loss 0.39542216062545776\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                   _step 18\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                val_loss 0.17052529752254486\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                      nd 0.1937996745109558\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              _timestamp 1650176924.425284\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                _runtime 5348.75354385376\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                  val_nd 0.13214406371116638\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           best_val_loss 0.15024636685848236\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              best_epoch 7\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             best-nd_std 0.015298414784723243\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     best-nrmse_mean_std 0.03790860278294904\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       best-nrmse_maxmin 0.027070744533747824\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   best-nrmse_maxmin_std 0.003955739437307725\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         best-nrmse_mean 0.2594240894357228\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              best-nd_95 0.0008995917700547753\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      best-nrmse_mean_95 0.002229137303289129\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    best-nrmse_maxmin_95 0.00023260910965995318\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 best-nd 0.19551415821737916\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 7 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced devoted-river-351: https://app.wandb.ai/matintavakoli/Interpreting_TS/runs/1i0vnwm8\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.9.7\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Wandb version 0.12.14 is available!  To upgrade, please run:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in wandb/run-20220417_062849-3o7p6ile\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mstill-microwave-352\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://app.wandb.ai/matintavakoli/Interpreting_TS\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://app.wandb.ai/matintavakoli/Interpreting_TS/runs/3o7p6ile\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb off` to turn off syncing.\n",
            "\n",
            "/usr/local/envs/eval-metric/lib/python3.7/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  return f(**kwargs)\n",
            "/usr/local/envs/eval-metric/lib/python3.7/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  return f(**kwargs)\n",
            "  4%|█▍                                     | 101/2660 [00:00<00:10, 233.11it/s]\n",
            "(9090, 30, 10) (9090, 6) (1111, 30, 10) (1111, 6)\n",
            "(9090, 30, 10) (9090, 6) (1111, 30, 10) (1111, 6)\n",
            "Model: \"sequential_67\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm_63 (LSTM)               (None, 30, 128)           71168     \n",
            "_________________________________________________________________\n",
            "lstm_64 (LSTM)               (None, 30, 128)           131584    \n",
            "_________________________________________________________________\n",
            "lstm_65 (LSTM)               (None, 128)               131584    \n",
            "_________________________________________________________________\n",
            "dropout_99 (Dropout)         (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_133 (Dense)            (None, 6)                 774       \n",
            "=================================================================\n",
            "Total params: 335,110\n",
            "Trainable params: 335,110\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/200\n",
            "18/18 [==============================] - 1s 70ms/step - loss: 0.8977 - nd: 0.7744 - val_loss: 0.6814 - val_nd: 0.7402\n",
            "Epoch 2/200\n",
            "18/18 [==============================] - 0s 21ms/step - loss: 0.6206 - nd: 0.5075 - val_loss: 0.4145 - val_nd: 0.4458\n",
            "Epoch 3/200\n",
            "18/18 [==============================] - 0s 23ms/step - loss: 0.4586 - nd: 0.3466 - val_loss: 0.3395 - val_nd: 0.3575\n",
            "Epoch 4/200\n",
            "18/18 [==============================] - 0s 21ms/step - loss: 0.4303 - nd: 0.2861 - val_loss: 0.3046 - val_nd: 0.3227\n",
            "Epoch 5/200\n",
            "18/18 [==============================] - 0s 22ms/step - loss: 0.4229 - nd: 0.2653 - val_loss: 0.2680 - val_nd: 0.2629\n",
            "Epoch 6/200\n",
            "18/18 [==============================] - 0s 23ms/step - loss: 0.4122 - nd: 0.2470 - val_loss: 0.2463 - val_nd: 0.2404\n",
            "Epoch 7/200\n",
            "18/18 [==============================] - 0s 22ms/step - loss: 0.4238 - nd: 0.2532 - val_loss: 0.2308 - val_nd: 0.2412\n",
            "Epoch 8/200\n",
            "18/18 [==============================] - 0s 22ms/step - loss: 0.4147 - nd: 0.2277 - val_loss: 0.2065 - val_nd: 0.1727\n",
            "Epoch 9/200\n",
            "18/18 [==============================] - 0s 22ms/step - loss: 0.3881 - nd: 0.2093 - val_loss: 0.1760 - val_nd: 0.1878\n",
            "Epoch 10/200\n",
            "18/18 [==============================] - 0s 19ms/step - loss: 0.3823 - nd: 0.1992 - val_loss: 0.1790 - val_nd: 0.1858\n",
            "Epoch 11/200\n",
            "18/18 [==============================] - 0s 19ms/step - loss: 0.3917 - nd: 0.2121 - val_loss: 0.2156 - val_nd: 0.1960\n",
            "Epoch 12/200\n",
            "18/18 [==============================] - 0s 22ms/step - loss: 0.3886 - nd: 0.2015 - val_loss: 0.1722 - val_nd: 0.2002\n",
            "Epoch 13/200\n",
            "18/18 [==============================] - 0s 18ms/step - loss: 0.3873 - nd: 0.2152 - val_loss: 0.2563 - val_nd: 0.2820\n",
            "Epoch 14/200\n",
            "18/18 [==============================] - 0s 22ms/step - loss: 0.3602 - nd: 0.2140 - val_loss: 0.1465 - val_nd: 0.1503\n",
            "Epoch 15/200\n",
            "18/18 [==============================] - 0s 19ms/step - loss: 0.3596 - nd: 0.1937 - val_loss: 0.2077 - val_nd: 0.1763\n",
            "Epoch 16/200\n",
            "18/18 [==============================] - 0s 19ms/step - loss: 0.3732 - nd: 0.1966 - val_loss: 0.1697 - val_nd: 0.1842\n",
            "Epoch 17/200\n",
            "18/18 [==============================] - 0s 18ms/step - loss: 0.3832 - nd: 0.2024 - val_loss: 0.1678 - val_nd: 0.1804\n",
            "Epoch 18/200\n",
            "18/18 [==============================] - 0s 20ms/step - loss: 0.3755 - nd: 0.1961 - val_loss: 0.1570 - val_nd: 0.1506\n",
            "Epoch 19/200\n",
            "18/18 [==============================] - 0s 19ms/step - loss: 0.3769 - nd: 0.2053 - val_loss: 0.1502 - val_nd: 0.1550\n",
            "Epoch 20/200\n",
            "18/18 [==============================] - 0s 20ms/step - loss: 0.3677 - nd: 0.2053 - val_loss: 0.1671 - val_nd: 0.1570\n",
            "Epoch 21/200\n",
            "18/18 [==============================] - 0s 21ms/step - loss: 0.3700 - nd: 0.1826 - val_loss: 0.1378 - val_nd: 0.1572\n",
            "Epoch 22/200\n",
            "18/18 [==============================] - 0s 23ms/step - loss: 0.3431 - nd: 0.1646 - val_loss: 0.1371 - val_nd: 0.1555\n",
            "Epoch 23/200\n",
            "18/18 [==============================] - 0s 22ms/step - loss: 0.3522 - nd: 0.1616 - val_loss: 0.1238 - val_nd: 0.1267\n",
            "Epoch 24/200\n",
            "18/18 [==============================] - 0s 19ms/step - loss: 0.3577 - nd: 0.1780 - val_loss: 0.2012 - val_nd: 0.2341\n",
            "Epoch 25/200\n",
            "18/18 [==============================] - 0s 20ms/step - loss: 0.3493 - nd: 0.1912 - val_loss: 0.1770 - val_nd: 0.2104\n",
            "Epoch 26/200\n",
            "18/18 [==============================] - 0s 19ms/step - loss: 0.3624 - nd: 0.1965 - val_loss: 0.1494 - val_nd: 0.1687\n",
            "Epoch 27/200\n",
            "18/18 [==============================] - 0s 19ms/step - loss: 0.3534 - nd: 0.1878 - val_loss: 0.1323 - val_nd: 0.1248\n",
            "Epoch 28/200\n",
            "18/18 [==============================] - 0s 22ms/step - loss: 0.3472 - nd: 0.1686 - val_loss: 0.1231 - val_nd: 0.1311\n",
            "Epoch 29/200\n",
            "18/18 [==============================] - 0s 21ms/step - loss: 0.3388 - nd: 0.1624 - val_loss: 0.1174 - val_nd: 0.1263\n",
            "Epoch 30/200\n",
            "18/18 [==============================] - 0s 20ms/step - loss: 0.3444 - nd: 0.1558 - val_loss: 0.1513 - val_nd: 0.1759\n",
            "Epoch 31/200\n",
            "18/18 [==============================] - 0s 18ms/step - loss: 0.3459 - nd: 0.1656 - val_loss: 0.1304 - val_nd: 0.1407\n",
            "Epoch 32/200\n",
            "18/18 [==============================] - 0s 19ms/step - loss: 0.3467 - nd: 0.1571 - val_loss: 0.1253 - val_nd: 0.1353\n",
            "Epoch 33/200\n",
            "18/18 [==============================] - 0s 18ms/step - loss: 0.3490 - nd: 0.1550 - val_loss: 0.1635 - val_nd: 0.1500\n",
            "Epoch 34/200\n",
            "18/18 [==============================] - 0s 20ms/step - loss: 0.3513 - nd: 0.1687 - val_loss: 0.1598 - val_nd: 0.1872\n",
            "Epoch 35/200\n",
            "18/18 [==============================] - 0s 19ms/step - loss: 0.3685 - nd: 0.1884 - val_loss: 0.1477 - val_nd: 0.1539\n",
            "Epoch 36/200\n",
            "18/18 [==============================] - 0s 19ms/step - loss: 0.3539 - nd: 0.1715 - val_loss: 0.1200 - val_nd: 0.1193\n",
            "Epoch 37/200\n",
            "18/18 [==============================] - 0s 21ms/step - loss: 0.3414 - nd: 0.1605 - val_loss: 0.1157 - val_nd: 0.1129\n",
            "Epoch 38/200\n",
            "18/18 [==============================] - 0s 23ms/step - loss: 0.3449 - nd: 0.1604 - val_loss: 0.1156 - val_nd: 0.1123\n",
            "Epoch 39/200\n",
            "18/18 [==============================] - 0s 19ms/step - loss: 0.3397 - nd: 0.1705 - val_loss: 0.2069 - val_nd: 0.2522\n",
            "Epoch 40/200\n",
            "18/18 [==============================] - 0s 19ms/step - loss: 0.3483 - nd: 0.1687 - val_loss: 0.1547 - val_nd: 0.1508\n",
            "Epoch 41/200\n",
            "18/18 [==============================] - 0s 19ms/step - loss: 0.3549 - nd: 0.1549 - val_loss: 0.1544 - val_nd: 0.1553\n",
            "Epoch 42/200\n",
            "18/18 [==============================] - 0s 19ms/step - loss: 0.3502 - nd: 0.1547 - val_loss: 0.1304 - val_nd: 0.1290\n",
            "Epoch 43/200\n",
            "18/18 [==============================] - 0s 21ms/step - loss: 0.3290 - nd: 0.1479 - val_loss: 0.1098 - val_nd: 0.1086\n",
            "Epoch 44/200\n",
            "18/18 [==============================] - 0s 19ms/step - loss: 0.3218 - nd: 0.1451 - val_loss: 0.1156 - val_nd: 0.1122\n",
            "Epoch 45/200\n",
            "18/18 [==============================] - 0s 19ms/step - loss: 0.3310 - nd: 0.1444 - val_loss: 0.1416 - val_nd: 0.1485\n",
            "Epoch 46/200\n",
            "18/18 [==============================] - 0s 19ms/step - loss: 0.3415 - nd: 0.1529 - val_loss: 0.1194 - val_nd: 0.1139\n",
            "Epoch 47/200\n",
            "18/18 [==============================] - 0s 20ms/step - loss: 0.3509 - nd: 0.1562 - val_loss: 0.1251 - val_nd: 0.1373\n",
            "Epoch 48/200\n",
            "18/18 [==============================] - 0s 19ms/step - loss: 0.3414 - nd: 0.1488 - val_loss: 0.1312 - val_nd: 0.1386\n",
            "Epoch 49/200\n",
            "18/18 [==============================] - 0s 19ms/step - loss: 0.3447 - nd: 0.1441 - val_loss: 0.1205 - val_nd: 0.1177\n",
            "Epoch 50/200\n",
            "18/18 [==============================] - 0s 19ms/step - loss: 0.3447 - nd: 0.1453 - val_loss: 0.1145 - val_nd: 0.1097\n",
            "Epoch 51/200\n",
            "18/18 [==============================] - 0s 19ms/step - loss: 0.3221 - nd: 0.1382 - val_loss: 0.1209 - val_nd: 0.1088\n",
            "Epoch 52/200\n",
            "18/18 [==============================] - 0s 19ms/step - loss: 0.3276 - nd: 0.1391 - val_loss: 0.1400 - val_nd: 0.1162\n",
            "Epoch 53/200\n",
            "18/18 [==============================] - 0s 19ms/step - loss: 0.3298 - nd: 0.1458 - val_loss: 0.1324 - val_nd: 0.1241\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 74976\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                   epoch 52\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                    loss 0.3298076093196869\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                   _step 53\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                val_loss 0.13240234553813934\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                      nd 0.14584006369113922\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              _timestamp 1650176958.131225\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                _runtime 5382.459485054016\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                  val_nd 0.12407660484313965\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           best_val_loss 0.10978832095861435\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              best_epoch 42\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             best-nd_std 0.021211088920496515\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     best-nrmse_mean_std 0.01793239526043363\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       best-nrmse_maxmin 0.01953187948247403\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   best-nrmse_maxmin_std 0.0018712344409853529\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         best-nrmse_mean 0.18717771295105168\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              best-nd_95 0.0012472743938040565\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      best-nrmse_mean_95 0.001054477566509986\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    best-nrmse_maxmin_95 0.00011003408697183683\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 best-nd 0.13533753116746122\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 7 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced still-microwave-352: https://app.wandb.ai/matintavakoli/Interpreting_TS/runs/3o7p6ile\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.9.7\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Wandb version 0.12.14 is available!  To upgrade, please run:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in wandb/run-20220417_062923-3nqbl5uu\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mclean-cherry-353\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://app.wandb.ai/matintavakoli/Interpreting_TS\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://app.wandb.ai/matintavakoli/Interpreting_TS/runs/3nqbl5uu\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb off` to turn off syncing.\n",
            "\n",
            "/usr/local/envs/eval-metric/lib/python3.7/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  return f(**kwargs)\n",
            "/usr/local/envs/eval-metric/lib/python3.7/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  return f(**kwargs)\n",
            "  4%|█▍                                     | 101/2660 [00:00<00:11, 216.03it/s]\n",
            "(9090, 30, 10) (9090, 6) (1111, 30, 10) (1111, 6)\n",
            "(9090, 30, 10) (9090, 6) (1111, 30, 10) (1111, 6)\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 75196\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                   _step 2\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                    loss 0.005115541285338472\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                      nd 0.1203764173657505\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              _timestamp 1650177219.3768609\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                _runtime 5643.705120801926\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                val_loss 0.020238968985170937\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                  val_nd 0.09920215786569062\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             best-nd_std 0.013694443711933511\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     best-nrmse_mean_std 0.038977082770130254\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       best-nrmse_maxmin 0.01979911055967519\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   best-nrmse_maxmin_std 0.0040672346682838435\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         best-nrmse_mean 0.18973863914890385\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              best-nd_95 0.00080527355494608\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      best-nrmse_mean_95 0.002291967068102166\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    best-nrmse_maxmin_95 0.00023916535706191474\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 best-nd 0.11667802664088264\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 8 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced clean-cherry-353: https://app.wandb.ai/matintavakoli/Interpreting_TS/runs/3nqbl5uu\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.9.7\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Wandb version 0.12.14 is available!  To upgrade, please run:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in wandb/run-20220417_063343-1t43stoq\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mearnest-dust-354\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://app.wandb.ai/matintavakoli/Interpreting_TS\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://app.wandb.ai/matintavakoli/Interpreting_TS/runs/1t43stoq\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb off` to turn off syncing.\n",
            "\n",
            "/usr/local/envs/eval-metric/lib/python3.7/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  return f(**kwargs)\n",
            "/usr/local/envs/eval-metric/lib/python3.7/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  return f(**kwargs)\n",
            "  4%|█▍                                     | 101/2660 [00:00<00:10, 236.14it/s]\n",
            "(9090, 30, 10) (9090, 6) (1111, 30, 10) (1111, 6)\n",
            "(9090, 30, 10) (9090, 6) (1111, 30, 10) (1111, 6)\n",
            "Model: \"sequential_68\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "reshape_34 (Reshape)         (None, 300)               0         \n",
            "_________________________________________________________________\n",
            "dense_134 (Dense)            (None, 128)               38528     \n",
            "_________________________________________________________________\n",
            "dropout_100 (Dropout)        (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_135 (Dense)            (None, 128)               16512     \n",
            "_________________________________________________________________\n",
            "dropout_101 (Dropout)        (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_136 (Dense)            (None, 128)               16512     \n",
            "_________________________________________________________________\n",
            "dropout_102 (Dropout)        (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_137 (Dense)            (None, 6)                 774       \n",
            "=================================================================\n",
            "Total params: 72,326\n",
            "Trainable params: 72,326\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/200\n",
            "18/18 [==============================] - 0s 11ms/step - loss: 1.0920 - nd: 1.0400 - val_loss: 0.8211 - val_nd: 0.9298\n",
            "Epoch 2/200\n",
            "18/18 [==============================] - 0s 8ms/step - loss: 1.0412 - nd: 0.9639 - val_loss: 0.7982 - val_nd: 0.8549\n",
            "Epoch 3/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.9983 - nd: 0.9007 - val_loss: 0.7451 - val_nd: 0.8218\n",
            "Epoch 4/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.9108 - nd: 0.8009 - val_loss: 0.5854 - val_nd: 0.6451\n",
            "Epoch 5/200\n",
            "18/18 [==============================] - 0s 8ms/step - loss: 0.7600 - nd: 0.6277 - val_loss: 0.3340 - val_nd: 0.3320\n",
            "Epoch 6/200\n",
            "18/18 [==============================] - 0s 8ms/step - loss: 0.6201 - nd: 0.5060 - val_loss: 0.2245 - val_nd: 0.2280\n",
            "Epoch 7/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.5661 - nd: 0.4407 - val_loss: 0.1917 - val_nd: 0.2147\n",
            "Epoch 8/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.5319 - nd: 0.3985 - val_loss: 0.1731 - val_nd: 0.1937\n",
            "Epoch 9/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.4928 - nd: 0.3593 - val_loss: 0.1653 - val_nd: 0.1848\n",
            "Epoch 10/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4882 - nd: 0.3482 - val_loss: 0.1791 - val_nd: 0.1921\n",
            "Epoch 11/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4735 - nd: 0.3499 - val_loss: 0.2630 - val_nd: 0.3186\n",
            "Epoch 12/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4849 - nd: 0.3450 - val_loss: 0.1992 - val_nd: 0.1785\n",
            "Epoch 13/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4663 - nd: 0.3216 - val_loss: 0.1834 - val_nd: 0.1847\n",
            "Epoch 14/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.4668 - nd: 0.3144 - val_loss: 0.1915 - val_nd: 0.1825\n",
            "Epoch 15/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4648 - nd: 0.3105 - val_loss: 0.1800 - val_nd: 0.1567\n",
            "Epoch 16/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4634 - nd: 0.3142 - val_loss: 0.1989 - val_nd: 0.1737\n",
            "Epoch 17/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4588 - nd: 0.3203 - val_loss: 0.2068 - val_nd: 0.1902\n",
            "Epoch 18/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4496 - nd: 0.2959 - val_loss: 0.1878 - val_nd: 0.1591\n",
            "Epoch 19/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4351 - nd: 0.2884 - val_loss: 0.1881 - val_nd: 0.1641\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 75292\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                   epoch 18\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                    loss 0.43512824177742004\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                   _step 19\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                val_loss 0.18806815147399902\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                      nd 0.2883581519126892\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              _timestamp 1650177227.9462562\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                _runtime 5652.274516105652\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                  val_nd 0.16408886015415192\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           best_val_loss 0.16528330743312836\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              best_epoch 8\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             best-nd_std 0.02178381110725249\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     best-nrmse_mean_std 0.03573001184512317\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       best-nrmse_maxmin 0.029306242279301745\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   best-nrmse_maxmin_std 0.0037284048098654484\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         best-nrmse_mean 0.28084728916903456\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              best-nd_95 0.0012809521423148321\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      best-nrmse_mean_95 0.0021010297506071913\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    best-nrmse_maxmin_95 0.00021924116515241115\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 best-nd 0.22234746185791548\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 7 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced earnest-dust-354: https://app.wandb.ai/matintavakoli/Interpreting_TS/runs/1t43stoq\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.9.7\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Wandb version 0.12.14 is available!  To upgrade, please run:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in wandb/run-20220417_063352-3c9j855e\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33myouthful-dust-355\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://app.wandb.ai/matintavakoli/Interpreting_TS\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://app.wandb.ai/matintavakoli/Interpreting_TS/runs/3c9j855e\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb off` to turn off syncing.\n",
            "\n",
            "/usr/local/envs/eval-metric/lib/python3.7/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  return f(**kwargs)\n",
            "/usr/local/envs/eval-metric/lib/python3.7/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  return f(**kwargs)\n",
            "  4%|█▍                                     | 101/2660 [00:00<00:11, 232.37it/s]\n",
            "(9090, 30, 10) (9090, 6) (1111, 30, 10) (1111, 6)\n",
            "(9090, 30, 10) (9090, 6) (1111, 30, 10) (1111, 6)\n",
            "Model: \"sequential_69\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm_66 (LSTM)               (None, 30, 128)           71168     \n",
            "_________________________________________________________________\n",
            "lstm_67 (LSTM)               (None, 30, 128)           131584    \n",
            "_________________________________________________________________\n",
            "lstm_68 (LSTM)               (None, 128)               131584    \n",
            "_________________________________________________________________\n",
            "dropout_103 (Dropout)        (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_138 (Dense)            (None, 6)                 774       \n",
            "=================================================================\n",
            "Total params: 335,110\n",
            "Trainable params: 335,110\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/200\n",
            "18/18 [==============================] - 1s 71ms/step - loss: 0.8990 - nd: 0.7941 - val_loss: 0.5836 - val_nd: 0.6660\n",
            "Epoch 2/200\n",
            "18/18 [==============================] - 0s 22ms/step - loss: 0.6931 - nd: 0.6035 - val_loss: 0.4088 - val_nd: 0.4478\n",
            "Epoch 3/200\n",
            "18/18 [==============================] - 0s 23ms/step - loss: 0.5696 - nd: 0.4729 - val_loss: 0.3120 - val_nd: 0.3091\n",
            "Epoch 4/200\n",
            "18/18 [==============================] - 0s 22ms/step - loss: 0.4983 - nd: 0.3852 - val_loss: 0.3016 - val_nd: 0.3034\n",
            "Epoch 5/200\n",
            "18/18 [==============================] - 0s 23ms/step - loss: 0.4650 - nd: 0.3346 - val_loss: 0.2640 - val_nd: 0.2715\n",
            "Epoch 6/200\n",
            "18/18 [==============================] - 0s 20ms/step - loss: 0.4616 - nd: 0.3316 - val_loss: 0.2671 - val_nd: 0.2644\n",
            "Epoch 7/200\n",
            "18/18 [==============================] - 0s 23ms/step - loss: 0.4522 - nd: 0.3067 - val_loss: 0.1947 - val_nd: 0.2212\n",
            "Epoch 8/200\n",
            "18/18 [==============================] - 0s 20ms/step - loss: 0.4348 - nd: 0.2968 - val_loss: 0.2481 - val_nd: 0.2605\n",
            "Epoch 9/200\n",
            "18/18 [==============================] - 0s 21ms/step - loss: 0.4416 - nd: 0.3044 - val_loss: 0.1961 - val_nd: 0.2197\n",
            "Epoch 10/200\n",
            "18/18 [==============================] - 0s 23ms/step - loss: 0.4238 - nd: 0.2784 - val_loss: 0.1891 - val_nd: 0.2099\n",
            "Epoch 11/200\n",
            "18/18 [==============================] - 0s 22ms/step - loss: 0.4361 - nd: 0.2801 - val_loss: 0.1803 - val_nd: 0.2073\n",
            "Epoch 12/200\n",
            "18/18 [==============================] - 0s 21ms/step - loss: 0.4215 - nd: 0.2678 - val_loss: 0.1862 - val_nd: 0.2072\n",
            "Epoch 13/200\n",
            "18/18 [==============================] - 0s 20ms/step - loss: 0.4234 - nd: 0.2649 - val_loss: 0.2203 - val_nd: 0.2397\n",
            "Epoch 14/200\n",
            "18/18 [==============================] - 0s 20ms/step - loss: 0.4147 - nd: 0.2613 - val_loss: 0.1929 - val_nd: 0.1970\n",
            "Epoch 15/200\n",
            "18/18 [==============================] - 0s 20ms/step - loss: 0.4121 - nd: 0.2561 - val_loss: 0.1844 - val_nd: 0.1900\n",
            "Epoch 16/200\n",
            "18/18 [==============================] - 0s 20ms/step - loss: 0.4062 - nd: 0.2548 - val_loss: 0.1909 - val_nd: 0.1970\n",
            "Epoch 17/200\n",
            "18/18 [==============================] - 0s 20ms/step - loss: 0.4050 - nd: 0.2483 - val_loss: 0.2355 - val_nd: 0.2416\n",
            "Epoch 18/200\n",
            "18/18 [==============================] - 0s 23ms/step - loss: 0.4161 - nd: 0.2631 - val_loss: 0.1734 - val_nd: 0.1936\n",
            "Epoch 19/200\n",
            "18/18 [==============================] - 0s 23ms/step - loss: 0.4118 - nd: 0.2491 - val_loss: 0.1636 - val_nd: 0.1780\n",
            "Epoch 20/200\n",
            "18/18 [==============================] - 0s 20ms/step - loss: 0.4003 - nd: 0.2435 - val_loss: 0.1890 - val_nd: 0.1907\n",
            "Epoch 21/200\n",
            "18/18 [==============================] - 0s 19ms/step - loss: 0.3910 - nd: 0.2318 - val_loss: 0.1689 - val_nd: 0.1823\n",
            "Epoch 22/200\n",
            "18/18 [==============================] - 0s 21ms/step - loss: 0.3895 - nd: 0.2282 - val_loss: 0.1802 - val_nd: 0.1836\n",
            "Epoch 23/200\n",
            "18/18 [==============================] - 0s 23ms/step - loss: 0.3867 - nd: 0.2321 - val_loss: 0.1601 - val_nd: 0.1669\n",
            "Epoch 24/200\n",
            "18/18 [==============================] - 0s 20ms/step - loss: 0.3917 - nd: 0.2304 - val_loss: 0.1842 - val_nd: 0.1887\n",
            "Epoch 25/200\n",
            "18/18 [==============================] - 0s 20ms/step - loss: 0.3986 - nd: 0.2355 - val_loss: 0.1608 - val_nd: 0.1796\n",
            "Epoch 26/200\n",
            "18/18 [==============================] - 0s 20ms/step - loss: 0.3901 - nd: 0.2497 - val_loss: 0.1649 - val_nd: 0.1830\n",
            "Epoch 27/200\n",
            "18/18 [==============================] - 0s 19ms/step - loss: 0.3987 - nd: 0.2277 - val_loss: 0.1805 - val_nd: 0.1766\n",
            "Epoch 28/200\n",
            "18/18 [==============================] - 0s 19ms/step - loss: 0.3835 - nd: 0.2205 - val_loss: 0.1614 - val_nd: 0.1698\n",
            "Epoch 29/200\n",
            "18/18 [==============================] - 0s 23ms/step - loss: 0.3954 - nd: 0.2136 - val_loss: 0.1597 - val_nd: 0.1647\n",
            "Epoch 30/200\n",
            "18/18 [==============================] - 0s 24ms/step - loss: 0.3845 - nd: 0.2134 - val_loss: 0.1556 - val_nd: 0.1710\n",
            "Epoch 31/200\n",
            "18/18 [==============================] - 0s 24ms/step - loss: 0.3881 - nd: 0.2133 - val_loss: 0.1553 - val_nd: 0.1631\n",
            "Epoch 32/200\n",
            "18/18 [==============================] - 0s 20ms/step - loss: 0.3762 - nd: 0.2111 - val_loss: 0.1587 - val_nd: 0.1600\n",
            "Epoch 33/200\n",
            "18/18 [==============================] - 0s 19ms/step - loss: 0.3812 - nd: 0.2171 - val_loss: 0.1857 - val_nd: 0.2018\n",
            "Epoch 34/200\n",
            "18/18 [==============================] - 0s 20ms/step - loss: 0.3723 - nd: 0.2241 - val_loss: 0.1692 - val_nd: 0.1828\n",
            "Epoch 35/200\n",
            "18/18 [==============================] - 0s 22ms/step - loss: 0.3852 - nd: 0.2165 - val_loss: 0.1382 - val_nd: 0.1356\n",
            "Epoch 36/200\n",
            "18/18 [==============================] - 0s 20ms/step - loss: 0.3764 - nd: 0.2103 - val_loss: 0.1515 - val_nd: 0.1526\n",
            "Epoch 37/200\n",
            "18/18 [==============================] - 0s 20ms/step - loss: 0.3825 - nd: 0.2189 - val_loss: 0.1943 - val_nd: 0.2088\n",
            "Epoch 38/200\n",
            "18/18 [==============================] - 0s 20ms/step - loss: 0.3840 - nd: 0.2318 - val_loss: 0.1637 - val_nd: 0.1751\n",
            "Epoch 39/200\n",
            "18/18 [==============================] - 0s 20ms/step - loss: 0.3897 - nd: 0.2259 - val_loss: 0.1519 - val_nd: 0.1646\n",
            "Epoch 40/200\n",
            "18/18 [==============================] - 0s 20ms/step - loss: 0.3893 - nd: 0.2150 - val_loss: 0.1571 - val_nd: 0.1592\n",
            "Epoch 41/200\n",
            "18/18 [==============================] - 0s 20ms/step - loss: 0.3775 - nd: 0.2050 - val_loss: 0.1410 - val_nd: 0.1487\n",
            "Epoch 42/200\n",
            "18/18 [==============================] - 0s 22ms/step - loss: 0.3737 - nd: 0.1969 - val_loss: 0.1323 - val_nd: 0.1300\n",
            "Epoch 43/200\n",
            "18/18 [==============================] - 0s 21ms/step - loss: 0.3655 - nd: 0.1952 - val_loss: 0.1453 - val_nd: 0.1455\n",
            "Epoch 44/200\n",
            "18/18 [==============================] - 0s 21ms/step - loss: 0.3739 - nd: 0.1917 - val_loss: 0.1345 - val_nd: 0.1394\n",
            "Epoch 45/200\n",
            "18/18 [==============================] - 0s 19ms/step - loss: 0.3632 - nd: 0.1972 - val_loss: 0.1586 - val_nd: 0.1538\n",
            "Epoch 46/200\n",
            "18/18 [==============================] - 0s 21ms/step - loss: 0.3666 - nd: 0.1933 - val_loss: 0.1630 - val_nd: 0.1562\n",
            "Epoch 47/200\n",
            "18/18 [==============================] - 0s 19ms/step - loss: 0.3817 - nd: 0.2025 - val_loss: 0.1438 - val_nd: 0.1336\n",
            "Epoch 48/200\n",
            "18/18 [==============================] - 0s 20ms/step - loss: 0.3752 - nd: 0.2000 - val_loss: 0.1500 - val_nd: 0.1534\n",
            "Epoch 49/200\n",
            "18/18 [==============================] - 0s 20ms/step - loss: 0.3618 - nd: 0.2066 - val_loss: 0.1467 - val_nd: 0.1478\n",
            "Epoch 50/200\n",
            "18/18 [==============================] - 0s 19ms/step - loss: 0.3576 - nd: 0.1982 - val_loss: 0.1512 - val_nd: 0.1484\n",
            "Epoch 51/200\n",
            "18/18 [==============================] - 0s 21ms/step - loss: 0.3724 - nd: 0.1948 - val_loss: 0.1370 - val_nd: 0.1493\n",
            "Epoch 52/200\n",
            "18/18 [==============================] - 0s 20ms/step - loss: 0.3508 - nd: 0.1903 - val_loss: 0.1504 - val_nd: 0.1496\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 75406\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                   epoch 51\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                    loss 0.3508312404155731\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                   _step 52\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                val_loss 0.15037471055984497\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                      nd 0.190264493227005\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              _timestamp 1650177261.285069\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                _runtime 5685.613328933716\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                  val_nd 0.1495789885520935\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           best_val_loss 0.1322658807039261\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              best_epoch 41\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             best-nd_std 0.011138875936207613\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     best-nrmse_mean_std 0.02796037478797592\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       best-nrmse_maxmin 0.023365442801621925\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   best-nrmse_maxmin_std 0.0029176479508880694\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         best-nrmse_mean 0.22391547877512458\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              best-nd_95 0.000654998655800587\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      best-nrmse_mean_95 0.0016441522472006372\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    best-nrmse_maxmin_95 0.0001715662780405891\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 best-nd 0.16268977344999644\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 7 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced youthful-dust-355: https://app.wandb.ai/matintavakoli/Interpreting_TS/runs/3c9j855e\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.9.7\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Wandb version 0.12.14 is available!  To upgrade, please run:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in wandb/run-20220417_063426-1ciwntyl\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mvibrant-haze-356\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://app.wandb.ai/matintavakoli/Interpreting_TS\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://app.wandb.ai/matintavakoli/Interpreting_TS/runs/1ciwntyl\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb off` to turn off syncing.\n",
            "\n",
            "/usr/local/envs/eval-metric/lib/python3.7/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  return f(**kwargs)\n",
            "/usr/local/envs/eval-metric/lib/python3.7/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  return f(**kwargs)\n",
            "  4%|█▍                                     | 101/2660 [00:00<00:10, 233.65it/s]\n",
            "(9090, 30, 10) (9090, 6) (1111, 30, 10) (1111, 6)\n",
            "(9090, 30, 10) (9090, 6) (1111, 30, 10) (1111, 6)\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 75623\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                   _step 2\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                    loss 0.0051247821290048975\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                      nd 0.12052057611674336\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              _timestamp 1650177517.5480757\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                _runtime 5941.87633562088\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                val_loss 0.020213027303978778\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                  val_nd 0.09935574530025659\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             best-nd_std 0.013584819703651626\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     best-nrmse_mean_std 0.038450249854744704\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       best-nrmse_maxmin 0.019790316655533092\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   best-nrmse_maxmin_std 0.004012259976861035\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         best-nrmse_mean 0.1896543654943021\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              best-nd_95 0.0007988273409403468\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      best-nrmse_mean_95 0.0022609877436725483\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    best-nrmse_maxmin_95 0.00023593268356853793\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 best-nd 0.11693237904087665\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 8 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced vibrant-haze-356: https://app.wandb.ai/matintavakoli/Interpreting_TS/runs/1ciwntyl\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.9.7\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Wandb version 0.12.14 is available!  To upgrade, please run:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in wandb/run-20220417_063841-2g9km3kj\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mexalted-universe-357\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://app.wandb.ai/matintavakoli/Interpreting_TS\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://app.wandb.ai/matintavakoli/Interpreting_TS/runs/2g9km3kj\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb off` to turn off syncing.\n",
            "\n",
            "/usr/local/envs/eval-metric/lib/python3.7/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  return f(**kwargs)\n",
            "/usr/local/envs/eval-metric/lib/python3.7/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  return f(**kwargs)\n",
            "  4%|█▍                                     | 101/2660 [00:00<00:12, 203.68it/s]\n",
            "(9090, 30, 10) (9090, 6) (1111, 30, 10) (1111, 6)\n",
            "(9090, 30, 10) (9090, 6) (1111, 30, 10) (1111, 6)\n",
            "Model: \"sequential_70\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "reshape_35 (Reshape)         (None, 300)               0         \n",
            "_________________________________________________________________\n",
            "dense_139 (Dense)            (None, 128)               38528     \n",
            "_________________________________________________________________\n",
            "dropout_104 (Dropout)        (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_140 (Dense)            (None, 128)               16512     \n",
            "_________________________________________________________________\n",
            "dropout_105 (Dropout)        (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_141 (Dense)            (None, 128)               16512     \n",
            "_________________________________________________________________\n",
            "dropout_106 (Dropout)        (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_142 (Dense)            (None, 6)                 774       \n",
            "=================================================================\n",
            "Total params: 72,326\n",
            "Trainable params: 72,326\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/200\n",
            "18/18 [==============================] - 0s 10ms/step - loss: 1.2095 - nd: 1.2346 - val_loss: 0.8241 - val_nd: 0.9197\n",
            "Epoch 2/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 1.0979 - nd: 1.0527 - val_loss: 0.8191 - val_nd: 0.9018\n",
            "Epoch 3/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 1.0370 - nd: 0.9492 - val_loss: 0.8145 - val_nd: 0.9034\n",
            "Epoch 4/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 1.0032 - nd: 0.9042 - val_loss: 0.8055 - val_nd: 0.9003\n",
            "Epoch 5/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.9905 - nd: 0.8870 - val_loss: 0.7699 - val_nd: 0.8414\n",
            "Epoch 6/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.9441 - nd: 0.8152 - val_loss: 0.6446 - val_nd: 0.7025\n",
            "Epoch 7/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.8247 - nd: 0.7039 - val_loss: 0.3785 - val_nd: 0.3686\n",
            "Epoch 8/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.7048 - nd: 0.5893 - val_loss: 0.2764 - val_nd: 0.2918\n",
            "Epoch 9/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.6386 - nd: 0.5213 - val_loss: 0.2233 - val_nd: 0.2432\n",
            "Epoch 10/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.5914 - nd: 0.4891 - val_loss: 0.2249 - val_nd: 0.2572\n",
            "Epoch 11/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.5760 - nd: 0.4618 - val_loss: 0.2036 - val_nd: 0.2473\n",
            "Epoch 12/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5670 - nd: 0.4549 - val_loss: 0.2263 - val_nd: 0.2764\n",
            "Epoch 13/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.5533 - nd: 0.4422 - val_loss: 0.2253 - val_nd: 0.2686\n",
            "Epoch 14/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5377 - nd: 0.4290 - val_loss: 0.2046 - val_nd: 0.2303\n",
            "Epoch 15/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.5495 - nd: 0.4220 - val_loss: 0.2369 - val_nd: 0.2751\n",
            "Epoch 16/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5210 - nd: 0.4152 - val_loss: 0.2348 - val_nd: 0.2691\n",
            "Epoch 17/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.5213 - nd: 0.4101 - val_loss: 0.2096 - val_nd: 0.2397\n",
            "Epoch 18/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.5381 - nd: 0.4064 - val_loss: 0.2141 - val_nd: 0.2271\n",
            "Epoch 19/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5297 - nd: 0.4037 - val_loss: 0.2064 - val_nd: 0.2109\n",
            "Epoch 20/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5127 - nd: 0.3988 - val_loss: 0.2287 - val_nd: 0.2456\n",
            "Epoch 21/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.5093 - nd: 0.3873 - val_loss: 0.2172 - val_nd: 0.2218\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 75717\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                   epoch 20\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                    loss 0.5093019604682922\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                   _step 21\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                val_loss 0.21722757816314697\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                      nd 0.38734033703804016\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              _timestamp 1650177526.6024392\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                _runtime 5950.930699110031\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                  val_nd 0.22184020280838013\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           best_val_loss 0.20359651744365692\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              best_epoch 10\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             best-nd_std 0.013861810111197956\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     best-nrmse_mean_std 0.040321949341011236\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       best-nrmse_maxmin 0.03545191174829775\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   best-nrmse_maxmin_std 0.004207570669660913\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         best-nrmse_mean 0.3397424076235577\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              best-nd_95 0.0008151151913169506\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      best-nrmse_mean_95 0.0023710491766742543\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    best-nrmse_maxmin_95 0.0002474175265616772\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 best-nd 0.2816435439117608\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 7 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced exalted-universe-357: https://app.wandb.ai/matintavakoli/Interpreting_TS/runs/2g9km3kj\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.9.7\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Wandb version 0.12.14 is available!  To upgrade, please run:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in wandb/run-20220417_063850-9zdkvn9c\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33musual-donkey-358\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://app.wandb.ai/matintavakoli/Interpreting_TS\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://app.wandb.ai/matintavakoli/Interpreting_TS/runs/9zdkvn9c\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb off` to turn off syncing.\n",
            "\n",
            "/usr/local/envs/eval-metric/lib/python3.7/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  return f(**kwargs)\n",
            "/usr/local/envs/eval-metric/lib/python3.7/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  return f(**kwargs)\n",
            "  4%|█▍                                     | 101/2660 [00:00<00:10, 236.36it/s]\n",
            "(9090, 30, 10) (9090, 6) (1111, 30, 10) (1111, 6)\n",
            "(9090, 30, 10) (9090, 6) (1111, 30, 10) (1111, 6)\n",
            "Model: \"sequential_71\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm_69 (LSTM)               (None, 30, 128)           71168     \n",
            "_________________________________________________________________\n",
            "lstm_70 (LSTM)               (None, 30, 128)           131584    \n",
            "_________________________________________________________________\n",
            "lstm_71 (LSTM)               (None, 128)               131584    \n",
            "_________________________________________________________________\n",
            "dropout_107 (Dropout)        (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_143 (Dense)            (None, 6)                 774       \n",
            "=================================================================\n",
            "Total params: 335,110\n",
            "Trainable params: 335,110\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/200\n",
            "18/18 [==============================] - 1s 71ms/step - loss: 0.9167 - nd: 0.8034 - val_loss: 0.7762 - val_nd: 0.7093\n",
            "Epoch 2/200\n",
            "18/18 [==============================] - 0s 22ms/step - loss: 0.6524 - nd: 0.5519 - val_loss: 0.6334 - val_nd: 0.6872\n",
            "Epoch 3/200\n",
            "18/18 [==============================] - 0s 23ms/step - loss: 0.5928 - nd: 0.4743 - val_loss: 0.5555 - val_nd: 0.5368\n",
            "Epoch 4/200\n",
            "18/18 [==============================] - 0s 22ms/step - loss: 0.5469 - nd: 0.4215 - val_loss: 0.5475 - val_nd: 0.5721\n",
            "Epoch 5/200\n",
            "18/18 [==============================] - 0s 23ms/step - loss: 0.5207 - nd: 0.3965 - val_loss: 0.5301 - val_nd: 0.5313\n",
            "Epoch 6/200\n",
            "18/18 [==============================] - 0s 20ms/step - loss: 0.5230 - nd: 0.4015 - val_loss: 0.5698 - val_nd: 0.5471\n",
            "Epoch 7/200\n",
            "18/18 [==============================] - 0s 23ms/step - loss: 0.5001 - nd: 0.3594 - val_loss: 0.5049 - val_nd: 0.5115\n",
            "Epoch 8/200\n",
            "18/18 [==============================] - 0s 22ms/step - loss: 0.4896 - nd: 0.3568 - val_loss: 0.4466 - val_nd: 0.4523\n",
            "Epoch 9/200\n",
            "18/18 [==============================] - 0s 22ms/step - loss: 0.4749 - nd: 0.3381 - val_loss: 0.3802 - val_nd: 0.4132\n",
            "Epoch 10/200\n",
            "18/18 [==============================] - 0s 20ms/step - loss: 0.4613 - nd: 0.3265 - val_loss: 0.3991 - val_nd: 0.4150\n",
            "Epoch 11/200\n",
            "18/18 [==============================] - 0s 21ms/step - loss: 0.4642 - nd: 0.3228 - val_loss: 0.3294 - val_nd: 0.3549\n",
            "Epoch 12/200\n",
            "18/18 [==============================] - 0s 20ms/step - loss: 0.4643 - nd: 0.3217 - val_loss: 0.3677 - val_nd: 0.4119\n",
            "Epoch 13/200\n",
            "18/18 [==============================] - 0s 22ms/step - loss: 0.4796 - nd: 0.3423 - val_loss: 0.2784 - val_nd: 0.3109\n",
            "Epoch 14/200\n",
            "18/18 [==============================] - 0s 20ms/step - loss: 0.4688 - nd: 0.3193 - val_loss: 0.3212 - val_nd: 0.3203\n",
            "Epoch 15/200\n",
            "18/18 [==============================] - 0s 20ms/step - loss: 0.4546 - nd: 0.3085 - val_loss: 0.3533 - val_nd: 0.3788\n",
            "Epoch 16/200\n",
            "18/18 [==============================] - 0s 20ms/step - loss: 0.4517 - nd: 0.2993 - val_loss: 0.3128 - val_nd: 0.3289\n",
            "Epoch 17/200\n",
            "18/18 [==============================] - 0s 20ms/step - loss: 0.4491 - nd: 0.2994 - val_loss: 0.3527 - val_nd: 0.3677\n",
            "Epoch 18/200\n",
            "18/18 [==============================] - 0s 19ms/step - loss: 0.4490 - nd: 0.2973 - val_loss: 0.3544 - val_nd: 0.3743\n",
            "Epoch 19/200\n",
            "18/18 [==============================] - 0s 20ms/step - loss: 0.4424 - nd: 0.2939 - val_loss: 0.2914 - val_nd: 0.3365\n",
            "Epoch 20/200\n",
            "18/18 [==============================] - 0s 19ms/step - loss: 0.4337 - nd: 0.2908 - val_loss: 0.2795 - val_nd: 0.3072\n",
            "Epoch 21/200\n",
            "18/18 [==============================] - 0s 20ms/step - loss: 0.4333 - nd: 0.2891 - val_loss: 0.2923 - val_nd: 0.3021\n",
            "Epoch 22/200\n",
            "18/18 [==============================] - 0s 20ms/step - loss: 0.4422 - nd: 0.2873 - val_loss: 0.3199 - val_nd: 0.3102\n",
            "Epoch 23/200\n",
            "18/18 [==============================] - 0s 20ms/step - loss: 0.4218 - nd: 0.2864 - val_loss: 0.2851 - val_nd: 0.3175\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 75837\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                   epoch 22\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                    loss 0.42179083824157715\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                   _step 23\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                val_loss 0.2851220667362213\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                      nd 0.28637439012527466\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              _timestamp 1650177547.7423222\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                _runtime 5972.070582151413\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                  val_nd 0.3175272047519684\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           best_val_loss 0.27842527627944946\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              best_epoch 12\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             best-nd_std 0.03173212829794116\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     best-nrmse_mean_std 0.1297314774373341\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       best-nrmse_maxmin 0.04609917848225931\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   best-nrmse_maxmin_std 0.013537400059225754\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         best-nrmse_mean 0.4417771881591941\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              best-nd_95 0.001865942443373654\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      best-nrmse_mean_95 0.007628592312467086\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    best-nrmse_maxmin_95 0.000796038926423894\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 best-nd 0.32919015455696465\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 7 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced usual-donkey-358: https://app.wandb.ai/matintavakoli/Interpreting_TS/runs/9zdkvn9c\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.9.7\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Wandb version 0.12.14 is available!  To upgrade, please run:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in wandb/run-20220417_063912-35ejyjpd\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mstellar-voice-359\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://app.wandb.ai/matintavakoli/Interpreting_TS\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://app.wandb.ai/matintavakoli/Interpreting_TS/runs/35ejyjpd\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb off` to turn off syncing.\n",
            "\n",
            "/usr/local/envs/eval-metric/lib/python3.7/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  return f(**kwargs)\n",
            "/usr/local/envs/eval-metric/lib/python3.7/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  return f(**kwargs)\n",
            "  4%|█▍                                     | 101/2660 [00:00<00:11, 225.78it/s]\n",
            "(9090, 30, 10) (9090, 6) (1111, 30, 10) (1111, 6)\n",
            "(9090, 30, 10) (9090, 6) (1111, 30, 10) (1111, 6)\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 75965\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                   _step 2\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                    loss 0.005126140585181629\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                      nd 0.12055915824278177\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              _timestamp 1650177799.6822934\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                _runtime 6224.010553359985\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                val_loss 0.020239708274404234\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                  val_nd 0.0992256905472076\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             best-nd_std 0.013783937573573418\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     best-nrmse_mean_std 0.03911025097367961\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       best-nrmse_maxmin 0.019800299175218517\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   best-nrmse_maxmin_std 0.004081130688603867\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         best-nrmse_mean 0.18975002987754203\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              best-nd_95 0.0008105360571421949\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      best-nrmse_mean_95 0.0022997977500147527\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    best-nrmse_maxmin_95 0.0002399824839140962\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 best-nd 0.1167653144760943\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 8 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced stellar-voice-359: https://app.wandb.ai/matintavakoli/Interpreting_TS/runs/35ejyjpd\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.9.7\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Wandb version 0.12.14 is available!  To upgrade, please run:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in wandb/run-20220417_064324-1gf2mg9d\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mdulcet-smoke-360\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://app.wandb.ai/matintavakoli/Interpreting_TS\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://app.wandb.ai/matintavakoli/Interpreting_TS/runs/1gf2mg9d\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb off` to turn off syncing.\n",
            "\n",
            "/usr/local/envs/eval-metric/lib/python3.7/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  return f(**kwargs)\n",
            "/usr/local/envs/eval-metric/lib/python3.7/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  return f(**kwargs)\n",
            "  4%|█▍                                     | 101/2660 [00:00<00:11, 227.41it/s]\n",
            "(9090, 30, 10) (9090, 6) (1111, 30, 10) (1111, 6)\n",
            "(9090, 30, 10) (9090, 6) (1111, 30, 10) (1111, 6)\n",
            "Model: \"sequential_72\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "reshape_36 (Reshape)         (None, 300)               0         \n",
            "_________________________________________________________________\n",
            "dense_144 (Dense)            (None, 16)                4816      \n",
            "_________________________________________________________________\n",
            "dropout_108 (Dropout)        (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_145 (Dense)            (None, 16)                272       \n",
            "_________________________________________________________________\n",
            "dropout_109 (Dropout)        (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_146 (Dense)            (None, 16)                272       \n",
            "_________________________________________________________________\n",
            "dropout_110 (Dropout)        (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_147 (Dense)            (None, 16)                272       \n",
            "_________________________________________________________________\n",
            "dropout_111 (Dropout)        (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_148 (Dense)            (None, 6)                 102       \n",
            "=================================================================\n",
            "Total params: 5,734\n",
            "Trainable params: 5,734\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/200\n",
            "18/18 [==============================] - 0s 11ms/step - loss: 1.0747 - nd: 1.0439 - val_loss: 0.8807 - val_nd: 1.0118\n",
            "Epoch 2/200\n",
            "18/18 [==============================] - 0s 8ms/step - loss: 1.0308 - nd: 0.9909 - val_loss: 0.8458 - val_nd: 0.9712\n",
            "Epoch 3/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 1.0056 - nd: 0.9385 - val_loss: 0.8318 - val_nd: 0.9361\n",
            "Epoch 4/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 1.0033 - nd: 0.9063 - val_loss: 0.8280 - val_nd: 0.9214\n",
            "Epoch 5/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 1.0018 - nd: 0.8941 - val_loss: 0.8270 - val_nd: 0.9128\n",
            "Epoch 6/200\n",
            "18/18 [==============================] - 0s 8ms/step - loss: 1.0039 - nd: 0.8859 - val_loss: 0.8267 - val_nd: 0.9097\n",
            "Epoch 7/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.9978 - nd: 0.8797 - val_loss: 0.8265 - val_nd: 0.9089\n",
            "Epoch 8/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.9946 - nd: 0.8840 - val_loss: 0.8267 - val_nd: 0.9123\n",
            "Epoch 9/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 1.0041 - nd: 0.8851 - val_loss: 0.8265 - val_nd: 0.9123\n",
            "Epoch 10/200\n",
            "18/18 [==============================] - 0s 8ms/step - loss: 1.0023 - nd: 0.8868 - val_loss: 0.8262 - val_nd: 0.9115\n",
            "Epoch 11/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 1.0035 - nd: 0.8903 - val_loss: 0.8262 - val_nd: 0.9131\n",
            "Epoch 12/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 1.0080 - nd: 0.8841 - val_loss: 0.8255 - val_nd: 0.9091\n",
            "Epoch 13/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 1.0002 - nd: 0.8881 - val_loss: 0.8246 - val_nd: 0.9090\n",
            "Epoch 14/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.9995 - nd: 0.8823 - val_loss: 0.8234 - val_nd: 0.9072\n",
            "Epoch 15/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.9969 - nd: 0.8791 - val_loss: 0.8211 - val_nd: 0.9006\n",
            "Epoch 16/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.9938 - nd: 0.8787 - val_loss: 0.8187 - val_nd: 0.8997\n",
            "Epoch 17/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.9953 - nd: 0.8717 - val_loss: 0.8145 - val_nd: 0.8941\n",
            "Epoch 18/200\n",
            "18/18 [==============================] - 0s 8ms/step - loss: 0.9930 - nd: 0.8678 - val_loss: 0.8079 - val_nd: 0.8809\n",
            "Epoch 19/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.9791 - nd: 0.8492 - val_loss: 0.7983 - val_nd: 0.8630\n",
            "Epoch 20/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.9739 - nd: 0.8286 - val_loss: 0.7810 - val_nd: 0.8365\n",
            "Epoch 21/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.9552 - nd: 0.8007 - val_loss: 0.7562 - val_nd: 0.7906\n",
            "Epoch 22/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.9303 - nd: 0.7571 - val_loss: 0.7207 - val_nd: 0.7340\n",
            "Epoch 23/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.8968 - nd: 0.6986 - val_loss: 0.6722 - val_nd: 0.6569\n",
            "Epoch 24/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.8528 - nd: 0.6317 - val_loss: 0.6104 - val_nd: 0.5838\n",
            "Epoch 25/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.7907 - nd: 0.5518 - val_loss: 0.5388 - val_nd: 0.4904\n",
            "Epoch 26/200\n",
            "18/18 [==============================] - 0s 8ms/step - loss: 0.7345 - nd: 0.4872 - val_loss: 0.4706 - val_nd: 0.4171\n",
            "Epoch 27/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.6868 - nd: 0.4293 - val_loss: 0.4127 - val_nd: 0.3680\n",
            "Epoch 28/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.6571 - nd: 0.3983 - val_loss: 0.3798 - val_nd: 0.3621\n",
            "Epoch 29/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.6414 - nd: 0.3899 - val_loss: 0.3433 - val_nd: 0.3181\n",
            "Epoch 30/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.5951 - nd: 0.3656 - val_loss: 0.3144 - val_nd: 0.3045\n",
            "Epoch 31/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.5829 - nd: 0.3518 - val_loss: 0.2978 - val_nd: 0.2981\n",
            "Epoch 32/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.5871 - nd: 0.3537 - val_loss: 0.2846 - val_nd: 0.2808\n",
            "Epoch 33/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.5758 - nd: 0.3429 - val_loss: 0.2997 - val_nd: 0.2868\n",
            "Epoch 34/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.5716 - nd: 0.3420 - val_loss: 0.2641 - val_nd: 0.2672\n",
            "Epoch 35/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.5549 - nd: 0.3285 - val_loss: 0.2732 - val_nd: 0.2689\n",
            "Epoch 36/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5609 - nd: 0.3390 - val_loss: 0.2718 - val_nd: 0.2862\n",
            "Epoch 37/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.5684 - nd: 0.3652 - val_loss: 0.2588 - val_nd: 0.2683\n",
            "Epoch 38/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.5556 - nd: 0.3344 - val_loss: 0.2654 - val_nd: 0.2698\n",
            "Epoch 39/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5435 - nd: 0.3315 - val_loss: 0.2712 - val_nd: 0.3062\n",
            "Epoch 40/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5430 - nd: 0.3373 - val_loss: 0.2591 - val_nd: 0.2910\n",
            "Epoch 41/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.5182 - nd: 0.3167 - val_loss: 0.2300 - val_nd: 0.2537\n",
            "Epoch 42/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.5140 - nd: 0.3121 - val_loss: 0.2276 - val_nd: 0.2511\n",
            "Epoch 43/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.5155 - nd: 0.3101 - val_loss: 0.2289 - val_nd: 0.2551\n",
            "Epoch 44/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.5014 - nd: 0.3052 - val_loss: 0.2377 - val_nd: 0.2496\n",
            "Epoch 45/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4986 - nd: 0.3029 - val_loss: 0.2295 - val_nd: 0.2462\n",
            "Epoch 46/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.4901 - nd: 0.3006 - val_loss: 0.2062 - val_nd: 0.2336\n",
            "Epoch 47/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4971 - nd: 0.3196 - val_loss: 0.2350 - val_nd: 0.2668\n",
            "Epoch 48/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.4825 - nd: 0.3151 - val_loss: 0.2002 - val_nd: 0.2295\n",
            "Epoch 49/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4767 - nd: 0.3025 - val_loss: 0.2014 - val_nd: 0.2267\n",
            "Epoch 50/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.4851 - nd: 0.2957 - val_loss: 0.1966 - val_nd: 0.2274\n",
            "Epoch 51/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4744 - nd: 0.2937 - val_loss: 0.2013 - val_nd: 0.2348\n",
            "Epoch 52/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4796 - nd: 0.3083 - val_loss: 0.2097 - val_nd: 0.2469\n",
            "Epoch 53/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4744 - nd: 0.2975 - val_loss: 0.2027 - val_nd: 0.2383\n",
            "Epoch 54/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4592 - nd: 0.2910 - val_loss: 0.2207 - val_nd: 0.2425\n",
            "Epoch 55/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.4564 - nd: 0.2877 - val_loss: 0.1982 - val_nd: 0.2289\n",
            "Epoch 56/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4605 - nd: 0.2925 - val_loss: 0.2036 - val_nd: 0.2332\n",
            "Epoch 57/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.4546 - nd: 0.2916 - val_loss: 0.2019 - val_nd: 0.2409\n",
            "Epoch 58/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.4486 - nd: 0.2897 - val_loss: 0.1886 - val_nd: 0.2274\n",
            "Epoch 59/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4588 - nd: 0.2828 - val_loss: 0.1952 - val_nd: 0.2306\n",
            "Epoch 60/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4534 - nd: 0.2842 - val_loss: 0.2061 - val_nd: 0.2409\n",
            "Epoch 61/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4491 - nd: 0.2936 - val_loss: 0.2185 - val_nd: 0.2582\n",
            "Epoch 62/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.4537 - nd: 0.2931 - val_loss: 0.2044 - val_nd: 0.2358\n",
            "Epoch 63/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4614 - nd: 0.2906 - val_loss: 0.2219 - val_nd: 0.2662\n",
            "Epoch 64/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4466 - nd: 0.2825 - val_loss: 0.2035 - val_nd: 0.2397\n",
            "Epoch 65/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.4429 - nd: 0.2782 - val_loss: 0.2040 - val_nd: 0.2353\n",
            "Epoch 66/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4416 - nd: 0.2760 - val_loss: 0.2030 - val_nd: 0.2243\n",
            "Epoch 67/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4373 - nd: 0.2727 - val_loss: 0.1919 - val_nd: 0.2194\n",
            "Epoch 68/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4425 - nd: 0.2776 - val_loss: 0.2112 - val_nd: 0.2323\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 76057\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                   epoch 67\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                    loss 0.4424780011177063\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                   _step 68\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                val_loss 0.2111705243587494\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                      nd 0.2776368260383606\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              _timestamp 1650177814.8496428\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                _runtime 6239.177902698517\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                  val_nd 0.2322535663843155\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           best_val_loss 0.18856461346149445\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              best_epoch 57\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             best-nd_std 0.03403797289275112\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     best-nrmse_mean_std 0.009727260061258274\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       best-nrmse_maxmin 0.033497544055086835\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   best-nrmse_maxmin_std 0.0010150336181362785\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         best-nrmse_mean 0.3210133306082664\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              best-nd_95 0.0020015328852400695\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      best-nrmse_mean_95 0.0005719914918916062\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    best-nrmse_maxmin_95 5.9686961169084066e-05\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 best-nd 0.2708294697807366\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 7 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced dulcet-smoke-360: https://app.wandb.ai/matintavakoli/Interpreting_TS/runs/1gf2mg9d\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.9.7\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Wandb version 0.12.14 is available!  To upgrade, please run:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in wandb/run-20220417_064339-3dk8e44i\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mskilled-frog-361\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://app.wandb.ai/matintavakoli/Interpreting_TS\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://app.wandb.ai/matintavakoli/Interpreting_TS/runs/3dk8e44i\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb off` to turn off syncing.\n",
            "\n",
            "/usr/local/envs/eval-metric/lib/python3.7/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  return f(**kwargs)\n",
            "/usr/local/envs/eval-metric/lib/python3.7/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  return f(**kwargs)\n",
            "  4%|█▍                                     | 101/2660 [00:00<00:11, 232.24it/s]\n",
            "(9090, 30, 10) (9090, 6) (1111, 30, 10) (1111, 6)\n",
            "(9090, 30, 10) (9090, 6) (1111, 30, 10) (1111, 6)\n",
            "Model: \"sequential_73\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm_72 (LSTM)               (None, 30, 16)            1728      \n",
            "_________________________________________________________________\n",
            "lstm_73 (LSTM)               (None, 30, 16)            2112      \n",
            "_________________________________________________________________\n",
            "lstm_74 (LSTM)               (None, 30, 16)            2112      \n",
            "_________________________________________________________________\n",
            "lstm_75 (LSTM)               (None, 16)                2112      \n",
            "_________________________________________________________________\n",
            "dropout_112 (Dropout)        (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_149 (Dense)            (None, 6)                 102       \n",
            "=================================================================\n",
            "Total params: 8,166\n",
            "Trainable params: 8,166\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/200\n",
            "18/18 [==============================] - 1s 71ms/step - loss: 1.0054 - nd: 0.9420 - val_loss: 0.8156 - val_nd: 0.9087\n",
            "Epoch 2/200\n",
            "18/18 [==============================] - 0s 16ms/step - loss: 0.9638 - nd: 0.8327 - val_loss: 0.7378 - val_nd: 0.7746\n",
            "Epoch 3/200\n",
            "18/18 [==============================] - 0s 14ms/step - loss: 0.8469 - nd: 0.6385 - val_loss: 0.5752 - val_nd: 0.5711\n",
            "Epoch 4/200\n",
            "18/18 [==============================] - 0s 14ms/step - loss: 0.7363 - nd: 0.5277 - val_loss: 0.4741 - val_nd: 0.4745\n",
            "Epoch 5/200\n",
            "18/18 [==============================] - 0s 14ms/step - loss: 0.6519 - nd: 0.4625 - val_loss: 0.3831 - val_nd: 0.3835\n",
            "Epoch 6/200\n",
            "18/18 [==============================] - 0s 13ms/step - loss: 0.5900 - nd: 0.4025 - val_loss: 0.3327 - val_nd: 0.3442\n",
            "Epoch 7/200\n",
            "18/18 [==============================] - 0s 15ms/step - loss: 0.5573 - nd: 0.3620 - val_loss: 0.3162 - val_nd: 0.3414\n",
            "Epoch 8/200\n",
            "18/18 [==============================] - 0s 13ms/step - loss: 0.5298 - nd: 0.3474 - val_loss: 0.2425 - val_nd: 0.2746\n",
            "Epoch 9/200\n",
            "18/18 [==============================] - 0s 14ms/step - loss: 0.5027 - nd: 0.3198 - val_loss: 0.2227 - val_nd: 0.2612\n",
            "Epoch 10/200\n",
            "18/18 [==============================] - 0s 13ms/step - loss: 0.4770 - nd: 0.2882 - val_loss: 0.2094 - val_nd: 0.2430\n",
            "Epoch 11/200\n",
            "18/18 [==============================] - 0s 14ms/step - loss: 0.4606 - nd: 0.2709 - val_loss: 0.1914 - val_nd: 0.2269\n",
            "Epoch 12/200\n",
            "18/18 [==============================] - 0s 11ms/step - loss: 0.4488 - nd: 0.2637 - val_loss: 0.1973 - val_nd: 0.2234\n",
            "Epoch 13/200\n",
            "18/18 [==============================] - 0s 14ms/step - loss: 0.4415 - nd: 0.2664 - val_loss: 0.1821 - val_nd: 0.2030\n",
            "Epoch 14/200\n",
            "18/18 [==============================] - 0s 11ms/step - loss: 0.4285 - nd: 0.2407 - val_loss: 0.1920 - val_nd: 0.2081\n",
            "Epoch 15/200\n",
            "18/18 [==============================] - 0s 11ms/step - loss: 0.4296 - nd: 0.2329 - val_loss: 0.1835 - val_nd: 0.1997\n",
            "Epoch 16/200\n",
            "18/18 [==============================] - 0s 11ms/step - loss: 0.4228 - nd: 0.2278 - val_loss: 0.1910 - val_nd: 0.1983\n",
            "Epoch 17/200\n",
            "18/18 [==============================] - 0s 12ms/step - loss: 0.4074 - nd: 0.2236 - val_loss: 0.1957 - val_nd: 0.2055\n",
            "Epoch 18/200\n",
            "18/18 [==============================] - 0s 10ms/step - loss: 0.4203 - nd: 0.2268 - val_loss: 0.1928 - val_nd: 0.1974\n",
            "Epoch 19/200\n",
            "18/18 [==============================] - 0s 11ms/step - loss: 0.4048 - nd: 0.2296 - val_loss: 0.1894 - val_nd: 0.1854\n",
            "Epoch 20/200\n",
            "18/18 [==============================] - 0s 11ms/step - loss: 0.4220 - nd: 0.2135 - val_loss: 0.1889 - val_nd: 0.1820\n",
            "Epoch 21/200\n",
            "18/18 [==============================] - 0s 12ms/step - loss: 0.4084 - nd: 0.2147 - val_loss: 0.1923 - val_nd: 0.1819\n",
            "Epoch 22/200\n",
            "18/18 [==============================] - 0s 11ms/step - loss: 0.4123 - nd: 0.2136 - val_loss: 0.1983 - val_nd: 0.1871\n",
            "Epoch 23/200\n",
            "18/18 [==============================] - 0s 12ms/step - loss: 0.3886 - nd: 0.2162 - val_loss: 0.2077 - val_nd: 0.1955\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 76320\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                   epoch 22\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                    loss 0.3885815739631653\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                   _step 23\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                val_loss 0.2076721340417862\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                      nd 0.21615064144134521\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              _timestamp 1650177835.2123485\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                _runtime 6259.540608406067\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                  val_nd 0.1955253630876541\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           best_val_loss 0.18212644755840302\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              best_epoch 12\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             best-nd_std 0.023982353587143398\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     best-nrmse_mean_std 0.06161246914228772\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       best-nrmse_maxmin 0.0319476766251871\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   best-nrmse_maxmin_std 0.006429223345727671\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         best-nrmse_mean 0.3061606564881816\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              best-nd_95 0.0014102329043321278\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      best-nrmse_mean_95 0.003622994339812478\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    best-nrmse_maxmin_95 0.0003780572360631854\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 best-nd 0.24709044707893635\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 7 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced skilled-frog-361: https://app.wandb.ai/matintavakoli/Interpreting_TS/runs/3dk8e44i\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.9.7\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Wandb version 0.12.14 is available!  To upgrade, please run:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in wandb/run-20220417_064400-3kkltzqr\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mfast-snowball-362\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://app.wandb.ai/matintavakoli/Interpreting_TS\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://app.wandb.ai/matintavakoli/Interpreting_TS/runs/3kkltzqr\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb off` to turn off syncing.\n",
            "\n",
            "/usr/local/envs/eval-metric/lib/python3.7/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  return f(**kwargs)\n",
            "/usr/local/envs/eval-metric/lib/python3.7/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  return f(**kwargs)\n",
            "  4%|█▍                                     | 101/2660 [00:00<00:10, 236.15it/s]\n",
            "(9090, 30, 10) (9090, 6) (1111, 30, 10) (1111, 6)\n",
            "(9090, 30, 10) (9090, 6) (1111, 30, 10) (1111, 6)\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 76448\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                   _step 2\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                    loss 0.003946073567348792\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                      nd 0.09894343010216634\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              _timestamp 1650178207.1663518\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                _runtime 6631.494611740112\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                val_loss 0.01933580865754916\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                  val_nd 0.09407629104522935\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             best-nd_std 0.011557111283623859\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     best-nrmse_mean_std 0.03307628709124117\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       best-nrmse_maxmin 0.019103508273548325\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   best-nrmse_maxmin_std 0.0034514902602895835\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         best-nrmse_mean 0.1830725502475475\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              best-nd_95 0.0006795921239328122\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      best-nrmse_mean_95 0.001944982932542958\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    best-nrmse_maxmin_95 0.00020295777544750978\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 best-nd 0.11144859980481403\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 8 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced fast-snowball-362: https://app.wandb.ai/matintavakoli/Interpreting_TS/runs/3kkltzqr\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.9.7\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Wandb version 0.12.14 is available!  To upgrade, please run:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in wandb/run-20220417_065011-3lg2hpz6\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mdevoted-universe-363\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://app.wandb.ai/matintavakoli/Interpreting_TS\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://app.wandb.ai/matintavakoli/Interpreting_TS/runs/3lg2hpz6\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb off` to turn off syncing.\n",
            "\n",
            "/usr/local/envs/eval-metric/lib/python3.7/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  return f(**kwargs)\n",
            "/usr/local/envs/eval-metric/lib/python3.7/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  return f(**kwargs)\n",
            "  4%|█▍                                     | 101/2660 [00:00<00:10, 240.53it/s]\n",
            "(9090, 30, 10) (9090, 6) (1111, 30, 10) (1111, 6)\n",
            "(9090, 30, 10) (9090, 6) (1111, 30, 10) (1111, 6)\n",
            "Model: \"sequential_74\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "reshape_37 (Reshape)         (None, 300)               0         \n",
            "_________________________________________________________________\n",
            "dense_150 (Dense)            (None, 16)                4816      \n",
            "_________________________________________________________________\n",
            "dropout_113 (Dropout)        (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_151 (Dense)            (None, 16)                272       \n",
            "_________________________________________________________________\n",
            "dropout_114 (Dropout)        (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_152 (Dense)            (None, 16)                272       \n",
            "_________________________________________________________________\n",
            "dropout_115 (Dropout)        (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_153 (Dense)            (None, 16)                272       \n",
            "_________________________________________________________________\n",
            "dropout_116 (Dropout)        (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_154 (Dense)            (None, 6)                 102       \n",
            "=================================================================\n",
            "Total params: 5,734\n",
            "Trainable params: 5,734\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/200\n",
            "18/18 [==============================] - 0s 10ms/step - loss: 1.0985 - nd: 1.1595 - val_loss: 0.8673 - val_nd: 1.0163\n",
            "Epoch 2/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 1.0584 - nd: 1.0244 - val_loss: 0.8368 - val_nd: 0.9466\n",
            "Epoch 3/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 1.0397 - nd: 0.9694 - val_loss: 0.8286 - val_nd: 0.9231\n",
            "Epoch 4/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 1.0312 - nd: 0.9453 - val_loss: 0.8271 - val_nd: 0.9153\n",
            "Epoch 5/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 1.0324 - nd: 0.9414 - val_loss: 0.8276 - val_nd: 0.9196\n",
            "Epoch 6/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 1.0278 - nd: 0.9335 - val_loss: 0.8275 - val_nd: 0.9193\n",
            "Epoch 7/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 1.0213 - nd: 0.9257 - val_loss: 0.8266 - val_nd: 0.9107\n",
            "Epoch 8/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 1.0130 - nd: 0.9201 - val_loss: 0.8269 - val_nd: 0.9146\n",
            "Epoch 9/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 1.0194 - nd: 0.9163 - val_loss: 0.8267 - val_nd: 0.9133\n",
            "Epoch 10/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 1.0172 - nd: 0.9162 - val_loss: 0.8270 - val_nd: 0.9166\n",
            "Epoch 11/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 1.0146 - nd: 0.9150 - val_loss: 0.8268 - val_nd: 0.9150\n",
            "Epoch 12/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 1.0175 - nd: 0.9100 - val_loss: 0.8268 - val_nd: 0.9160\n",
            "Epoch 13/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 1.0192 - nd: 0.9114 - val_loss: 0.8267 - val_nd: 0.9158\n",
            "Epoch 14/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 1.0099 - nd: 0.9056 - val_loss: 0.8262 - val_nd: 0.9115\n",
            "Epoch 15/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 1.0161 - nd: 0.9057 - val_loss: 0.8263 - val_nd: 0.9145\n",
            "Epoch 16/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 1.0111 - nd: 0.9056 - val_loss: 0.8262 - val_nd: 0.9147\n",
            "Epoch 17/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 1.0137 - nd: 0.9042 - val_loss: 0.8259 - val_nd: 0.9132\n",
            "Epoch 18/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 1.0065 - nd: 0.9012 - val_loss: 0.8257 - val_nd: 0.9139\n",
            "Epoch 19/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.9982 - nd: 0.8976 - val_loss: 0.8252 - val_nd: 0.9109\n",
            "Epoch 20/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 1.0115 - nd: 0.8945 - val_loss: 0.8246 - val_nd: 0.9088\n",
            "Epoch 21/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 1.0065 - nd: 0.8950 - val_loss: 0.8243 - val_nd: 0.9104\n",
            "Epoch 22/200\n",
            "18/18 [==============================] - 0s 8ms/step - loss: 1.0057 - nd: 0.8968 - val_loss: 0.8237 - val_nd: 0.9102\n",
            "Epoch 23/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 1.0000 - nd: 0.8944 - val_loss: 0.8226 - val_nd: 0.9067\n",
            "Epoch 24/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 1.0013 - nd: 0.8907 - val_loss: 0.8213 - val_nd: 0.9051\n",
            "Epoch 25/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.9957 - nd: 0.8896 - val_loss: 0.8194 - val_nd: 0.9006\n",
            "Epoch 26/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.9957 - nd: 0.8845 - val_loss: 0.8168 - val_nd: 0.8950\n",
            "Epoch 27/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.9976 - nd: 0.8851 - val_loss: 0.8136 - val_nd: 0.8945\n",
            "Epoch 28/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.9920 - nd: 0.8772 - val_loss: 0.8083 - val_nd: 0.8792\n",
            "Epoch 29/200\n",
            "18/18 [==============================] - 0s 8ms/step - loss: 0.9869 - nd: 0.8683 - val_loss: 0.8012 - val_nd: 0.8649\n",
            "Epoch 30/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.9844 - nd: 0.8524 - val_loss: 0.7914 - val_nd: 0.8507\n",
            "Epoch 31/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.9737 - nd: 0.8479 - val_loss: 0.7771 - val_nd: 0.8251\n",
            "Epoch 32/200\n",
            "18/18 [==============================] - 0s 8ms/step - loss: 0.9706 - nd: 0.8260 - val_loss: 0.7570 - val_nd: 0.7863\n",
            "Epoch 33/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.9495 - nd: 0.8016 - val_loss: 0.7302 - val_nd: 0.7352\n",
            "Epoch 34/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.9439 - nd: 0.7810 - val_loss: 0.6982 - val_nd: 0.6797\n",
            "Epoch 35/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.9194 - nd: 0.7485 - val_loss: 0.6572 - val_nd: 0.6153\n",
            "Epoch 36/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.9025 - nd: 0.7201 - val_loss: 0.6089 - val_nd: 0.5509\n",
            "Epoch 37/200\n",
            "18/18 [==============================] - 0s 8ms/step - loss: 0.8714 - nd: 0.6983 - val_loss: 0.5643 - val_nd: 0.4892\n",
            "Epoch 38/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.8491 - nd: 0.6896 - val_loss: 0.5267 - val_nd: 0.4557\n",
            "Epoch 39/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.8317 - nd: 0.6675 - val_loss: 0.4957 - val_nd: 0.4266\n",
            "Epoch 40/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.8218 - nd: 0.6640 - val_loss: 0.4709 - val_nd: 0.4051\n",
            "Epoch 41/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.8091 - nd: 0.6433 - val_loss: 0.4557 - val_nd: 0.4005\n",
            "Epoch 42/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.7948 - nd: 0.6289 - val_loss: 0.4410 - val_nd: 0.3937\n",
            "Epoch 43/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.7858 - nd: 0.6182 - val_loss: 0.4108 - val_nd: 0.3691\n",
            "Epoch 44/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.7752 - nd: 0.6116 - val_loss: 0.3996 - val_nd: 0.3939\n",
            "Epoch 45/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.7565 - nd: 0.5863 - val_loss: 0.3818 - val_nd: 0.3845\n",
            "Epoch 46/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.7386 - nd: 0.5700 - val_loss: 0.3715 - val_nd: 0.3861\n",
            "Epoch 47/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.7273 - nd: 0.5630 - val_loss: 0.3819 - val_nd: 0.3851\n",
            "Epoch 48/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.7321 - nd: 0.5697 - val_loss: 0.3523 - val_nd: 0.3603\n",
            "Epoch 49/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.6963 - nd: 0.5335 - val_loss: 0.3087 - val_nd: 0.3046\n",
            "Epoch 50/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.7014 - nd: 0.5246 - val_loss: 0.3318 - val_nd: 0.3290\n",
            "Epoch 51/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.6810 - nd: 0.5158 - val_loss: 0.3194 - val_nd: 0.3300\n",
            "Epoch 52/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.6886 - nd: 0.5089 - val_loss: 0.3078 - val_nd: 0.3071\n",
            "Epoch 53/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.6802 - nd: 0.5135 - val_loss: 0.3001 - val_nd: 0.3009\n",
            "Epoch 54/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.6696 - nd: 0.5063 - val_loss: 0.3041 - val_nd: 0.3123\n",
            "Epoch 55/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.6810 - nd: 0.5046 - val_loss: 0.3109 - val_nd: 0.3182\n",
            "Epoch 56/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.6669 - nd: 0.4980 - val_loss: 0.2965 - val_nd: 0.3096\n",
            "Epoch 57/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.6657 - nd: 0.4861 - val_loss: 0.3071 - val_nd: 0.3142\n",
            "Epoch 58/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.6650 - nd: 0.4812 - val_loss: 0.3116 - val_nd: 0.3188\n",
            "Epoch 59/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.6688 - nd: 0.4906 - val_loss: 0.3041 - val_nd: 0.3125\n",
            "Epoch 60/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.6703 - nd: 0.4889 - val_loss: 0.2948 - val_nd: 0.3056\n",
            "Epoch 61/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.6498 - nd: 0.4685 - val_loss: 0.2864 - val_nd: 0.2838\n",
            "Epoch 62/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.6533 - nd: 0.4831 - val_loss: 0.3003 - val_nd: 0.2947\n",
            "Epoch 63/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.6391 - nd: 0.4761 - val_loss: 0.3018 - val_nd: 0.3137\n",
            "Epoch 64/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.6577 - nd: 0.4846 - val_loss: 0.2746 - val_nd: 0.2826\n",
            "Epoch 65/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.6559 - nd: 0.4785 - val_loss: 0.2852 - val_nd: 0.2866\n",
            "Epoch 66/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.6537 - nd: 0.4769 - val_loss: 0.3254 - val_nd: 0.3253\n",
            "Epoch 67/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.6570 - nd: 0.4749 - val_loss: 0.3077 - val_nd: 0.2929\n",
            "Epoch 68/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.6488 - nd: 0.4747 - val_loss: 0.2819 - val_nd: 0.2804\n",
            "Epoch 69/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.6392 - nd: 0.4688 - val_loss: 0.2841 - val_nd: 0.2956\n",
            "Epoch 70/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.6511 - nd: 0.4744 - val_loss: 0.3301 - val_nd: 0.3274\n",
            "Epoch 71/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.6467 - nd: 0.4733 - val_loss: 0.3205 - val_nd: 0.3200\n",
            "Epoch 72/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.6572 - nd: 0.4749 - val_loss: 0.2841 - val_nd: 0.3032\n",
            "Epoch 73/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.6405 - nd: 0.4610 - val_loss: 0.2934 - val_nd: 0.2898\n",
            "Epoch 74/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.6270 - nd: 0.4643 - val_loss: 0.2894 - val_nd: 0.2980\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 76564\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                   epoch 73\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                    loss 0.6269752383232117\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                   _step 74\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                val_loss 0.289449542760849\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                      nd 0.46433115005493164\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              _timestamp 1650178221.914784\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                _runtime 6646.243043899536\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                  val_nd 0.2979983687400818\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           best_val_loss 0.2746451199054718\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              best_epoch 63\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             best-nd_std 0.03420004477682499\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     best-nrmse_mean_std 0.11246231996810363\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       best-nrmse_maxmin 0.048903705549010384\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   best-nrmse_maxmin_std 0.011735374074748204\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         best-nrmse_mean 0.46865350401680633\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              best-nd_95 0.0020110631885506916\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      best-nrmse_mean_95 0.0066131150781452075\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    best-nrmse_maxmin_95 0.0006900745001828399\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 best-nd 0.35041865676441863\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 7 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced devoted-universe-363: https://app.wandb.ai/matintavakoli/Interpreting_TS/runs/3lg2hpz6\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.9.7\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Wandb version 0.12.14 is available!  To upgrade, please run:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in wandb/run-20220417_065026-1xrfw2rt\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mpeachy-dew-364\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://app.wandb.ai/matintavakoli/Interpreting_TS\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://app.wandb.ai/matintavakoli/Interpreting_TS/runs/1xrfw2rt\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb off` to turn off syncing.\n",
            "\n",
            "/usr/local/envs/eval-metric/lib/python3.7/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  return f(**kwargs)\n",
            "/usr/local/envs/eval-metric/lib/python3.7/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  return f(**kwargs)\n",
            "  4%|█▍                                     | 101/2660 [00:00<00:11, 228.75it/s]\n",
            "(9090, 30, 10) (9090, 6) (1111, 30, 10) (1111, 6)\n",
            "(9090, 30, 10) (9090, 6) (1111, 30, 10) (1111, 6)\n",
            "Model: \"sequential_75\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm_76 (LSTM)               (None, 30, 16)            1728      \n",
            "_________________________________________________________________\n",
            "lstm_77 (LSTM)               (None, 30, 16)            2112      \n",
            "_________________________________________________________________\n",
            "lstm_78 (LSTM)               (None, 30, 16)            2112      \n",
            "_________________________________________________________________\n",
            "lstm_79 (LSTM)               (None, 16)                2112      \n",
            "_________________________________________________________________\n",
            "dropout_117 (Dropout)        (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_155 (Dense)            (None, 6)                 102       \n",
            "=================================================================\n",
            "Total params: 8,166\n",
            "Trainable params: 8,166\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/200\n",
            "18/18 [==============================] - 1s 72ms/step - loss: 0.9955 - nd: 0.9003 - val_loss: 0.8126 - val_nd: 0.9379\n",
            "Epoch 2/200\n",
            "18/18 [==============================] - 0s 14ms/step - loss: 0.9579 - nd: 0.8466 - val_loss: 0.7837 - val_nd: 0.8784\n",
            "Epoch 3/200\n",
            "18/18 [==============================] - 0s 13ms/step - loss: 0.9038 - nd: 0.7792 - val_loss: 0.7109 - val_nd: 0.7424\n",
            "Epoch 4/200\n",
            "18/18 [==============================] - 0s 13ms/step - loss: 0.8161 - nd: 0.6664 - val_loss: 0.6287 - val_nd: 0.5645\n",
            "Epoch 5/200\n",
            "18/18 [==============================] - 0s 14ms/step - loss: 0.7016 - nd: 0.5633 - val_loss: 0.6248 - val_nd: 0.5281\n",
            "Epoch 6/200\n",
            "18/18 [==============================] - 0s 14ms/step - loss: 0.6490 - nd: 0.5177 - val_loss: 0.5003 - val_nd: 0.4492\n",
            "Epoch 7/200\n",
            "18/18 [==============================] - 0s 13ms/step - loss: 0.6185 - nd: 0.4898 - val_loss: 0.3393 - val_nd: 0.3324\n",
            "Epoch 8/200\n",
            "18/18 [==============================] - 0s 12ms/step - loss: 0.6016 - nd: 0.4647 - val_loss: 0.3819 - val_nd: 0.3708\n",
            "Epoch 9/200\n",
            "18/18 [==============================] - 0s 14ms/step - loss: 0.5731 - nd: 0.4391 - val_loss: 0.3252 - val_nd: 0.3643\n",
            "Epoch 10/200\n",
            "18/18 [==============================] - 0s 14ms/step - loss: 0.5544 - nd: 0.4241 - val_loss: 0.2714 - val_nd: 0.3355\n",
            "Epoch 11/200\n",
            "18/18 [==============================] - 0s 12ms/step - loss: 0.5376 - nd: 0.4079 - val_loss: 0.2818 - val_nd: 0.3230\n",
            "Epoch 12/200\n",
            "18/18 [==============================] - 0s 14ms/step - loss: 0.5221 - nd: 0.3856 - val_loss: 0.2481 - val_nd: 0.3007\n",
            "Epoch 13/200\n",
            "18/18 [==============================] - 0s 11ms/step - loss: 0.5082 - nd: 0.3763 - val_loss: 0.2598 - val_nd: 0.2880\n",
            "Epoch 14/200\n",
            "18/18 [==============================] - 0s 13ms/step - loss: 0.4979 - nd: 0.3668 - val_loss: 0.2334 - val_nd: 0.2548\n",
            "Epoch 15/200\n",
            "18/18 [==============================] - 0s 12ms/step - loss: 0.4907 - nd: 0.3554 - val_loss: 0.2624 - val_nd: 0.2687\n",
            "Epoch 16/200\n",
            "18/18 [==============================] - 0s 14ms/step - loss: 0.4965 - nd: 0.3511 - val_loss: 0.2332 - val_nd: 0.2407\n",
            "Epoch 17/200\n",
            "18/18 [==============================] - 0s 15ms/step - loss: 0.4841 - nd: 0.3419 - val_loss: 0.2319 - val_nd: 0.2273\n",
            "Epoch 18/200\n",
            "18/18 [==============================] - 0s 14ms/step - loss: 0.4767 - nd: 0.3390 - val_loss: 0.2177 - val_nd: 0.2110\n",
            "Epoch 19/200\n",
            "18/18 [==============================] - 0s 12ms/step - loss: 0.4863 - nd: 0.3349 - val_loss: 0.2233 - val_nd: 0.2090\n",
            "Epoch 20/200\n",
            "18/18 [==============================] - 0s 14ms/step - loss: 0.4637 - nd: 0.3193 - val_loss: 0.2116 - val_nd: 0.1936\n",
            "Epoch 21/200\n",
            "18/18 [==============================] - 0s 11ms/step - loss: 0.4603 - nd: 0.3147 - val_loss: 0.2153 - val_nd: 0.1982\n",
            "Epoch 22/200\n",
            "18/18 [==============================] - 0s 11ms/step - loss: 0.4699 - nd: 0.3074 - val_loss: 0.2172 - val_nd: 0.1925\n",
            "Epoch 23/200\n",
            "18/18 [==============================] - 0s 12ms/step - loss: 0.4504 - nd: 0.3060 - val_loss: 0.2218 - val_nd: 0.2046\n",
            "Epoch 24/200\n",
            "18/18 [==============================] - 0s 12ms/step - loss: 0.4594 - nd: 0.3037 - val_loss: 0.2178 - val_nd: 0.1940\n",
            "Epoch 25/200\n",
            "18/18 [==============================] - 0s 12ms/step - loss: 0.4438 - nd: 0.2972 - val_loss: 0.2130 - val_nd: 0.1866\n",
            "Epoch 26/200\n",
            "18/18 [==============================] - 0s 12ms/step - loss: 0.4518 - nd: 0.2972 - val_loss: 0.2226 - val_nd: 0.1968\n",
            "Epoch 27/200\n",
            "18/18 [==============================] - 0s 12ms/step - loss: 0.4556 - nd: 0.2977 - val_loss: 0.2362 - val_nd: 0.2121\n",
            "Epoch 28/200\n",
            "18/18 [==============================] - 0s 15ms/step - loss: 0.4496 - nd: 0.2985 - val_loss: 0.2115 - val_nd: 0.1861\n",
            "Epoch 29/200\n",
            "18/18 [==============================] - 0s 13ms/step - loss: 0.4566 - nd: 0.2955 - val_loss: 0.2160 - val_nd: 0.1946\n",
            "Epoch 30/200\n",
            "18/18 [==============================] - 0s 12ms/step - loss: 0.4441 - nd: 0.2915 - val_loss: 0.2273 - val_nd: 0.2036\n",
            "Epoch 31/200\n",
            "18/18 [==============================] - 0s 14ms/step - loss: 0.4549 - nd: 0.2913 - val_loss: 0.2112 - val_nd: 0.1899\n",
            "Epoch 32/200\n",
            "18/18 [==============================] - 0s 14ms/step - loss: 0.4562 - nd: 0.2974 - val_loss: 0.2074 - val_nd: 0.1790\n",
            "Epoch 33/200\n",
            "18/18 [==============================] - 0s 14ms/step - loss: 0.4427 - nd: 0.2890 - val_loss: 0.2068 - val_nd: 0.1867\n",
            "Epoch 34/200\n",
            "18/18 [==============================] - 0s 14ms/step - loss: 0.4455 - nd: 0.2895 - val_loss: 0.2051 - val_nd: 0.1751\n",
            "Epoch 35/200\n",
            "18/18 [==============================] - 0s 14ms/step - loss: 0.4491 - nd: 0.2818 - val_loss: 0.1998 - val_nd: 0.1748\n",
            "Epoch 36/200\n",
            "18/18 [==============================] - 0s 11ms/step - loss: 0.4444 - nd: 0.2836 - val_loss: 0.2005 - val_nd: 0.1741\n",
            "Epoch 37/200\n",
            "18/18 [==============================] - 0s 12ms/step - loss: 0.4476 - nd: 0.2807 - val_loss: 0.2038 - val_nd: 0.1765\n",
            "Epoch 38/200\n",
            "18/18 [==============================] - 0s 12ms/step - loss: 0.4514 - nd: 0.2782 - val_loss: 0.2087 - val_nd: 0.1741\n",
            "Epoch 39/200\n",
            "18/18 [==============================] - 0s 11ms/step - loss: 0.4349 - nd: 0.2785 - val_loss: 0.2013 - val_nd: 0.1800\n",
            "Epoch 40/200\n",
            "18/18 [==============================] - 0s 12ms/step - loss: 0.4470 - nd: 0.2778 - val_loss: 0.2247 - val_nd: 0.2074\n",
            "Epoch 41/200\n",
            "18/18 [==============================] - 0s 12ms/step - loss: 0.4428 - nd: 0.2828 - val_loss: 0.2067 - val_nd: 0.1740\n",
            "Epoch 42/200\n",
            "18/18 [==============================] - 0s 10ms/step - loss: 0.4377 - nd: 0.2731 - val_loss: 0.2055 - val_nd: 0.1884\n",
            "Epoch 43/200\n",
            "18/18 [==============================] - 0s 12ms/step - loss: 0.4378 - nd: 0.2727 - val_loss: 0.2002 - val_nd: 0.1780\n",
            "Epoch 44/200\n",
            "18/18 [==============================] - 0s 12ms/step - loss: 0.4471 - nd: 0.2726 - val_loss: 0.2011 - val_nd: 0.1688\n",
            "Epoch 45/200\n",
            "18/18 [==============================] - 0s 12ms/step - loss: 0.4378 - nd: 0.2757 - val_loss: 0.2032 - val_nd: 0.1776\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 76843\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                   epoch 44\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                    loss 0.4377715289592743\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                   _step 45\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                val_loss 0.20323969423770905\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                      nd 0.27571141719818115\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              _timestamp 1650178247.6003442\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                _runtime 6671.928604125977\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                  val_nd 0.1775914430618286\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           best_val_loss 0.19980983436107635\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              best_epoch 34\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             best-nd_std 0.0341548490515477\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     best-nrmse_mean_std 0.15810563559037868\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       best-nrmse_maxmin 0.03438699413022653\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   best-nrmse_maxmin_std 0.016498226050335348\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         best-nrmse_mean 0.3295370997108854\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              best-nd_95 0.0020084055470189883\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      best-nrmse_mean_95 0.0092970762381481\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    best-nrmse_maxmin_95 0.0009701442001824684\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 best-nd 0.22266886696888039\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 7 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced peachy-dew-364: https://app.wandb.ai/matintavakoli/Interpreting_TS/runs/1xrfw2rt\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.9.7\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Wandb version 0.12.14 is available!  To upgrade, please run:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in wandb/run-20220417_065052-3psk8r3o\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mgallant-pine-365\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://app.wandb.ai/matintavakoli/Interpreting_TS\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://app.wandb.ai/matintavakoli/Interpreting_TS/runs/3psk8r3o\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb off` to turn off syncing.\n",
            "\n",
            "/usr/local/envs/eval-metric/lib/python3.7/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  return f(**kwargs)\n",
            "/usr/local/envs/eval-metric/lib/python3.7/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  return f(**kwargs)\n",
            "  4%|█▍                                     | 101/2660 [00:00<00:10, 233.29it/s]\n",
            "(9090, 30, 10) (9090, 6) (1111, 30, 10) (1111, 6)\n",
            "(9090, 30, 10) (9090, 6) (1111, 30, 10) (1111, 6)\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 77039\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                   _step 2\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                    loss 0.003963266789991376\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                      nd 0.09898032989723443\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              _timestamp 1650178624.5004556\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                _runtime 7048.82871556282\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                val_loss 0.01913442992971974\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                  val_nd 0.09360211996866684\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             best-nd_std 0.011848253351497527\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     best-nrmse_mean_std 0.029974552115326708\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       best-nrmse_maxmin 0.01894819066276385\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   best-nrmse_maxmin_std 0.0031278261189717606\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         best-nrmse_mean 0.18158411206658567\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              best-nd_95 0.0006967121335456656\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      best-nrmse_mean_95 0.0017625917961743693\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    best-nrmse_maxmin_95 0.00018392537229407243\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 best-nd 0.1108517235361908\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 8 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced gallant-pine-365: https://app.wandb.ai/matintavakoli/Interpreting_TS/runs/3psk8r3o\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.9.7\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Wandb version 0.12.14 is available!  To upgrade, please run:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in wandb/run-20220417_065708-217y26w5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mpleasant-dream-366\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://app.wandb.ai/matintavakoli/Interpreting_TS\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://app.wandb.ai/matintavakoli/Interpreting_TS/runs/217y26w5\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb off` to turn off syncing.\n",
            "\n",
            "/usr/local/envs/eval-metric/lib/python3.7/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  return f(**kwargs)\n",
            "/usr/local/envs/eval-metric/lib/python3.7/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  return f(**kwargs)\n",
            "  4%|█▍                                     | 101/2660 [00:00<00:10, 248.69it/s]\n",
            "(9090, 30, 10) (9090, 6) (1111, 30, 10) (1111, 6)\n",
            "(9090, 30, 10) (9090, 6) (1111, 30, 10) (1111, 6)\n",
            "Model: \"sequential_76\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "reshape_38 (Reshape)         (None, 300)               0         \n",
            "_________________________________________________________________\n",
            "dense_156 (Dense)            (None, 16)                4816      \n",
            "_________________________________________________________________\n",
            "dropout_118 (Dropout)        (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_157 (Dense)            (None, 16)                272       \n",
            "_________________________________________________________________\n",
            "dropout_119 (Dropout)        (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_158 (Dense)            (None, 16)                272       \n",
            "_________________________________________________________________\n",
            "dropout_120 (Dropout)        (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_159 (Dense)            (None, 16)                272       \n",
            "_________________________________________________________________\n",
            "dropout_121 (Dropout)        (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_160 (Dense)            (None, 6)                 102       \n",
            "=================================================================\n",
            "Total params: 5,734\n",
            "Trainable params: 5,734\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/200\n",
            "18/18 [==============================] - 0s 10ms/step - loss: 1.3354 - nd: 1.4677 - val_loss: 0.9782 - val_nd: 1.1355\n",
            "Epoch 2/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 1.2327 - nd: 1.3017 - val_loss: 0.8976 - val_nd: 1.0209\n",
            "Epoch 3/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 1.1742 - nd: 1.1938 - val_loss: 0.8579 - val_nd: 0.9747\n",
            "Epoch 4/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 1.1327 - nd: 1.1305 - val_loss: 0.8414 - val_nd: 0.9528\n",
            "Epoch 5/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 1.1120 - nd: 1.0906 - val_loss: 0.8347 - val_nd: 0.9414\n",
            "Epoch 6/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 1.0971 - nd: 1.0542 - val_loss: 0.8319 - val_nd: 0.9348\n",
            "Epoch 7/200\n",
            "18/18 [==============================] - 0s 8ms/step - loss: 1.0837 - nd: 1.0350 - val_loss: 0.8307 - val_nd: 0.9326\n",
            "Epoch 8/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 1.0658 - nd: 1.0103 - val_loss: 0.8300 - val_nd: 0.9305\n",
            "Epoch 9/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 1.0604 - nd: 0.9996 - val_loss: 0.8298 - val_nd: 0.9309\n",
            "Epoch 10/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 1.0526 - nd: 0.9827 - val_loss: 0.8296 - val_nd: 0.9299\n",
            "Epoch 11/200\n",
            "18/18 [==============================] - 0s 8ms/step - loss: 1.0458 - nd: 0.9748 - val_loss: 0.8293 - val_nd: 0.9290\n",
            "Epoch 12/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 1.0411 - nd: 0.9629 - val_loss: 0.8292 - val_nd: 0.9282\n",
            "Epoch 13/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 1.0290 - nd: 0.9512 - val_loss: 0.8291 - val_nd: 0.9279\n",
            "Epoch 14/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 1.0301 - nd: 0.9456 - val_loss: 0.8291 - val_nd: 0.9282\n",
            "Epoch 15/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 1.0260 - nd: 0.9423 - val_loss: 0.8289 - val_nd: 0.9268\n",
            "Epoch 16/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 1.0244 - nd: 0.9376 - val_loss: 0.8288 - val_nd: 0.9260\n",
            "Epoch 17/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 1.0287 - nd: 0.9335 - val_loss: 0.8289 - val_nd: 0.9262\n",
            "Epoch 18/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 1.0185 - nd: 0.9286 - val_loss: 0.8289 - val_nd: 0.9262\n",
            "Epoch 19/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 1.0206 - nd: 0.9288 - val_loss: 0.8290 - val_nd: 0.9273\n",
            "Epoch 20/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 1.0194 - nd: 0.9240 - val_loss: 0.8289 - val_nd: 0.9262\n",
            "Epoch 21/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 1.0157 - nd: 0.9220 - val_loss: 0.8289 - val_nd: 0.9258\n",
            "Epoch 22/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 1.0192 - nd: 0.9201 - val_loss: 0.8289 - val_nd: 0.9260\n",
            "Epoch 23/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 1.0167 - nd: 0.9179 - val_loss: 0.8290 - val_nd: 0.9265\n",
            "Epoch 24/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 1.0079 - nd: 0.9191 - val_loss: 0.8289 - val_nd: 0.9256\n",
            "Epoch 25/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 1.0153 - nd: 0.9141 - val_loss: 0.8288 - val_nd: 0.9250\n",
            "Epoch 26/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 1.0141 - nd: 0.9128 - val_loss: 0.8288 - val_nd: 0.9253\n",
            "Epoch 27/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 1.0111 - nd: 0.9120 - val_loss: 0.8287 - val_nd: 0.9244\n",
            "Epoch 28/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 1.0075 - nd: 0.9111 - val_loss: 0.8286 - val_nd: 0.9238\n",
            "Epoch 29/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 1.0046 - nd: 0.9096 - val_loss: 0.8286 - val_nd: 0.9237\n",
            "Epoch 30/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 1.0046 - nd: 0.9071 - val_loss: 0.8285 - val_nd: 0.9222\n",
            "Epoch 31/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 1.0031 - nd: 0.9049 - val_loss: 0.8283 - val_nd: 0.9213\n",
            "Epoch 32/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 1.0091 - nd: 0.9046 - val_loss: 0.8283 - val_nd: 0.9209\n",
            "Epoch 33/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 1.0081 - nd: 0.9043 - val_loss: 0.8283 - val_nd: 0.9210\n",
            "Epoch 34/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 1.0064 - nd: 0.9024 - val_loss: 0.8282 - val_nd: 0.9201\n",
            "Epoch 35/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 1.0108 - nd: 0.9022 - val_loss: 0.8282 - val_nd: 0.9200\n",
            "Epoch 36/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 1.0096 - nd: 0.9005 - val_loss: 0.8282 - val_nd: 0.9205\n",
            "Epoch 37/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 1.0105 - nd: 0.9015 - val_loss: 0.8281 - val_nd: 0.9199\n",
            "Epoch 38/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 1.0076 - nd: 0.8997 - val_loss: 0.8281 - val_nd: 0.9196\n",
            "Epoch 39/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 1.0041 - nd: 0.8990 - val_loss: 0.8280 - val_nd: 0.9191\n",
            "Epoch 40/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 1.0049 - nd: 0.8984 - val_loss: 0.8279 - val_nd: 0.9180\n",
            "Epoch 41/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 1.0062 - nd: 0.8978 - val_loss: 0.8279 - val_nd: 0.9185\n",
            "Epoch 42/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 1.0074 - nd: 0.8968 - val_loss: 0.8280 - val_nd: 0.9188\n",
            "Epoch 43/200\n",
            "18/18 [==============================] - 0s 8ms/step - loss: 1.0057 - nd: 0.8976 - val_loss: 0.8279 - val_nd: 0.9180\n",
            "Epoch 44/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 1.0056 - nd: 0.8952 - val_loss: 0.8278 - val_nd: 0.9177\n",
            "Epoch 45/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 1.0031 - nd: 0.8961 - val_loss: 0.8278 - val_nd: 0.9172\n",
            "Epoch 46/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 1.0053 - nd: 0.8956 - val_loss: 0.8278 - val_nd: 0.9174\n",
            "Epoch 47/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.9999 - nd: 0.8948 - val_loss: 0.8277 - val_nd: 0.9170\n",
            "Epoch 48/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 1.0034 - nd: 0.8940 - val_loss: 0.8277 - val_nd: 0.9164\n",
            "Epoch 49/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 1.0032 - nd: 0.8943 - val_loss: 0.8276 - val_nd: 0.9162\n",
            "Epoch 50/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 1.0051 - nd: 0.8939 - val_loss: 0.8277 - val_nd: 0.9164\n",
            "Epoch 51/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 1.0074 - nd: 0.8937 - val_loss: 0.8276 - val_nd: 0.9161\n",
            "Epoch 52/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 1.0078 - nd: 0.8942 - val_loss: 0.8276 - val_nd: 0.9162\n",
            "Epoch 53/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 1.0058 - nd: 0.8948 - val_loss: 0.8276 - val_nd: 0.9161\n",
            "Epoch 54/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 1.0063 - nd: 0.8925 - val_loss: 0.8275 - val_nd: 0.9154\n",
            "Epoch 55/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 1.0057 - nd: 0.8915 - val_loss: 0.8275 - val_nd: 0.9149\n",
            "Epoch 56/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 1.0007 - nd: 0.8913 - val_loss: 0.8275 - val_nd: 0.9148\n",
            "Epoch 57/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.9983 - nd: 0.8896 - val_loss: 0.8274 - val_nd: 0.9145\n",
            "Epoch 58/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 1.0064 - nd: 0.8909 - val_loss: 0.8274 - val_nd: 0.9141\n",
            "Epoch 59/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 1.0068 - nd: 0.8907 - val_loss: 0.8274 - val_nd: 0.9144\n",
            "Epoch 60/200\n",
            "18/18 [==============================] - 0s 8ms/step - loss: 0.9965 - nd: 0.8901 - val_loss: 0.8273 - val_nd: 0.9139\n",
            "Epoch 61/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 1.0064 - nd: 0.8905 - val_loss: 0.8274 - val_nd: 0.9142\n",
            "Epoch 62/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 1.0080 - nd: 0.8920 - val_loss: 0.8274 - val_nd: 0.9146\n",
            "Epoch 63/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 1.0064 - nd: 0.8909 - val_loss: 0.8274 - val_nd: 0.9146\n",
            "Epoch 64/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 1.0014 - nd: 0.8904 - val_loss: 0.8274 - val_nd: 0.9142\n",
            "Epoch 65/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 1.0050 - nd: 0.8894 - val_loss: 0.8273 - val_nd: 0.9136\n",
            "Epoch 66/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 1.0019 - nd: 0.8889 - val_loss: 0.8272 - val_nd: 0.9130\n",
            "Epoch 67/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 1.0027 - nd: 0.8895 - val_loss: 0.8273 - val_nd: 0.9141\n",
            "Epoch 68/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 1.0009 - nd: 0.8892 - val_loss: 0.8272 - val_nd: 0.9131\n",
            "Epoch 69/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 1.0052 - nd: 0.8884 - val_loss: 0.8272 - val_nd: 0.9129\n",
            "Epoch 70/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 1.0014 - nd: 0.8880 - val_loss: 0.8272 - val_nd: 0.9124\n",
            "Epoch 71/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 1.0027 - nd: 0.8868 - val_loss: 0.8271 - val_nd: 0.9114\n",
            "Epoch 72/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 1.0029 - nd: 0.8863 - val_loss: 0.8271 - val_nd: 0.9117\n",
            "Epoch 73/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 1.0030 - nd: 0.8878 - val_loss: 0.8272 - val_nd: 0.9125\n",
            "Epoch 74/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 1.0071 - nd: 0.8873 - val_loss: 0.8271 - val_nd: 0.9119\n",
            "Epoch 75/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.9980 - nd: 0.8874 - val_loss: 0.8272 - val_nd: 0.9128\n",
            "Epoch 76/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 1.0041 - nd: 0.8884 - val_loss: 0.8272 - val_nd: 0.9129\n",
            "Epoch 77/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 1.0035 - nd: 0.8880 - val_loss: 0.8272 - val_nd: 0.9124\n",
            "Epoch 78/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 1.0106 - nd: 0.8884 - val_loss: 0.8272 - val_nd: 0.9130\n",
            "Epoch 79/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 1.0081 - nd: 0.8887 - val_loss: 0.8272 - val_nd: 0.9131\n",
            "Epoch 80/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 1.0101 - nd: 0.8889 - val_loss: 0.8272 - val_nd: 0.9135\n",
            "Epoch 81/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 1.0039 - nd: 0.8887 - val_loss: 0.8272 - val_nd: 0.9128\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 77157\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                   epoch 80\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                    loss 1.0039082765579224\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                   _step 81\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                val_loss 0.8271778225898743\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                      nd 0.8886876702308655\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              _timestamp 1650178639.7238543\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                _runtime 7064.052114248276\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                  val_nd 0.9128158092498779\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           best_val_loss 0.8270898461341858\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              best_epoch 70\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             best-nd_std 0.03334618115850539\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     best-nrmse_mean_std 0.3480879200966695\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       best-nrmse_maxmin 0.1392169451921143\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   best-nrmse_maxmin_std 0.03632276085353781\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         best-nrmse_mean 1.334142442793281\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              best-nd_95 0.001960853497246169\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      best-nrmse_mean_95 0.020468593156929032\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    best-nrmse_maxmin_95 0.002135885135114769\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 best-nd 0.9880137198029608\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 7 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced pleasant-dream-366: https://app.wandb.ai/matintavakoli/Interpreting_TS/runs/217y26w5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.9.7\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Wandb version 0.12.14 is available!  To upgrade, please run:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in wandb/run-20220417_065724-3q33wmi2\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mmajor-night-367\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://app.wandb.ai/matintavakoli/Interpreting_TS\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://app.wandb.ai/matintavakoli/Interpreting_TS/runs/3q33wmi2\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb off` to turn off syncing.\n",
            "\n",
            "/usr/local/envs/eval-metric/lib/python3.7/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  return f(**kwargs)\n",
            "/usr/local/envs/eval-metric/lib/python3.7/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  return f(**kwargs)\n",
            "  4%|█▍                                     | 101/2660 [00:00<00:10, 245.35it/s]\n",
            "(9090, 30, 10) (9090, 6) (1111, 30, 10) (1111, 6)\n",
            "(9090, 30, 10) (9090, 6) (1111, 30, 10) (1111, 6)\n",
            "Model: \"sequential_77\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm_80 (LSTM)               (None, 30, 16)            1728      \n",
            "_________________________________________________________________\n",
            "lstm_81 (LSTM)               (None, 30, 16)            2112      \n",
            "_________________________________________________________________\n",
            "lstm_82 (LSTM)               (None, 30, 16)            2112      \n",
            "_________________________________________________________________\n",
            "lstm_83 (LSTM)               (None, 16)                2112      \n",
            "_________________________________________________________________\n",
            "dropout_122 (Dropout)        (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_161 (Dense)            (None, 6)                 102       \n",
            "=================================================================\n",
            "Total params: 8,166\n",
            "Trainable params: 8,166\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/200\n",
            "18/18 [==============================] - 1s 72ms/step - loss: 1.0102 - nd: 0.9429 - val_loss: 0.8180 - val_nd: 0.9157\n",
            "Epoch 2/200\n",
            "18/18 [==============================] - 0s 15ms/step - loss: 1.0044 - nd: 0.9167 - val_loss: 0.8029 - val_nd: 0.9018\n",
            "Epoch 3/200\n",
            "18/18 [==============================] - 0s 13ms/step - loss: 0.9933 - nd: 0.8996 - val_loss: 0.7909 - val_nd: 0.8740\n",
            "Epoch 4/200\n",
            "18/18 [==============================] - 0s 15ms/step - loss: 0.9614 - nd: 0.8654 - val_loss: 0.7562 - val_nd: 0.7019\n",
            "Epoch 5/200\n",
            "18/18 [==============================] - 0s 13ms/step - loss: 0.8766 - nd: 0.7479 - val_loss: 0.7350 - val_nd: 0.6688\n",
            "Epoch 6/200\n",
            "18/18 [==============================] - 0s 16ms/step - loss: 0.7855 - nd: 0.6510 - val_loss: 0.7042 - val_nd: 0.7263\n",
            "Epoch 7/200\n",
            "18/18 [==============================] - 0s 14ms/step - loss: 0.7406 - nd: 0.6132 - val_loss: 0.6467 - val_nd: 0.7080\n",
            "Epoch 8/200\n",
            "18/18 [==============================] - 0s 14ms/step - loss: 0.7039 - nd: 0.5895 - val_loss: 0.6011 - val_nd: 0.6970\n",
            "Epoch 9/200\n",
            "18/18 [==============================] - 0s 14ms/step - loss: 0.6809 - nd: 0.5578 - val_loss: 0.5507 - val_nd: 0.6372\n",
            "Epoch 10/200\n",
            "18/18 [==============================] - 0s 14ms/step - loss: 0.6735 - nd: 0.5425 - val_loss: 0.5367 - val_nd: 0.6079\n",
            "Epoch 11/200\n",
            "18/18 [==============================] - 0s 14ms/step - loss: 0.6567 - nd: 0.5259 - val_loss: 0.5268 - val_nd: 0.5883\n",
            "Epoch 12/200\n",
            "18/18 [==============================] - 0s 13ms/step - loss: 0.6450 - nd: 0.5162 - val_loss: 0.5094 - val_nd: 0.5711\n",
            "Epoch 13/200\n",
            "18/18 [==============================] - 0s 14ms/step - loss: 0.6363 - nd: 0.4945 - val_loss: 0.4854 - val_nd: 0.5678\n",
            "Epoch 14/200\n",
            "18/18 [==============================] - 0s 14ms/step - loss: 0.6170 - nd: 0.4826 - val_loss: 0.4677 - val_nd: 0.5383\n",
            "Epoch 15/200\n",
            "18/18 [==============================] - 0s 14ms/step - loss: 0.6121 - nd: 0.4796 - val_loss: 0.4578 - val_nd: 0.5044\n",
            "Epoch 16/200\n",
            "18/18 [==============================] - 0s 13ms/step - loss: 0.6080 - nd: 0.4571 - val_loss: 0.4381 - val_nd: 0.4764\n",
            "Epoch 17/200\n",
            "18/18 [==============================] - 0s 12ms/step - loss: 0.5906 - nd: 0.4509 - val_loss: 0.4528 - val_nd: 0.4475\n",
            "Epoch 18/200\n",
            "18/18 [==============================] - 0s 12ms/step - loss: 0.5978 - nd: 0.4416 - val_loss: 0.4450 - val_nd: 0.4564\n",
            "Epoch 19/200\n",
            "18/18 [==============================] - 0s 10ms/step - loss: 0.5765 - nd: 0.4317 - val_loss: 0.4494 - val_nd: 0.4752\n",
            "Epoch 20/200\n",
            "18/18 [==============================] - 0s 13ms/step - loss: 0.5715 - nd: 0.4256 - val_loss: 0.4542 - val_nd: 0.4886\n",
            "Epoch 21/200\n",
            "18/18 [==============================] - 0s 10ms/step - loss: 0.5714 - nd: 0.4285 - val_loss: 0.4758 - val_nd: 0.4904\n",
            "Epoch 22/200\n",
            "18/18 [==============================] - 0s 11ms/step - loss: 0.5575 - nd: 0.4252 - val_loss: 0.4802 - val_nd: 0.4994\n",
            "Epoch 23/200\n",
            "18/18 [==============================] - 0s 11ms/step - loss: 0.5522 - nd: 0.4173 - val_loss: 0.4695 - val_nd: 0.4806\n",
            "Epoch 24/200\n",
            "18/18 [==============================] - 0s 10ms/step - loss: 0.5512 - nd: 0.4177 - val_loss: 0.4848 - val_nd: 0.4959\n",
            "Epoch 25/200\n",
            "18/18 [==============================] - 0s 11ms/step - loss: 0.5449 - nd: 0.4089 - val_loss: 0.4588 - val_nd: 0.4862\n",
            "Epoch 26/200\n",
            "18/18 [==============================] - 0s 12ms/step - loss: 0.5428 - nd: 0.4031 - val_loss: 0.4403 - val_nd: 0.4634\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 77457\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                   epoch 25\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                    loss 0.5428140759468079\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                   _step 26\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                val_loss 0.4402945637702942\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                      nd 0.4030732810497284\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              _timestamp 1650178661.0028312\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                _runtime 7085.331091165543\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                  val_nd 0.4634265899658203\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           best_val_loss 0.4380553364753723\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              best_epoch 15\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             best-nd_std 0.12406579149688358\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     best-nrmse_mean_std 0.17368140599048673\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       best-nrmse_maxmin 0.07521585331046043\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   best-nrmse_maxmin_std 0.018123548133318342\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         best-nrmse_mean 0.7208078164186211\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              best-nd_95 0.007295433320802543\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      best-nrmse_mean_95 0.010212977333874175\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    best-nrmse_maxmin_95 0.001065717917467206\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 best-nd 0.5591199405157105\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 7 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced major-night-367: https://app.wandb.ai/matintavakoli/Interpreting_TS/runs/3q33wmi2\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.9.7\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Wandb version 0.12.14 is available!  To upgrade, please run:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in wandb/run-20220417_065746-3cjxcvhq\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mtrim-dawn-368\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://app.wandb.ai/matintavakoli/Interpreting_TS\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://app.wandb.ai/matintavakoli/Interpreting_TS/runs/3cjxcvhq\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb off` to turn off syncing.\n",
            "\n",
            "/usr/local/envs/eval-metric/lib/python3.7/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  return f(**kwargs)\n",
            "/usr/local/envs/eval-metric/lib/python3.7/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  return f(**kwargs)\n",
            "  4%|█▍                                     | 101/2660 [00:00<00:10, 241.86it/s]\n",
            "(9090, 30, 10) (9090, 6) (1111, 30, 10) (1111, 6)\n",
            "(9090, 30, 10) (9090, 6) (1111, 30, 10) (1111, 6)\n"
          ]
        }
      ]
    }
  ]
}