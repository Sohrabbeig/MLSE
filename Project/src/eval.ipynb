{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "eval.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "%env PYTHONPATH=\n",
        "!echo $PYTHONPATH"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ie26w8D29ewo",
        "outputId": "90f26ba0-e32e-4503-babf-c3b419df2d7a"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "env: PYTHONPATH=\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "MINICONDA_INSTALLER_SCRIPT=Miniconda3-4.5.4-Linux-x86_64.sh\n",
        "MINICONDA_PREFIX=/usr/local\n",
        "wget https://repo.continuum.io/miniconda/$MINICONDA_INSTALLER_SCRIPT\n",
        "chmod +x $MINICONDA_INSTALLER_SCRIPT\n",
        "./$MINICONDA_INSTALLER_SCRIPT -b -f -p $MINICONDA_PREFIX"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6jhI4Q209_3x",
        "outputId": "af04b7ba-89d2-4557-9021-3b2da0bbeec7"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PREFIX=/usr/local\n",
            "installing: python-3.6.5-hc3d631a_2 ...\n",
            "installing: ca-certificates-2018.03.07-0 ...\n",
            "installing: conda-env-2.6.0-h36134e3_1 ...\n",
            "installing: libgcc-ng-7.2.0-hdf63c60_3 ...\n",
            "installing: libstdcxx-ng-7.2.0-hdf63c60_3 ...\n",
            "installing: libffi-3.2.1-hd88cf55_4 ...\n",
            "installing: ncurses-6.1-hf484d3e_0 ...\n",
            "installing: openssl-1.0.2o-h20670df_0 ...\n",
            "installing: tk-8.6.7-hc745277_3 ...\n",
            "installing: xz-5.2.4-h14c3975_4 ...\n",
            "installing: yaml-0.1.7-had09818_2 ...\n",
            "installing: zlib-1.2.11-ha838bed_2 ...\n",
            "installing: libedit-3.1.20170329-h6b74fdf_2 ...\n",
            "installing: readline-7.0-ha6073c6_4 ...\n",
            "installing: sqlite-3.23.1-he433501_0 ...\n",
            "installing: asn1crypto-0.24.0-py36_0 ...\n",
            "installing: certifi-2018.4.16-py36_0 ...\n",
            "installing: chardet-3.0.4-py36h0f667ec_1 ...\n",
            "installing: idna-2.6-py36h82fb2a8_1 ...\n",
            "installing: pycosat-0.6.3-py36h0a5515d_0 ...\n",
            "installing: pycparser-2.18-py36hf9f622e_1 ...\n",
            "installing: pysocks-1.6.8-py36_0 ...\n",
            "installing: ruamel_yaml-0.15.37-py36h14c3975_2 ...\n",
            "installing: six-1.11.0-py36h372c433_1 ...\n",
            "installing: cffi-1.11.5-py36h9745a5d_0 ...\n",
            "installing: setuptools-39.2.0-py36_0 ...\n",
            "installing: cryptography-2.2.2-py36h14c3975_0 ...\n",
            "installing: wheel-0.31.1-py36_0 ...\n",
            "installing: pip-10.0.1-py36_0 ...\n",
            "installing: pyopenssl-18.0.0-py36_0 ...\n",
            "installing: urllib3-1.22-py36hbe7ace6_0 ...\n",
            "installing: requests-2.18.4-py36he2e5f8d_1 ...\n",
            "installing: conda-4.5.4-py36_0 ...\n",
            "installation finished.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "--2022-04-17 18:22:00--  https://repo.continuum.io/miniconda/Miniconda3-4.5.4-Linux-x86_64.sh\n",
            "Resolving repo.continuum.io (repo.continuum.io)... 104.18.200.79, 104.18.201.79, 2606:4700::6812:c94f, ...\n",
            "Connecting to repo.continuum.io (repo.continuum.io)|104.18.200.79|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://repo.anaconda.com/miniconda/Miniconda3-4.5.4-Linux-x86_64.sh [following]\n",
            "--2022-04-17 18:22:00--  https://repo.anaconda.com/miniconda/Miniconda3-4.5.4-Linux-x86_64.sh\n",
            "Resolving repo.anaconda.com (repo.anaconda.com)... 104.16.131.3, 104.16.130.3, 2606:4700::6810:8303, ...\n",
            "Connecting to repo.anaconda.com (repo.anaconda.com)|104.16.131.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 58468498 (56M) [application/x-sh]\n",
            "Saving to: ‘Miniconda3-4.5.4-Linux-x86_64.sh’\n",
            "\n",
            "     0K .......... .......... .......... .......... ..........  0% 78.9M 1s\n",
            "    50K .......... .......... .......... .......... ..........  0% 28.9M 1s\n",
            "   100K .......... .......... .......... .......... ..........  0% 56.9M 1s\n",
            "   150K .......... .......... .......... .......... ..........  0%  137M 1s\n",
            "   200K .......... .......... .......... .......... ..........  0%  114M 1s\n",
            "   250K .......... .......... .......... .......... ..........  0% 65.6M 1s\n",
            "   300K .......... .......... .......... .......... ..........  0%  219M 1s\n",
            "   350K .......... .......... .......... .......... ..........  0%  141M 1s\n",
            "   400K .......... .......... .......... .......... ..........  0%  217M 1s\n",
            "   450K .......... .......... .......... .......... ..........  0%  236M 1s\n",
            "   500K .......... .......... .......... .......... ..........  0%  181M 1s\n",
            "   550K .......... .......... .......... .......... ..........  1%  160M 1s\n",
            "   600K .......... .......... .......... .......... ..........  1%  173M 1s\n",
            "   650K .......... .......... .......... .......... ..........  1%  188M 1s\n",
            "   700K .......... .......... .......... .......... ..........  1%  192M 1s\n",
            "   750K .......... .......... .......... .......... ..........  1%  193M 1s\n",
            "   800K .......... .......... .......... .......... ..........  1%  215M 0s\n",
            "   850K .......... .......... .......... .......... ..........  1%  151M 0s\n",
            "   900K .......... .......... .......... .......... ..........  1%  175M 0s\n",
            "   950K .......... .......... .......... .......... ..........  1%  162M 0s\n",
            "  1000K .......... .......... .......... .......... ..........  1%  139M 0s\n",
            "  1050K .......... .......... .......... .......... ..........  1%  137M 0s\n",
            "  1100K .......... .......... .......... .......... ..........  2%  180M 0s\n",
            "  1150K .......... .......... .......... .......... ..........  2%  103M 0s\n",
            "  1200K .......... .......... .......... .......... ..........  2%  244M 0s\n",
            "  1250K .......... .......... .......... .......... ..........  2%  244M 0s\n",
            "  1300K .......... .......... .......... .......... ..........  2%  185M 0s\n",
            "  1350K .......... .......... .......... .......... ..........  2%  218M 0s\n",
            "  1400K .......... .......... .......... .......... ..........  2%  320M 0s\n",
            "  1450K .......... .......... .......... .......... ..........  2%  412M 0s\n",
            "  1500K .......... .......... .......... .......... ..........  2%  367M 0s\n",
            "  1550K .......... .......... .......... .......... ..........  2%  280M 0s\n",
            "  1600K .......... .......... .......... .......... ..........  2%  283M 0s\n",
            "  1650K .......... .......... .......... .......... ..........  2%  241M 0s\n",
            "  1700K .......... .......... .......... .......... ..........  3%  288M 0s\n",
            "  1750K .......... .......... .......... .......... ..........  3%  265M 0s\n",
            "  1800K .......... .......... .......... .......... ..........  3%  297M 0s\n",
            "  1850K .......... .......... .......... .......... ..........  3%  249M 0s\n",
            "  1900K .......... .......... .......... .......... ..........  3% 80.7M 0s\n",
            "  1950K .......... .......... .......... .......... ..........  3% 65.3M 0s\n",
            "  2000K .......... .......... .......... .......... ..........  3%  240M 0s\n",
            "  2050K .......... .......... .......... .......... ..........  3%  253M 0s\n",
            "  2100K .......... .......... .......... .......... ..........  3%  242M 0s\n",
            "  2150K .......... .......... .......... .......... ..........  3%  211M 0s\n",
            "  2200K .......... .......... .......... .......... ..........  3%  253M 0s\n",
            "  2250K .......... .......... .......... .......... ..........  4%  270M 0s\n",
            "  2300K .......... .......... .......... .......... ..........  4%  230M 0s\n",
            "  2350K .......... .......... .......... .......... ..........  4%  200M 0s\n",
            "  2400K .......... .......... .......... .......... ..........  4%  223M 0s\n",
            "  2450K .......... .......... .......... .......... ..........  4%  201M 0s\n",
            "  2500K .......... .......... .......... .......... ..........  4%  201M 0s\n",
            "  2550K .......... .......... .......... .......... ..........  4%  166M 0s\n",
            "  2600K .......... .......... .......... .......... ..........  4%  195M 0s\n",
            "  2650K .......... .......... .......... .......... ..........  4%  244M 0s\n",
            "  2700K .......... .......... .......... .......... ..........  4%  256M 0s\n",
            "  2750K .......... .......... .......... .......... ..........  4%  195M 0s\n",
            "  2800K .......... .......... .......... .......... ..........  4%  226M 0s\n",
            "  2850K .......... .......... .......... .......... ..........  5%  259M 0s\n",
            "  2900K .......... .......... .......... .......... ..........  5%  256M 0s\n",
            "  2950K .......... .......... .......... .......... ..........  5%  364M 0s\n",
            "  3000K .......... .......... .......... .......... ..........  5%  414M 0s\n",
            "  3050K .......... .......... .......... .......... ..........  5%  291M 0s\n",
            "  3100K .......... .......... .......... .......... ..........  5%  250M 0s\n",
            "  3150K .......... .......... .......... .......... ..........  5%  252M 0s\n",
            "  3200K .......... .......... .......... .......... ..........  5%  344M 0s\n",
            "  3250K .......... .......... .......... .......... ..........  5%  296M 0s\n",
            "  3300K .......... .......... .......... .......... ..........  5%  396M 0s\n",
            "  3350K .......... .......... .......... .......... ..........  5%  272M 0s\n",
            "  3400K .......... .......... .......... .......... ..........  6%  438M 0s\n",
            "  3450K .......... .......... .......... .......... ..........  6%  364M 0s\n",
            "  3500K .......... .......... .......... .......... ..........  6%  401M 0s\n",
            "  3550K .......... .......... .......... .......... ..........  6%  284M 0s\n",
            "  3600K .......... .......... .......... .......... ..........  6%  261M 0s\n",
            "  3650K .......... .......... .......... .......... ..........  6%  334M 0s\n",
            "  3700K .......... .......... .......... .......... ..........  6%  406M 0s\n",
            "  3750K .......... .......... .......... .......... ..........  6%  361M 0s\n",
            "  3800K .......... .......... .......... .......... ..........  6%  239M 0s\n",
            "  3850K .......... .......... .......... .......... ..........  6%  270M 0s\n",
            "  3900K .......... .......... .......... .......... ..........  6%  241M 0s\n",
            "  3950K .......... .......... .......... .......... ..........  7%  207M 0s\n",
            "  4000K .......... .......... .......... .......... ..........  7%  250M 0s\n",
            "  4050K .......... .......... .......... .......... ..........  7%  250M 0s\n",
            "  4100K .......... .......... .......... .......... ..........  7%  244M 0s\n",
            "  4150K .......... .......... .......... .......... ..........  7%  224M 0s\n",
            "  4200K .......... .......... .......... .......... ..........  7%  251M 0s\n",
            "  4250K .......... .......... .......... .......... ..........  7%  249M 0s\n",
            "  4300K .......... .......... .......... .......... ..........  7% 90.4M 0s\n",
            "  4350K .......... .......... .......... .......... ..........  7%  194M 0s\n",
            "  4400K .......... .......... .......... .......... ..........  7%  208M 0s\n",
            "  4450K .......... .......... .......... .......... ..........  7%  197M 0s\n",
            "  4500K .......... .......... .......... .......... ..........  7%  241M 0s\n",
            "  4550K .......... .......... .......... .......... ..........  8%  221M 0s\n",
            "  4600K .......... .......... .......... .......... ..........  8%  204M 0s\n",
            "  4650K .......... .......... .......... .......... ..........  8%  205M 0s\n",
            "  4700K .......... .......... .......... .......... ..........  8%  193M 0s\n",
            "  4750K .......... .......... .......... .......... ..........  8%  169M 0s\n",
            "  4800K .......... .......... .......... .......... ..........  8%  236M 0s\n",
            "  4850K .......... .......... .......... .......... ..........  8%  188M 0s\n",
            "  4900K .......... .......... .......... .......... ..........  8%  202M 0s\n",
            "  4950K .......... .......... .......... .......... ..........  8%  206M 0s\n",
            "  5000K .......... .......... .......... .......... ..........  8%  207M 0s\n",
            "  5050K .......... .......... .......... .......... ..........  8%  208M 0s\n",
            "  5100K .......... .......... .......... .......... ..........  9%  219M 0s\n",
            "  5150K .......... .......... .......... .......... ..........  9%  182M 0s\n",
            "  5200K .......... .......... .......... .......... ..........  9%  225M 0s\n",
            "  5250K .......... .......... .......... .......... ..........  9%  239M 0s\n",
            "  5300K .......... .......... .......... .......... ..........  9%  260M 0s\n",
            "  5350K .......... .......... .......... .......... ..........  9%  230M 0s\n",
            "  5400K .......... .......... .......... .......... ..........  9%  254M 0s\n",
            "  5450K .......... .......... .......... .......... ..........  9%  236M 0s\n",
            "  5500K .......... .......... .......... .......... ..........  9%  249M 0s\n",
            "  5550K .......... .......... .......... .......... ..........  9%  187M 0s\n",
            "  5600K .......... .......... .......... .......... ..........  9%  236M 0s\n",
            "  5650K .......... .......... .......... .......... ..........  9%  211M 0s\n",
            "  5700K .......... .......... .......... .......... .......... 10%  178M 0s\n",
            "  5750K .......... .......... .......... .......... .......... 10%  199M 0s\n",
            "  5800K .......... .......... .......... .......... .......... 10%  185M 0s\n",
            "  5850K .......... .......... .......... .......... .......... 10%  190M 0s\n",
            "  5900K .......... .......... .......... .......... .......... 10%  200M 0s\n",
            "  5950K .......... .......... .......... .......... .......... 10%  159M 0s\n",
            "  6000K .......... .......... .......... .......... .......... 10%  199M 0s\n",
            "  6050K .......... .......... .......... .......... .......... 10%  229M 0s\n",
            "  6100K .......... .......... .......... .......... .......... 10%  227M 0s\n",
            "  6150K .......... .......... .......... .......... .......... 10%  200M 0s\n",
            "  6200K .......... .......... .......... .......... .......... 10%  240M 0s\n",
            "  6250K .......... .......... .......... .......... .......... 11%  233M 0s\n",
            "  6300K .......... .......... .......... .......... .......... 11%  216M 0s\n",
            "  6350K .......... .......... .......... .......... .......... 11%  175M 0s\n",
            "  6400K .......... .......... .......... .......... .......... 11%  232M 0s\n",
            "  6450K .......... .......... .......... .......... .......... 11%  200M 0s\n",
            "  6500K .......... .......... .......... .......... .......... 11%  230M 0s\n",
            "  6550K .......... .......... .......... .......... .......... 11%  161M 0s\n",
            "  6600K .......... .......... .......... .......... .......... 11%  200M 0s\n",
            "  6650K .......... .......... .......... .......... .......... 11%  192M 0s\n",
            "  6700K .......... .......... .......... .......... .......... 11%  194M 0s\n",
            "  6750K .......... .......... .......... .......... .......... 11%  154M 0s\n",
            "  6800K .......... .......... .......... .......... .......... 11%  179M 0s\n",
            "  6850K .......... .......... .......... .......... .......... 12%  187M 0s\n",
            "  6900K .......... .......... .......... .......... .......... 12%  201M 0s\n",
            "  6950K .......... .......... .......... .......... .......... 12%  215M 0s\n",
            "  7000K .......... .......... .......... .......... .......... 12%  232M 0s\n",
            "  7050K .......... .......... .......... .......... .......... 12%  234M 0s\n",
            "  7100K .......... .......... .......... .......... .......... 12%  244M 0s\n",
            "  7150K .......... .......... .......... .......... .......... 12%  181M 0s\n",
            "  7200K .......... .......... .......... .......... .......... 12%  216M 0s\n",
            "  7250K .......... .......... .......... .......... .......... 12%  221M 0s\n",
            "  7300K .......... .......... .......... .......... .......... 12%  255M 0s\n",
            "  7350K .......... .......... .......... .......... .......... 12%  224M 0s\n",
            "  7400K .......... .......... .......... .......... .......... 13%  246M 0s\n",
            "  7450K .......... .......... .......... .......... .......... 13%  262M 0s\n",
            "  7500K .......... .......... .......... .......... .......... 13%  254M 0s\n",
            "  7550K .......... .......... .......... .......... .......... 13%  218M 0s\n",
            "  7600K .......... .......... .......... .......... .......... 13%  253M 0s\n",
            "  7650K .......... .......... .......... .......... .......... 13%  237M 0s\n",
            "  7700K .......... .......... .......... .......... .......... 13%  210M 0s\n",
            "  7750K .......... .......... .......... .......... .......... 13%  184M 0s\n",
            "  7800K .......... .......... .......... .......... .......... 13%  208M 0s\n",
            "  7850K .......... .......... .......... .......... .......... 13%  205M 0s\n",
            "  7900K .......... .......... .......... .......... .......... 13%  249M 0s\n",
            "  7950K .......... .......... .......... .......... .......... 14%  217M 0s\n",
            "  8000K .......... .......... .......... .......... .......... 14%  246M 0s\n",
            "  8050K .......... .......... .......... .......... .......... 14%  211M 0s\n",
            "  8100K .......... .......... .......... .......... .......... 14%  250M 0s\n",
            "  8150K .......... .......... .......... .......... .......... 14%  231M 0s\n",
            "  8200K .......... .......... .......... .......... .......... 14%  255M 0s\n",
            "  8250K .......... .......... .......... .......... .......... 14%  256M 0s\n",
            "  8300K .......... .......... .......... .......... .......... 14%  207M 0s\n",
            "  8350K .......... .......... .......... .......... .......... 14%  187M 0s\n",
            "  8400K .......... .......... .......... .......... .......... 14%  222M 0s\n",
            "  8450K .......... .......... .......... .......... .......... 14%  237M 0s\n",
            "  8500K .......... .......... .......... .......... .......... 14%  236M 0s\n",
            "  8550K .......... .......... .......... .......... .......... 15%  218M 0s\n",
            "  8600K .......... .......... .......... .......... .......... 15%  244M 0s\n",
            "  8650K .......... .......... .......... .......... .......... 15%  219M 0s\n",
            "  8700K .......... .......... .......... .......... .......... 15%  226M 0s\n",
            "  8750K .......... .......... .......... .......... .......... 15%  175M 0s\n",
            "  8800K .......... .......... .......... .......... .......... 15%  253M 0s\n",
            "  8850K .......... .......... .......... .......... .......... 15%  261M 0s\n",
            "  8900K .......... .......... .......... .......... .......... 15%  259M 0s\n",
            "  8950K .......... .......... .......... .......... .......... 15%  202M 0s\n",
            "  9000K .......... .......... .......... .......... .......... 15%  246M 0s\n",
            "  9050K .......... .......... .......... .......... .......... 15%  257M 0s\n",
            "  9100K .......... .......... .......... .......... .......... 16%  259M 0s\n",
            "  9150K .......... .......... .......... .......... .......... 16%  213M 0s\n",
            "  9200K .......... .......... .......... .......... .......... 16%  243M 0s\n",
            "  9250K .......... .......... .......... .......... .......... 16%  246M 0s\n",
            "  9300K .......... .......... .......... .......... .......... 16%  255M 0s\n",
            "  9350K .......... .......... .......... .......... .......... 16%  197M 0s\n",
            "  9400K .......... .......... .......... .......... .......... 16%  225M 0s\n",
            "  9450K .......... .......... .......... .......... .......... 16%  262M 0s\n",
            "  9500K .......... .......... .......... .......... .......... 16%  161M 0s\n",
            "  9550K .......... .......... .......... .......... .......... 16%  249M 0s\n",
            "  9600K .......... .......... .......... .......... .......... 16%  321M 0s\n",
            "  9650K .......... .......... .......... .......... .......... 16%  301M 0s\n",
            "  9700K .......... .......... .......... .......... .......... 17%  254M 0s\n",
            "  9750K .......... .......... .......... .......... .......... 17%  195M 0s\n",
            "  9800K .......... .......... .......... .......... .......... 17%  248M 0s\n",
            "  9850K .......... .......... .......... .......... .......... 17%  229M 0s\n",
            "  9900K .......... .......... .......... .......... .......... 17%  340M 0s\n",
            "  9950K .......... .......... .......... .......... .......... 17%  321M 0s\n",
            " 10000K .......... .......... .......... .......... .......... 17%  412M 0s\n",
            " 10050K .......... .......... .......... .......... .......... 17%  258M 0s\n",
            " 10100K .......... .......... .......... .......... .......... 17%  228M 0s\n",
            " 10150K .......... .......... .......... .......... .......... 17%  231M 0s\n",
            " 10200K .......... .......... .......... .......... .......... 17%  248M 0s\n",
            " 10250K .......... .......... .......... .......... .......... 18%  233M 0s\n",
            " 10300K .......... .......... .......... .......... .......... 18%  239M 0s\n",
            " 10350K .......... .......... .......... .......... .......... 18%  163M 0s\n",
            " 10400K .......... .......... .......... .......... .......... 18%  270M 0s\n",
            " 10450K .......... .......... .......... .......... .......... 18%  223M 0s\n",
            " 10500K .......... .......... .......... .......... .......... 18%  252M 0s\n",
            " 10550K .......... .......... .......... .......... .......... 18%  311M 0s\n",
            " 10600K .......... .......... .......... .......... .......... 18%  407M 0s\n",
            " 10650K .......... .......... .......... .......... .......... 18%  256M 0s\n",
            " 10700K .......... .......... .......... .......... .......... 18%  226M 0s\n",
            " 10750K .......... .......... .......... .......... .......... 18%  212M 0s\n",
            " 10800K .......... .......... .......... .......... .......... 19%  212M 0s\n",
            " 10850K .......... .......... .......... .......... .......... 19%  252M 0s\n",
            " 10900K .......... .......... .......... .......... .......... 19%  283M 0s\n",
            " 10950K .......... .......... .......... .......... .......... 19%  265M 0s\n",
            " 11000K .......... .......... .......... .......... .......... 19%  278M 0s\n",
            " 11050K .......... .......... .......... .......... .......... 19% 74.3M 0s\n",
            " 11100K .......... .......... .......... .......... .......... 19% 56.5M 0s\n",
            " 11150K .......... .......... .......... .......... .......... 19% 50.1M 0s\n",
            " 11200K .......... .......... .......... .......... .......... 19% 51.4M 0s\n",
            " 11250K .......... .......... .......... .......... .......... 19% 57.6M 0s\n",
            " 11300K .......... .......... .......... .......... .......... 19% 62.5M 0s\n",
            " 11350K .......... .......... .......... .......... .......... 19% 55.1M 0s\n",
            " 11400K .......... .......... .......... .......... .......... 20% 55.9M 0s\n",
            " 11450K .......... .......... .......... .......... .......... 20% 57.3M 0s\n",
            " 11500K .......... .......... .......... .......... .......... 20% 54.5M 0s\n",
            " 11550K .......... .......... .......... .......... .......... 20% 51.6M 0s\n",
            " 11600K .......... .......... .......... .......... .......... 20% 58.6M 0s\n",
            " 11650K .......... .......... .......... .......... .......... 20% 59.4M 0s\n",
            " 11700K .......... .......... .......... .......... .......... 20% 67.2M 0s\n",
            " 11750K .......... .......... .......... .......... .......... 20%  207M 0s\n",
            " 11800K .......... .......... .......... .......... .......... 20%  260M 0s\n",
            " 11850K .......... .......... .......... .......... .......... 20%  263M 0s\n",
            " 11900K .......... .......... .......... .......... .......... 20%  227M 0s\n",
            " 11950K .......... .......... .......... .......... .......... 21%  194M 0s\n",
            " 12000K .......... .......... .......... .......... .......... 21%  242M 0s\n",
            " 12050K .......... .......... .......... .......... .......... 21%  264M 0s\n",
            " 12100K .......... .......... .......... .......... .......... 21%  258M 0s\n",
            " 12150K .......... .......... .......... .......... .......... 21%  237M 0s\n",
            " 12200K .......... .......... .......... .......... .......... 21%  257M 0s\n",
            " 12250K .......... .......... .......... .......... .......... 21%  220M 0s\n",
            " 12300K .......... .......... .......... .......... .......... 21%  248M 0s\n",
            " 12350K .......... .......... .......... .......... .......... 21%  206M 0s\n",
            " 12400K .......... .......... .......... .......... .......... 21%  258M 0s\n",
            " 12450K .......... .......... .......... .......... .......... 21%  244M 0s\n",
            " 12500K .......... .......... .......... .......... .......... 21%  238M 0s\n",
            " 12550K .......... .......... .......... .......... .......... 22%  219M 0s\n",
            " 12600K .......... .......... .......... .......... .......... 22%  246M 0s\n",
            " 12650K .......... .......... .......... .......... .......... 22%  233M 0s\n",
            " 12700K .......... .......... .......... .......... .......... 22%  208M 0s\n",
            " 12750K .......... .......... .......... .......... .......... 22%  208M 0s\n",
            " 12800K .......... .......... .......... .......... .......... 22%  265M 0s\n",
            " 12850K .......... .......... .......... .......... .......... 22%  248M 0s\n",
            " 12900K .......... .......... .......... .......... .......... 22%  223M 0s\n",
            " 12950K .......... .......... .......... .......... .......... 22%  192M 0s\n",
            " 13000K .......... .......... .......... .......... .......... 22%  236M 0s\n",
            " 13050K .......... .......... .......... .......... .......... 22%  236M 0s\n",
            " 13100K .......... .......... .......... .......... .......... 23%  254M 0s\n",
            " 13150K .......... .......... .......... .......... .......... 23%  210M 0s\n",
            " 13200K .......... .......... .......... .......... .......... 23%  243M 0s\n",
            " 13250K .......... .......... .......... .......... .......... 23%  256M 0s\n",
            " 13300K .......... .......... .......... .......... .......... 23%  239M 0s\n",
            " 13350K .......... .......... .......... .......... .......... 23%  221M 0s\n",
            " 13400K .......... .......... .......... .......... .......... 23%  233M 0s\n",
            " 13450K .......... .......... .......... .......... .......... 23%  246M 0s\n",
            " 13500K .......... .......... .......... .......... .......... 23%  240M 0s\n",
            " 13550K .......... .......... .......... .......... .......... 23%  209M 0s\n",
            " 13600K .......... .......... .......... .......... .......... 23%  238M 0s\n",
            " 13650K .......... .......... .......... .......... .......... 23%  226M 0s\n",
            " 13700K .......... .......... .......... .......... .......... 24%  209M 0s\n",
            " 13750K .......... .......... .......... .......... .......... 24%  201M 0s\n",
            " 13800K .......... .......... .......... .......... .......... 24%  233M 0s\n",
            " 13850K .......... .......... .......... .......... .......... 24%  252M 0s\n",
            " 13900K .......... .......... .......... .......... .......... 24%  247M 0s\n",
            " 13950K .......... .......... .......... .......... .......... 24%  214M 0s\n",
            " 14000K .......... .......... .......... .......... .......... 24%  216M 0s\n",
            " 14050K .......... .......... .......... .......... .......... 24%  203M 0s\n",
            " 14100K .......... .......... .......... .......... .......... 24%  196M 0s\n",
            " 14150K .......... .......... .......... .......... .......... 24%  234M 0s\n",
            " 14200K .......... .......... .......... .......... .......... 24%  270M 0s\n",
            " 14250K .......... .......... .......... .......... .......... 25%  263M 0s\n",
            " 14300K .......... .......... .......... .......... .......... 25%  251M 0s\n",
            " 14350K .......... .......... .......... .......... .......... 25%  212M 0s\n",
            " 14400K .......... .......... .......... .......... .......... 25%  271M 0s\n",
            " 14450K .......... .......... .......... .......... .......... 25%  236M 0s\n",
            " 14500K .......... .......... .......... .......... .......... 25%  218M 0s\n",
            " 14550K .......... .......... .......... .......... .......... 25%  214M 0s\n",
            " 14600K .......... .......... .......... .......... .......... 25%  246M 0s\n",
            " 14650K .......... .......... .......... .......... .......... 25%  266M 0s\n",
            " 14700K .......... .......... .......... .......... .......... 25%  266M 0s\n",
            " 14750K .......... .......... .......... .......... .......... 25%  206M 0s\n",
            " 14800K .......... .......... .......... .......... .......... 26%  227M 0s\n",
            " 14850K .......... .......... .......... .......... .......... 26%  212M 0s\n",
            " 14900K .......... .......... .......... .......... .......... 26%  237M 0s\n",
            " 14950K .......... .......... .......... .......... .......... 26%  233M 0s\n",
            " 15000K .......... .......... .......... .......... .......... 26%  257M 0s\n",
            " 15050K .......... .......... .......... .......... .......... 26%  256M 0s\n",
            " 15100K .......... .......... .......... .......... .......... 26%  245M 0s\n",
            " 15150K .......... .......... .......... .......... .......... 26%  190M 0s\n",
            " 15200K .......... .......... .......... .......... .......... 26%  214M 0s\n",
            " 15250K .......... .......... .......... .......... .......... 26%  218M 0s\n",
            " 15300K .......... .......... .......... .......... .......... 26%  220M 0s\n",
            " 15350K .......... .......... .......... .......... .......... 26%  220M 0s\n",
            " 15400K .......... .......... .......... .......... .......... 27%  254M 0s\n",
            " 15450K .......... .......... .......... .......... .......... 27%  217M 0s\n",
            " 15500K .......... .......... .......... .......... .......... 27%  221M 0s\n",
            " 15550K .......... .......... .......... .......... .......... 27%  183M 0s\n",
            " 15600K .......... .......... .......... .......... .......... 27%  245M 0s\n",
            " 15650K .......... .......... .......... .......... .......... 27%  237M 0s\n",
            " 15700K .......... .......... .......... .......... .......... 27%  251M 0s\n",
            " 15750K .......... .......... .......... .......... .......... 27%  191M 0s\n",
            " 15800K .......... .......... .......... .......... .......... 27%  253M 0s\n",
            " 15850K .......... .......... .......... .......... .......... 27%  256M 0s\n",
            " 15900K .......... .......... .......... .......... .......... 27%  260M 0s\n",
            " 15950K .......... .......... .......... .......... .......... 28%  214M 0s\n",
            " 16000K .......... .......... .......... .......... .......... 28%  243M 0s\n",
            " 16050K .......... .......... .......... .......... .......... 28%  247M 0s\n",
            " 16100K .......... .......... .......... .......... .......... 28%  208M 0s\n",
            " 16150K .......... .......... .......... .......... .......... 28%  204M 0s\n",
            " 16200K .......... .......... .......... .......... .......... 28%  181M 0s\n",
            " 16250K .......... .......... .......... .......... .......... 28%  185M 0s\n",
            " 16300K .......... .......... .......... .......... .......... 28%  229M 0s\n",
            " 16350K .......... .......... .......... .......... .......... 28%  196M 0s\n",
            " 16400K .......... .......... .......... .......... .......... 28%  217M 0s\n",
            " 16450K .......... .......... .......... .......... .......... 28%  221M 0s\n",
            " 16500K .......... .......... .......... .......... .......... 28%  244M 0s\n",
            " 16550K .......... .......... .......... .......... .......... 29%  186M 0s\n",
            " 16600K .......... .......... .......... .......... .......... 29%  228M 0s\n",
            " 16650K .......... .......... .......... .......... .......... 29%  232M 0s\n",
            " 16700K .......... .......... .......... .......... .......... 29%  251M 0s\n",
            " 16750K .......... .......... .......... .......... .......... 29%  212M 0s\n",
            " 16800K .......... .......... .......... .......... .......... 29%  251M 0s\n",
            " 16850K .......... .......... .......... .......... .......... 29%  252M 0s\n",
            " 16900K .......... .......... .......... .......... .......... 29%  224M 0s\n",
            " 16950K .......... .......... .......... .......... .......... 29%  188M 0s\n",
            " 17000K .......... .......... .......... .......... .......... 29%  214M 0s\n",
            " 17050K .......... .......... .......... .......... .......... 29%  207M 0s\n",
            " 17100K .......... .......... .......... .......... .......... 30%  200M 0s\n",
            " 17150K .......... .......... .......... .......... .......... 30%  192M 0s\n",
            " 17200K .......... .......... .......... .......... .......... 30%  265M 0s\n",
            " 17250K .......... .......... .......... .......... .......... 30%  259M 0s\n",
            " 17300K .......... .......... .......... .......... .......... 30%  250M 0s\n",
            " 17350K .......... .......... .......... .......... .......... 30%  217M 0s\n",
            " 17400K .......... .......... .......... .......... .......... 30%  248M 0s\n",
            " 17450K .......... .......... .......... .......... .......... 30%  264M 0s\n",
            " 17500K .......... .......... .......... .......... .......... 30%  261M 0s\n",
            " 17550K .......... .......... .......... .......... .......... 30%  189M 0s\n",
            " 17600K .......... .......... .......... .......... .......... 30%  218M 0s\n",
            " 17650K .......... .......... .......... .......... .......... 30%  234M 0s\n",
            " 17700K .......... .......... .......... .......... .......... 31%  213M 0s\n",
            " 17750K .......... .......... .......... .......... .......... 31%  203M 0s\n",
            " 17800K .......... .......... .......... .......... .......... 31%  240M 0s\n",
            " 17850K .......... .......... .......... .......... .......... 31%  254M 0s\n",
            " 17900K .......... .......... .......... .......... .......... 31%  256M 0s\n",
            " 17950K .......... .......... .......... .......... .......... 31%  170M 0s\n",
            " 18000K .......... .......... .......... .......... .......... 31%  233M 0s\n",
            " 18050K .......... .......... .......... .......... .......... 31%  229M 0s\n",
            " 18100K .......... .......... .......... .......... .......... 31%  227M 0s\n",
            " 18150K .......... .......... .......... .......... .......... 31%  210M 0s\n",
            " 18200K .......... .......... .......... .......... .......... 31%  207M 0s\n",
            " 18250K .......... .......... .......... .......... .......... 32%  247M 0s\n",
            " 18300K .......... .......... .......... .......... .......... 32%  217M 0s\n",
            " 18350K .......... .......... .......... .......... .......... 32%  223M 0s\n",
            " 18400K .......... .......... .......... .......... .......... 32%  264M 0s\n",
            " 18450K .......... .......... .......... .......... .......... 32%  242M 0s\n",
            " 18500K .......... .......... .......... .......... .......... 32%  233M 0s\n",
            " 18550K .......... .......... .......... .......... .......... 32%  196M 0s\n",
            " 18600K .......... .......... .......... .......... .......... 32%  208M 0s\n",
            " 18650K .......... .......... .......... .......... .......... 32%  205M 0s\n",
            " 18700K .......... .......... .......... .......... .......... 32%  216M 0s\n",
            " 18750K .......... .......... .......... .......... .......... 32%  177M 0s\n",
            " 18800K .......... .......... .......... .......... .......... 33%  241M 0s\n",
            " 18850K .......... .......... .......... .......... .......... 33%  258M 0s\n",
            " 18900K .......... .......... .......... .......... .......... 33%  242M 0s\n",
            " 18950K .......... .......... .......... .......... .......... 33%  215M 0s\n",
            " 19000K .......... .......... .......... .......... .......... 33%  268M 0s\n",
            " 19050K .......... .......... .......... .......... .......... 33%  255M 0s\n",
            " 19100K .......... .......... .......... .......... .......... 33%  264M 0s\n",
            " 19150K .......... .......... .......... .......... .......... 33%  219M 0s\n",
            " 19200K .......... .......... .......... .......... .......... 33%  251M 0s\n",
            " 19250K .......... .......... .......... .......... .......... 33%  353M 0s\n",
            " 19300K .......... .......... .......... .......... .......... 33%  380M 0s\n",
            " 19350K .......... .......... .......... .......... .......... 33%  386M 0s\n",
            " 19400K .......... .......... .......... .......... .......... 34%  429M 0s\n",
            " 19450K .......... .......... .......... .......... .......... 34%  435M 0s\n",
            " 19500K .......... .......... .......... .......... .......... 34%  397M 0s\n",
            " 19550K .......... .......... .......... .......... .......... 34%  288M 0s\n",
            " 19600K .......... .......... .......... .......... .......... 34%  159M 0s\n",
            " 19650K .......... .......... .......... .......... .......... 34%  228M 0s\n",
            " 19700K .......... .......... .......... .......... .......... 34%  245M 0s\n",
            " 19750K .......... .......... .......... .......... .......... 34%  215M 0s\n",
            " 19800K .......... .......... .......... .......... .......... 34%  247M 0s\n",
            " 19850K .......... .......... .......... .......... .......... 34%  251M 0s\n",
            " 19900K .......... .......... .......... .......... .......... 34%  250M 0s\n",
            " 19950K .......... .......... .......... .......... .......... 35%  217M 0s\n",
            " 20000K .......... .......... .......... .......... .......... 35%  214M 0s\n",
            " 20050K .......... .......... .......... .......... .......... 35%  169M 0s\n",
            " 20100K .......... .......... .......... .......... .......... 35%  183M 0s\n",
            " 20150K .......... .......... .......... .......... .......... 35%  177M 0s\n",
            " 20200K .......... .......... .......... .......... .......... 35%  205M 0s\n",
            " 20250K .......... .......... .......... .......... .......... 35%  235M 0s\n",
            " 20300K .......... .......... .......... .......... .......... 35%  220M 0s\n",
            " 20350K .......... .......... .......... .......... .......... 35%  218M 0s\n",
            " 20400K .......... .......... .......... .......... .......... 35%  246M 0s\n",
            " 20450K .......... .......... .......... .......... .......... 35%  229M 0s\n",
            " 20500K .......... .......... .......... .......... .......... 35%  261M 0s\n",
            " 20550K .......... .......... .......... .......... .......... 36%  215M 0s\n",
            " 20600K .......... .......... .......... .......... .......... 36%  242M 0s\n",
            " 20650K .......... .......... .......... .......... .......... 36%  233M 0s\n",
            " 20700K .......... .......... .......... .......... .......... 36% 20.1M 0s\n",
            " 20750K .......... .......... .......... .......... .......... 36%  167M 0s\n",
            " 20800K .......... .......... .......... .......... .......... 36%  205M 0s\n",
            " 20850K .......... .......... .......... .......... .......... 36%  202M 0s\n",
            " 20900K .......... .......... .......... .......... .......... 36%  234M 0s\n",
            " 20950K .......... .......... .......... .......... .......... 36%  221M 0s\n",
            " 21000K .......... .......... .......... .......... .......... 36%  255M 0s\n",
            " 21050K .......... .......... .......... .......... .......... 36%  246M 0s\n",
            " 21100K .......... .......... .......... .......... .......... 37%  153M 0s\n",
            " 21150K .......... .......... .......... .......... .......... 37%  190M 0s\n",
            " 21200K .......... .......... .......... .......... .......... 37%  234M 0s\n",
            " 21250K .......... .......... .......... .......... .......... 37%  243M 0s\n",
            " 21300K .......... .......... .......... .......... .......... 37%  117M 0s\n",
            " 21350K .......... .......... .......... .......... .......... 37%  120M 0s\n",
            " 21400K .......... .......... .......... .......... .......... 37% 89.3M 0s\n",
            " 21450K .......... .......... .......... .......... .......... 37% 50.5M 0s\n",
            " 21500K .......... .......... .......... .......... .......... 37% 22.9M 0s\n",
            " 21550K .......... .......... .......... .......... .......... 37%  187M 0s\n",
            " 21600K .......... .......... .......... .......... .......... 37%  218M 0s\n",
            " 21650K .......... .......... .......... .......... .......... 38%  226M 0s\n",
            " 21700K .......... .......... .......... .......... .......... 38%  252M 0s\n",
            " 21750K .......... .......... .......... .......... .......... 38%  218M 0s\n",
            " 21800K .......... .......... .......... .......... .......... 38%  244M 0s\n",
            " 21850K .......... .......... .......... .......... .......... 38%  144M 0s\n",
            " 21900K .......... .......... .......... .......... .......... 38%  177M 0s\n",
            " 21950K .......... .......... .......... .......... .......... 38%  118M 0s\n",
            " 22000K .......... .......... .......... .......... .......... 38%  164M 0s\n",
            " 22050K .......... .......... .......... .......... .......... 38% 92.0M 0s\n",
            " 22100K .......... .......... .......... .......... .......... 38%  196M 0s\n",
            " 22150K .......... .......... .......... .......... .......... 38%  115M 0s\n",
            " 22200K .......... .......... .......... .......... .......... 38%  104M 0s\n",
            " 22250K .......... .......... .......... .......... .......... 39%  133M 0s\n",
            " 22300K .......... .......... .......... .......... .......... 39%  115M 0s\n",
            " 22350K .......... .......... .......... .......... .......... 39% 77.7M 0s\n",
            " 22400K .......... .......... .......... .......... .......... 39%  128M 0s\n",
            " 22450K .......... .......... .......... .......... .......... 39%  105M 0s\n",
            " 22500K .......... .......... .......... .......... .......... 39%  126M 0s\n",
            " 22550K .......... .......... .......... .......... .......... 39%  174M 0s\n",
            " 22600K .......... .......... .......... .......... .......... 39%  104M 0s\n",
            " 22650K .......... .......... .......... .......... .......... 39%  126M 0s\n",
            " 22700K .......... .......... .......... .......... .......... 39%  147M 0s\n",
            " 22750K .......... .......... .......... .......... .......... 39%  110M 0s\n",
            " 22800K .......... .......... .......... .......... .......... 40%  123M 0s\n",
            " 22850K .......... .......... .......... .......... .......... 40% 56.9M 0s\n",
            " 22900K .......... .......... .......... .......... .......... 40%  224M 0s\n",
            " 22950K .......... .......... .......... .......... .......... 40% 10.8M 0s\n",
            " 23000K .......... .......... .......... .......... .......... 40%  213M 0s\n",
            " 23050K .......... .......... .......... .......... .......... 40%  218M 0s\n",
            " 23100K .......... .......... .......... .......... .......... 40%  203M 0s\n",
            " 23150K .......... .......... .......... .......... .......... 40%  186M 0s\n",
            " 23200K .......... .......... .......... .......... .......... 40%  235M 0s\n",
            " 23250K .......... .......... .......... .......... .......... 40%  257M 0s\n",
            " 23300K .......... .......... .......... .......... .......... 40%  248M 0s\n",
            " 23350K .......... .......... .......... .......... .......... 40%  204M 0s\n",
            " 23400K .......... .......... .......... .......... .......... 41% 43.0M 0s\n",
            " 23450K .......... .......... .......... .......... .......... 41%  222M 0s\n",
            " 23500K .......... .......... .......... .......... .......... 41%  223M 0s\n",
            " 23550K .......... .......... .......... .......... .......... 41%  190M 0s\n",
            " 23600K .......... .......... .......... .......... .......... 41%  199M 0s\n",
            " 23650K .......... .......... .......... .......... .......... 41%  226M 0s\n",
            " 23700K .......... .......... .......... .......... .......... 41%  223M 0s\n",
            " 23750K .......... .......... .......... .......... .......... 41%  169M 0s\n",
            " 23800K .......... .......... .......... .......... .......... 41%  225M 0s\n",
            " 23850K .......... .......... .......... .......... .......... 41% 13.6M 0s\n",
            " 23900K .......... .......... .......... .......... .......... 41%  206M 0s\n",
            " 23950K .......... .......... .......... .......... .......... 42%  152M 0s\n",
            " 24000K .......... .......... .......... .......... .......... 42%  252M 0s\n",
            " 24050K .......... .......... .......... .......... .......... 42%  259M 0s\n",
            " 24100K .......... .......... .......... .......... .......... 42%  244M 0s\n",
            " 24150K .......... .......... .......... .......... .......... 42%  219M 0s\n",
            " 24200K .......... .......... .......... .......... .......... 42% 37.5M 0s\n",
            " 24250K .......... .......... .......... .......... .......... 42%  225M 0s\n",
            " 24300K .......... .......... .......... .......... .......... 42%  243M 0s\n",
            " 24350K .......... .......... .......... .......... .......... 42%  183M 0s\n",
            " 24400K .......... .......... .......... .......... .......... 42%  218M 0s\n",
            " 24450K .......... .......... .......... .......... .......... 42%  180M 0s\n",
            " 24500K .......... .......... .......... .......... .......... 42%  225M 0s\n",
            " 24550K .......... .......... .......... .......... .......... 43%  158M 0s\n",
            " 24600K .......... .......... .......... .......... .......... 43%  203M 0s\n",
            " 24650K .......... .......... .......... .......... .......... 43%  222M 0s\n",
            " 24700K .......... .......... .......... .......... .......... 43%  209M 0s\n",
            " 24750K .......... .......... .......... .......... .......... 43%  191M 0s\n",
            " 24800K .......... .......... .......... .......... .......... 43%  246M 0s\n",
            " 24850K .......... .......... .......... .......... .......... 43%  239M 0s\n",
            " 24900K .......... .......... .......... .......... .......... 43%  230M 0s\n",
            " 24950K .......... .......... .......... .......... .......... 43%  204M 0s\n",
            " 25000K .......... .......... .......... .......... .......... 43% 8.33M 0s\n",
            " 25050K .......... .......... .......... .......... .......... 43%  200M 0s\n",
            " 25100K .......... .......... .......... .......... .......... 44%  178M 0s\n",
            " 25150K .......... .......... .......... .......... .......... 44%  215M 0s\n",
            " 25200K .......... .......... .......... .......... .......... 44%  215M 0s\n",
            " 25250K .......... .......... .......... .......... .......... 44%  214M 0s\n",
            " 25300K .......... .......... .......... .......... .......... 44%  219M 0s\n",
            " 25350K .......... .......... .......... .......... .......... 44%  175M 0s\n",
            " 25400K .......... .......... .......... .......... .......... 44%  281M 0s\n",
            " 25450K .......... .......... .......... .......... .......... 44%  266M 0s\n",
            " 25500K .......... .......... .......... .......... .......... 44%  241M 0s\n",
            " 25550K .......... .......... .......... .......... .......... 44%  180M 0s\n",
            " 25600K .......... .......... .......... .......... .......... 44%  222M 0s\n",
            " 25650K .......... .......... .......... .......... .......... 45%  211M 0s\n",
            " 25700K .......... .......... .......... .......... .......... 45%  227M 0s\n",
            " 25750K .......... .......... .......... .......... .......... 45%  170M 0s\n",
            " 25800K .......... .......... .......... .......... .......... 45%  238M 0s\n",
            " 25850K .......... .......... .......... .......... .......... 45%  227M 0s\n",
            " 25900K .......... .......... .......... .......... .......... 45%  173M 0s\n",
            " 25950K .......... .......... .......... .......... .......... 45%  175M 0s\n",
            " 26000K .......... .......... .......... .......... .......... 45%  225M 0s\n",
            " 26050K .......... .......... .......... .......... .......... 45%  208M 0s\n",
            " 26100K .......... .......... .......... .......... .......... 45% 25.3M 0s\n",
            " 26150K .......... .......... .......... .......... .......... 45%  174M 0s\n",
            " 26200K .......... .......... .......... .......... .......... 45%  192M 0s\n",
            " 26250K .......... .......... .......... .......... .......... 46%  229M 0s\n",
            " 26300K .......... .......... .......... .......... .......... 46%  238M 0s\n",
            " 26350K .......... .......... .......... .......... .......... 46%  206M 0s\n",
            " 26400K .......... .......... .......... .......... .......... 46%  200M 0s\n",
            " 26450K .......... .......... .......... .......... .......... 46%  206M 0s\n",
            " 26500K .......... .......... .......... .......... .......... 46%  221M 0s\n",
            " 26550K .......... .......... .......... .......... .......... 46%  218M 0s\n",
            " 26600K .......... .......... .......... .......... .......... 46%  228M 0s\n",
            " 26650K .......... .......... .......... .......... .......... 46%  219M 0s\n",
            " 26700K .......... .......... .......... .......... .......... 46% 21.4M 0s\n",
            " 26750K .......... .......... .......... .......... .......... 46%  208M 0s\n",
            " 26800K .......... .......... .......... .......... .......... 47%  216M 0s\n",
            " 26850K .......... .......... .......... .......... .......... 47%  225M 0s\n",
            " 26900K .......... .......... .......... .......... .......... 47%  382M 0s\n",
            " 26950K .......... .......... .......... .......... .......... 47% 22.0M 0s\n",
            " 27000K .......... .......... .......... .......... .......... 47%  201M 0s\n",
            " 27050K .......... .......... .......... .......... .......... 47%  214M 0s\n",
            " 27100K .......... .......... .......... .......... .......... 47%  225M 0s\n",
            " 27150K .......... .......... .......... .......... .......... 47%  209M 0s\n",
            " 27200K .......... .......... .......... .......... .......... 47% 21.8M 0s\n",
            " 27250K .......... .......... .......... .......... .......... 47%  207M 0s\n",
            " 27300K .......... .......... .......... .......... .......... 47%  196M 0s\n",
            " 27350K .......... .......... .......... .......... .......... 47%  166M 0s\n",
            " 27400K .......... .......... .......... .......... .......... 48%  205M 0s\n",
            " 27450K .......... .......... .......... .......... .......... 48%  196M 0s\n",
            " 27500K .......... .......... .......... .......... .......... 48%  201M 0s\n",
            " 27550K .......... .......... .......... .......... .......... 48%  187M 0s\n",
            " 27600K .......... .......... .......... .......... .......... 48%  227M 0s\n",
            " 27650K .......... .......... .......... .......... .......... 48%  208M 0s\n",
            " 27700K .......... .......... .......... .......... .......... 48%  209M 0s\n",
            " 27750K .......... .......... .......... .......... .......... 48%  213M 0s\n",
            " 27800K .......... .......... .......... .......... .......... 48%  245M 0s\n",
            " 27850K .......... .......... .......... .......... .......... 48% 24.4M 0s\n",
            " 27900K .......... .......... .......... .......... .......... 48%  204M 0s\n",
            " 27950K .......... .......... .......... .......... .......... 49%  188M 0s\n",
            " 28000K .......... .......... .......... .......... .......... 49%  224M 0s\n",
            " 28050K .......... .......... .......... .......... .......... 49%  247M 0s\n",
            " 28100K .......... .......... .......... .......... .......... 49%  243M 0s\n",
            " 28150K .......... .......... .......... .......... .......... 49%  189M 0s\n",
            " 28200K .......... .......... .......... .......... .......... 49%  228M 0s\n",
            " 28250K .......... .......... .......... .......... .......... 49%  254M 0s\n",
            " 28300K .......... .......... .......... .......... .......... 49%  225M 0s\n",
            " 28350K .......... .......... .......... .......... .......... 49%  180M 0s\n",
            " 28400K .......... .......... .......... .......... .......... 49%  242M 0s\n",
            " 28450K .......... .......... .......... .......... .......... 49% 50.6M 0s\n",
            " 28500K .......... .......... .......... .......... .......... 50%  182M 0s\n",
            " 28550K .......... .......... .......... .......... .......... 50%  183M 0s\n",
            " 28600K .......... .......... .......... .......... .......... 50%  238M 0s\n",
            " 28650K .......... .......... .......... .......... .......... 50%  175M 0s\n",
            " 28700K .......... .......... .......... .......... .......... 50%  224M 0s\n",
            " 28750K .......... .......... .......... .......... .......... 50%  209M 0s\n",
            " 28800K .......... .......... .......... .......... .......... 50% 16.7M 0s\n",
            " 28850K .......... .......... .......... .......... .......... 50%  226M 0s\n",
            " 28900K .......... .......... .......... .......... .......... 50%  213M 0s\n",
            " 28950K .......... .......... .......... .......... .......... 50%  163M 0s\n",
            " 29000K .......... .......... .......... .......... .......... 50%  225M 0s\n",
            " 29050K .......... .......... .......... .......... .......... 50%  225M 0s\n",
            " 29100K .......... .......... .......... .......... .......... 51%  231M 0s\n",
            " 29150K .......... .......... .......... .......... .......... 51%  204M 0s\n",
            " 29200K .......... .......... .......... .......... .......... 51%  256M 0s\n",
            " 29250K .......... .......... .......... .......... .......... 51%  268M 0s\n",
            " 29300K .......... .......... .......... .......... .......... 51%  268M 0s\n",
            " 29350K .......... .......... .......... .......... .......... 51%  224M 0s\n",
            " 29400K .......... .......... .......... .......... .......... 51% 83.8M 0s\n",
            " 29450K .......... .......... .......... .......... .......... 51%  140M 0s\n",
            " 29500K .......... .......... .......... .......... .......... 51%  158M 0s\n",
            " 29550K .......... .......... .......... .......... .......... 51%  147M 0s\n",
            " 29600K .......... .......... .......... .......... .......... 51%  244M 0s\n",
            " 29650K .......... .......... .......... .......... .......... 52%  258M 0s\n",
            " 29700K .......... .......... .......... .......... .......... 52%  222M 0s\n",
            " 29750K .......... .......... .......... .......... .......... 52%  161M 0s\n",
            " 29800K .......... .......... .......... .......... .......... 52%  257M 0s\n",
            " 29850K .......... .......... .......... .......... .......... 52%  261M 0s\n",
            " 29900K .......... .......... .......... .......... .......... 52% 8.04M 0s\n",
            " 29950K .......... .......... .......... .......... .......... 52%  180M 0s\n",
            " 30000K .......... .......... .......... .......... .......... 52% 78.3M 0s\n",
            " 30050K .......... .......... .......... .......... .......... 52%  173M 0s\n",
            " 30100K .......... .......... .......... .......... .......... 52%  183M 0s\n",
            " 30150K .......... .......... .......... .......... .......... 52%  168M 0s\n",
            " 30200K .......... .......... .......... .......... .......... 52%  227M 0s\n",
            " 30250K .......... .......... .......... .......... .......... 53%  190M 0s\n",
            " 30300K .......... .......... .......... .......... .......... 53%  171M 0s\n",
            " 30350K .......... .......... .......... .......... .......... 53%  223M 0s\n",
            " 30400K .......... .......... .......... .......... .......... 53%  287M 0s\n",
            " 30450K .......... .......... .......... .......... .......... 53%  297M 0s\n",
            " 30500K .......... .......... .......... .......... .......... 53%  187M 0s\n",
            " 30550K .......... .......... .......... .......... .......... 53%  156M 0s\n",
            " 30600K .......... .......... .......... .......... .......... 53%  159M 0s\n",
            " 30650K .......... .......... .......... .......... .......... 53%  194M 0s\n",
            " 30700K .......... .......... .......... .......... .......... 53%  243M 0s\n",
            " 30750K .......... .......... .......... .......... .......... 53%  165M 0s\n",
            " 30800K .......... .......... .......... .......... .......... 54%  198M 0s\n",
            " 30850K .......... .......... .......... .......... .......... 54%  216M 0s\n",
            " 30900K .......... .......... .......... .......... .......... 54%  255M 0s\n",
            " 30950K .......... .......... .......... .......... .......... 54%  151M 0s\n",
            " 31000K .......... .......... .......... .......... .......... 54%  256M 0s\n",
            " 31050K .......... .......... .......... .......... .......... 54%  258M 0s\n",
            " 31100K .......... .......... .......... .......... .......... 54%  262M 0s\n",
            " 31150K .......... .......... .......... .......... .......... 54%  204M 0s\n",
            " 31200K .......... .......... .......... .......... .......... 54%  245M 0s\n",
            " 31250K .......... .......... .......... .......... .......... 54%  242M 0s\n",
            " 31300K .......... .......... .......... .......... .......... 54%  254M 0s\n",
            " 31350K .......... .......... .......... .......... .......... 54%  205M 0s\n",
            " 31400K .......... .......... .......... .......... .......... 55%  244M 0s\n",
            " 31450K .......... .......... .......... .......... .......... 55% 25.7M 0s\n",
            " 31500K .......... .......... .......... .......... .......... 55%  190M 0s\n",
            " 31550K .......... .......... .......... .......... .......... 55%  148M 0s\n",
            " 31600K .......... .......... .......... .......... .......... 55%  185M 0s\n",
            " 31650K .......... .......... .......... .......... .......... 55%  191M 0s\n",
            " 31700K .......... .......... .......... .......... .......... 55%  245M 0s\n",
            " 31750K .......... .......... .......... .......... .......... 55%  230M 0s\n",
            " 31800K .......... .......... .......... .......... .......... 55%  231M 0s\n",
            " 31850K .......... .......... .......... .......... .......... 55%  230M 0s\n",
            " 31900K .......... .......... .......... .......... .......... 55%  243M 0s\n",
            " 31950K .......... .......... .......... .......... .......... 56%  200M 0s\n",
            " 32000K .......... .......... .......... .......... .......... 56%  161M 0s\n",
            " 32050K .......... .......... .......... .......... .......... 56%  190M 0s\n",
            " 32100K .......... .......... .......... .......... .......... 56%  153M 0s\n",
            " 32150K .......... .......... .......... .......... .......... 56%  171M 0s\n",
            " 32200K .......... .......... .......... .......... .......... 56%  223M 0s\n",
            " 32250K .......... .......... .......... .......... .......... 56%  259M 0s\n",
            " 32300K .......... .......... .......... .......... .......... 56%  244M 0s\n",
            " 32350K .......... .......... .......... .......... .......... 56%  211M 0s\n",
            " 32400K .......... .......... .......... .......... .......... 56%  246M 0s\n",
            " 32450K .......... .......... .......... .......... .......... 56% 13.9M 0s\n",
            " 32500K .......... .......... .......... .......... .......... 57%  154M 0s\n",
            " 32550K .......... .......... .......... .......... .......... 57%  155M 0s\n",
            " 32600K .......... .......... .......... .......... .......... 57%  179M 0s\n",
            " 32650K .......... .......... .......... .......... .......... 57%  183M 0s\n",
            " 32700K .......... .......... .......... .......... .......... 57%  182M 0s\n",
            " 32750K .......... .......... .......... .......... .......... 57%  162M 0s\n",
            " 32800K .......... .......... .......... .......... .......... 57%  232M 0s\n",
            " 32850K .......... .......... .......... .......... .......... 57%  236M 0s\n",
            " 32900K .......... .......... .......... .......... .......... 57%  196M 0s\n",
            " 32950K .......... .......... .......... .......... .......... 57%  155M 0s\n",
            " 33000K .......... .......... .......... .......... .......... 57%  242M 0s\n",
            " 33050K .......... .......... .......... .......... .......... 57%  185M 0s\n",
            " 33100K .......... .......... .......... .......... .......... 58%  207M 0s\n",
            " 33150K .......... .......... .......... .......... .......... 58%  221M 0s\n",
            " 33200K .......... .......... .......... .......... .......... 58%  209M 0s\n",
            " 33250K .......... .......... .......... .......... .......... 58%  238M 0s\n",
            " 33300K .......... .......... .......... .......... .......... 58%  242M 0s\n",
            " 33350K .......... .......... .......... .......... .......... 58%  205M 0s\n",
            " 33400K .......... .......... .......... .......... .......... 58%  252M 0s\n",
            " 33450K .......... .......... .......... .......... .......... 58%  223M 0s\n",
            " 33500K .......... .......... .......... .......... .......... 58% 49.2M 0s\n",
            " 33550K .......... .......... .......... .......... .......... 58%  142M 0s\n",
            " 33600K .......... .......... .......... .......... .......... 58%  183M 0s\n",
            " 33650K .......... .......... .......... .......... .......... 59%  208M 0s\n",
            " 33700K .......... .......... .......... .......... .......... 59%  219M 0s\n",
            " 33750K .......... .......... .......... .......... .......... 59%  148M 0s\n",
            " 33800K .......... .......... .......... .......... .......... 59%  208M 0s\n",
            " 33850K .......... .......... .......... .......... .......... 59%  248M 0s\n",
            " 33900K .......... .......... .......... .......... .......... 59%  225M 0s\n",
            " 33950K .......... .......... .......... .......... .......... 59%  199M 0s\n",
            " 34000K .......... .......... .......... .......... .......... 59%  302M 0s\n",
            " 34050K .......... .......... .......... .......... .......... 59%  131M 0s\n",
            " 34100K .......... .......... .......... .......... .......... 59%  188M 0s\n",
            " 34150K .......... .......... .......... .......... .......... 59%  203M 0s\n",
            " 34200K .......... .......... .......... .......... .......... 59% 39.9M 0s\n",
            " 34250K .......... .......... .......... .......... .......... 60%  168M 0s\n",
            " 34300K .......... .......... .......... .......... .......... 60%  194M 0s\n",
            " 34350K .......... .......... .......... .......... .......... 60%  226M 0s\n",
            " 34400K .......... .......... .......... .......... .......... 60%  263M 0s\n",
            " 34450K .......... .......... .......... .......... .......... 60%  229M 0s\n",
            " 34500K .......... .......... .......... .......... .......... 60%  271M 0s\n",
            " 34550K .......... .......... .......... .......... .......... 60% 93.8M 0s\n",
            " 34600K .......... .......... .......... .......... .......... 60%  171M 0s\n",
            " 34650K .......... .......... .......... .......... .......... 60%  236M 0s\n",
            " 34700K .......... .......... .......... .......... .......... 60%  276M 0s\n",
            " 34750K .......... .......... .......... .......... .......... 60%  232M 0s\n",
            " 34800K .......... .......... .......... .......... .......... 61% 23.6M 0s\n",
            " 34850K .......... .......... .......... .......... .......... 61%  169M 0s\n",
            " 34900K .......... .......... .......... .......... .......... 61%  184M 0s\n",
            " 34950K .......... .......... .......... .......... .......... 61%  127M 0s\n",
            " 35000K .......... .......... .......... .......... .......... 61%  239M 0s\n",
            " 35050K .......... .......... .......... .......... .......... 61%  169M 0s\n",
            " 35100K .......... .......... .......... .......... .......... 61%  204M 0s\n",
            " 35150K .......... .......... .......... .......... .......... 61%  210M 0s\n",
            " 35200K .......... .......... .......... .......... .......... 61%  233M 0s\n",
            " 35250K .......... .......... .......... .......... .......... 61%  228M 0s\n",
            " 35300K .......... .......... .......... .......... .......... 61%  225M 0s\n",
            " 35350K .......... .......... .......... .......... .......... 61% 33.5M 0s\n",
            " 35400K .......... .......... .......... .......... .......... 62%  162M 0s\n",
            " 35450K .......... .......... .......... .......... .......... 62%  163M 0s\n",
            " 35500K .......... .......... .......... .......... .......... 62%  216M 0s\n",
            " 35550K .......... .......... .......... .......... .......... 62%  224M 0s\n",
            " 35600K .......... .......... .......... .......... .......... 62%  272M 0s\n",
            " 35650K .......... .......... .......... .......... .......... 62%  176M 0s\n",
            " 35700K .......... .......... .......... .......... .......... 62%  215M 0s\n",
            " 35750K .......... .......... .......... .......... .......... 62% 64.4M 0s\n",
            " 35800K .......... .......... .......... .......... .......... 62%  153M 0s\n",
            " 35850K .......... .......... .......... .......... .......... 62%  173M 0s\n",
            " 35900K .......... .......... .......... .......... .......... 62%  244M 0s\n",
            " 35950K .......... .......... .......... .......... .......... 63%  185M 0s\n",
            " 36000K .......... .......... .......... .......... .......... 63%  237M 0s\n",
            " 36050K .......... .......... .......... .......... .......... 63%  186M 0s\n",
            " 36100K .......... .......... .......... .......... .......... 63% 15.3M 0s\n",
            " 36150K .......... .......... .......... .......... .......... 63%  144M 0s\n",
            " 36200K .......... .......... .......... .......... .......... 63%  172M 0s\n",
            " 36250K .......... .......... .......... .......... .......... 63%  188M 0s\n",
            " 36300K .......... .......... .......... .......... .......... 63%  223M 0s\n",
            " 36350K .......... .......... .......... .......... .......... 63%  209M 0s\n",
            " 36400K .......... .......... .......... .......... .......... 63%  163M 0s\n",
            " 36450K .......... .......... .......... .......... .......... 63%  155M 0s\n",
            " 36500K .......... .......... .......... .......... .......... 64%  228M 0s\n",
            " 36550K .......... .......... .......... .......... .......... 64%  220M 0s\n",
            " 36600K .......... .......... .......... .......... .......... 64%  255M 0s\n",
            " 36650K .......... .......... .......... .......... .......... 64%  231M 0s\n",
            " 36700K .......... .......... .......... .......... .......... 64%  219M 0s\n",
            " 36750K .......... .......... .......... .......... .......... 64%  212M 0s\n",
            " 36800K .......... .......... .......... .......... .......... 64%  252M 0s\n",
            " 36850K .......... .......... .......... .......... .......... 64%  206M 0s\n",
            " 36900K .......... .......... .......... .......... .......... 64%  180M 0s\n",
            " 36950K .......... .......... .......... .......... .......... 64%  161M 0s\n",
            " 37000K .......... .......... .......... .......... .......... 64%  250M 0s\n",
            " 37050K .......... .......... .......... .......... .......... 64%  260M 0s\n",
            " 37100K .......... .......... .......... .......... .......... 65%  264M 0s\n",
            " 37150K .......... .......... .......... .......... .......... 65%  233M 0s\n",
            " 37200K .......... .......... .......... .......... .......... 65% 34.0M 0s\n",
            " 37250K .......... .......... .......... .......... .......... 65%  176M 0s\n",
            " 37300K .......... .......... .......... .......... .......... 65%  173M 0s\n",
            " 37350K .......... .......... .......... .......... .......... 65%  171M 0s\n",
            " 37400K .......... .......... .......... .......... .......... 65%  241M 0s\n",
            " 37450K .......... .......... .......... .......... .......... 65%  250M 0s\n",
            " 37500K .......... .......... .......... .......... .......... 65%  256M 0s\n",
            " 37550K .......... .......... .......... .......... .......... 65%  225M 0s\n",
            " 37600K .......... .......... .......... .......... .......... 65% 17.1M 0s\n",
            " 37650K .......... .......... .......... .......... .......... 66%  151M 0s\n",
            " 37700K .......... .......... .......... .......... .......... 66%  164M 0s\n",
            " 37750K .......... .......... .......... .......... .......... 66%  167M 0s\n",
            " 37800K .......... .......... .......... .......... .......... 66%  184M 0s\n",
            " 37850K .......... .......... .......... .......... .......... 66%  219M 0s\n",
            " 37900K .......... .......... .......... .......... .......... 66%  255M 0s\n",
            " 37950K .......... .......... .......... .......... .......... 66%  205M 0s\n",
            " 38000K .......... .......... .......... .......... .......... 66%  240M 0s\n",
            " 38050K .......... .......... .......... .......... .......... 66%  235M 0s\n",
            " 38100K .......... .......... .......... .......... .......... 66%  227M 0s\n",
            " 38150K .......... .......... .......... .......... .......... 66%  186M 0s\n",
            " 38200K .......... .......... .......... .......... .......... 66% 2.64M 0s\n",
            " 38250K .......... .......... .......... .......... .......... 67%  167M 0s\n",
            " 38300K .......... .......... .......... .......... .......... 67%  151M 0s\n",
            " 38350K .......... .......... .......... .......... .......... 67%  164M 0s\n",
            " 38400K .......... .......... .......... .......... .......... 67%  157M 0s\n",
            " 38450K .......... .......... .......... .......... .......... 67%  164M 0s\n",
            " 38500K .......... .......... .......... .......... .......... 67%  157M 0s\n",
            " 38550K .......... .......... .......... .......... .......... 67%  166M 0s\n",
            " 38600K .......... .......... .......... .......... .......... 67%  222M 0s\n",
            " 38650K .......... .......... .......... .......... .......... 67%  235M 0s\n",
            " 38700K .......... .......... .......... .......... .......... 67%  244M 0s\n",
            " 38750K .......... .......... .......... .......... .......... 67%  207M 0s\n",
            " 38800K .......... .......... .......... .......... .......... 68%  224M 0s\n",
            " 38850K .......... .......... .......... .......... .......... 68%  243M 0s\n",
            " 38900K .......... .......... .......... .......... .......... 68%  238M 0s\n",
            " 38950K .......... .......... .......... .......... .......... 68%  207M 0s\n",
            " 39000K .......... .......... .......... .......... .......... 68%  241M 0s\n",
            " 39050K .......... .......... .......... .......... .......... 68%  234M 0s\n",
            " 39100K .......... .......... .......... .......... .......... 68% 46.0M 0s\n",
            " 39150K .......... .......... .......... .......... .......... 68%  142M 0s\n",
            " 39200K .......... .......... .......... .......... .......... 68%  183M 0s\n",
            " 39250K .......... .......... .......... .......... .......... 68%  157M 0s\n",
            " 39300K .......... .......... .......... .......... .......... 68%  146M 0s\n",
            " 39350K .......... .......... .......... .......... .......... 69%  138M 0s\n",
            " 39400K .......... .......... .......... .......... .......... 69%  147M 0s\n",
            " 39450K .......... .......... .......... .......... .......... 69%  148M 0s\n",
            " 39500K .......... .......... .......... .......... .......... 69%  153M 0s\n",
            " 39550K .......... .......... .......... .......... .......... 69%  128M 0s\n",
            " 39600K .......... .......... .......... .......... .......... 69%  172M 0s\n",
            " 39650K .......... .......... .......... .......... .......... 69%  213M 0s\n",
            " 39700K .......... .......... .......... .......... .......... 69%  112M 0s\n",
            " 39750K .......... .......... .......... .......... .......... 69%  154M 0s\n",
            " 39800K .......... .......... .......... .......... .......... 69%  242M 0s\n",
            " 39850K .......... .......... .......... .......... .......... 69%  221M 0s\n",
            " 39900K .......... .......... .......... .......... .......... 69%  227M 0s\n",
            " 39950K .......... .......... .......... .......... .......... 70%  215M 0s\n",
            " 40000K .......... .......... .......... .......... .......... 70%  228M 0s\n",
            " 40050K .......... .......... .......... .......... .......... 70%  245M 0s\n",
            " 40100K .......... .......... .......... .......... .......... 70%  235M 0s\n",
            " 40150K .......... .......... .......... .......... .......... 70%  189M 0s\n",
            " 40200K .......... .......... .......... .......... .......... 70%  221M 0s\n",
            " 40250K .......... .......... .......... .......... .......... 70%  240M 0s\n",
            " 40300K .......... .......... .......... .......... .......... 70%  245M 0s\n",
            " 40350K .......... .......... .......... .......... .......... 70%  211M 0s\n",
            " 40400K .......... .......... .......... .......... .......... 70%  235M 0s\n",
            " 40450K .......... .......... .......... .......... .......... 70%  246M 0s\n",
            " 40500K .......... .......... .......... .......... .......... 71%  176M 0s\n",
            " 40550K .......... .......... .......... .......... .......... 71%  113M 0s\n",
            " 40600K .......... .......... .......... .......... .......... 71%  140M 0s\n",
            " 40650K .......... .......... .......... .......... .......... 71%  191M 0s\n",
            " 40700K .......... .......... .......... .......... .......... 71%  197M 0s\n",
            " 40750K .......... .......... .......... .......... .......... 71%  173M 0s\n",
            " 40800K .......... .......... .......... .......... .......... 71%  240M 0s\n",
            " 40850K .......... .......... .......... .......... .......... 71%  242M 0s\n",
            " 40900K .......... .......... .......... .......... .......... 71%  246M 0s\n",
            " 40950K .......... .......... .......... .......... .......... 71%  189M 0s\n",
            " 41000K .......... .......... .......... .......... .......... 71%  237M 0s\n",
            " 41050K .......... .......... .......... .......... .......... 71%  235M 0s\n",
            " 41100K .......... .......... .......... .......... .......... 72%  241M 0s\n",
            " 41150K .......... .......... .......... .......... .......... 72%  207M 0s\n",
            " 41200K .......... .......... .......... .......... .......... 72%  226M 0s\n",
            " 41250K .......... .......... .......... .......... .......... 72%  237M 0s\n",
            " 41300K .......... .......... .......... .......... .......... 72%  244M 0s\n",
            " 41350K .......... .......... .......... .......... .......... 72%  203M 0s\n",
            " 41400K .......... .......... .......... .......... .......... 72%  244M 0s\n",
            " 41450K .......... .......... .......... .......... .......... 72%  164M 0s\n",
            " 41500K .......... .......... .......... .......... .......... 72%  162M 0s\n",
            " 41550K .......... .......... .......... .......... .......... 72%  130M 0s\n",
            " 41600K .......... .......... .......... .......... .......... 72%  218M 0s\n",
            " 41650K .......... .......... .......... .......... .......... 73%  164M 0s\n",
            " 41700K .......... .......... .......... .......... .......... 73%  223M 0s\n",
            " 41750K .......... .......... .......... .......... .......... 73%  168M 0s\n",
            " 41800K .......... .......... .......... .......... .......... 73%  232M 0s\n",
            " 41850K .......... .......... .......... .......... .......... 73% 80.3M 0s\n",
            " 41900K .......... .......... .......... .......... .......... 73%  175M 0s\n",
            " 41950K .......... .......... .......... .......... .......... 73%  147M 0s\n",
            " 42000K .......... .......... .......... .......... .......... 73%  206M 0s\n",
            " 42050K .......... .......... .......... .......... .......... 73%  210M 0s\n",
            " 42100K .......... .......... .......... .......... .......... 73% 19.3M 0s\n",
            " 42150K .......... .......... .......... .......... .......... 73% 91.1M 0s\n",
            " 42200K .......... .......... .......... .......... .......... 73%  123M 0s\n",
            " 42250K .......... .......... .......... .......... .......... 74%  191M 0s\n",
            " 42300K .......... .......... .......... .......... .......... 74%  241M 0s\n",
            " 42350K .......... .......... .......... .......... .......... 74%  194M 0s\n",
            " 42400K .......... .......... .......... .......... .......... 74%  251M 0s\n",
            " 42450K .......... .......... .......... .......... .......... 74%  230M 0s\n",
            " 42500K .......... .......... .......... .......... .......... 74%  234M 0s\n",
            " 42550K .......... .......... .......... .......... .......... 74%  144M 0s\n",
            " 42600K .......... .......... .......... .......... .......... 74%  142M 0s\n",
            " 42650K .......... .......... .......... .......... .......... 74%  174M 0s\n",
            " 42700K .......... .......... .......... .......... .......... 74%  181M 0s\n",
            " 42750K .......... .......... .......... .......... .......... 74%  174M 0s\n",
            " 42800K .......... .......... .......... .......... .......... 75%  198M 0s\n",
            " 42850K .......... .......... .......... .......... .......... 75%  211M 0s\n",
            " 42900K .......... .......... .......... .......... .......... 75%  206M 0s\n",
            " 42950K .......... .......... .......... .......... .......... 75%  176M 0s\n",
            " 43000K .......... .......... .......... .......... .......... 75%  183M 0s\n",
            " 43050K .......... .......... .......... .......... .......... 75%  181M 0s\n",
            " 43100K .......... .......... .......... .......... .......... 75%  202M 0s\n",
            " 43150K .......... .......... .......... .......... .......... 75%  156M 0s\n",
            " 43200K .......... .......... .......... .......... .......... 75%  177M 0s\n",
            " 43250K .......... .......... .......... .......... .......... 75%  202M 0s\n",
            " 43300K .......... .......... .......... .......... .......... 75%  193M 0s\n",
            " 43350K .......... .......... .......... .......... .......... 76%  154M 0s\n",
            " 43400K .......... .......... .......... .......... .......... 76%  181M 0s\n",
            " 43450K .......... .......... .......... .......... .......... 76%  160M 0s\n",
            " 43500K .......... .......... .......... .......... .......... 76%  196M 0s\n",
            " 43550K .......... .......... .......... .......... .......... 76%  136M 0s\n",
            " 43600K .......... .......... .......... .......... .......... 76%  170M 0s\n",
            " 43650K .......... .......... .......... .......... .......... 76% 69.9M 0s\n",
            " 43700K .......... .......... .......... .......... .......... 76%  197M 0s\n",
            " 43750K .......... .......... .......... .......... .......... 76%  155M 0s\n",
            " 43800K .......... .......... .......... .......... .......... 76%  228M 0s\n",
            " 43850K .......... .......... .......... .......... .......... 76%  188M 0s\n",
            " 43900K .......... .......... .......... .......... .......... 76%  158M 0s\n",
            " 43950K .......... .......... .......... .......... .......... 77%  130M 0s\n",
            " 44000K .......... .......... .......... .......... .......... 77%  178M 0s\n",
            " 44050K .......... .......... .......... .......... .......... 77%  171M 0s\n",
            " 44100K .......... .......... .......... .......... .......... 77% 10.1M 0s\n",
            " 44150K .......... .......... .......... .......... .......... 77%  174M 0s\n",
            " 44200K .......... .......... .......... .......... .......... 77%  227M 0s\n",
            " 44250K .......... .......... .......... .......... .......... 77%  239M 0s\n",
            " 44300K .......... .......... .......... .......... .......... 77% 17.9M 0s\n",
            " 44350K .......... .......... .......... .......... .......... 77%  158M 0s\n",
            " 44400K .......... .......... .......... .......... .......... 77%  165M 0s\n",
            " 44450K .......... .......... .......... .......... .......... 77%  199M 0s\n",
            " 44500K .......... .......... .......... .......... .......... 78%  176M 0s\n",
            " 44550K .......... .......... .......... .......... .......... 78%  149M 0s\n",
            " 44600K .......... .......... .......... .......... .......... 78%  153M 0s\n",
            " 44650K .......... .......... .......... .......... .......... 78%  142M 0s\n",
            " 44700K .......... .......... .......... .......... .......... 78%  227M 0s\n",
            " 44750K .......... .......... .......... .......... .......... 78%  201M 0s\n",
            " 44800K .......... .......... .......... .......... .......... 78%  231M 0s\n",
            " 44850K .......... .......... .......... .......... .......... 78%  227M 0s\n",
            " 44900K .......... .......... .......... .......... .......... 78%  233M 0s\n",
            " 44950K .......... .......... .......... .......... .......... 78%  219M 0s\n",
            " 45000K .......... .......... .......... .......... .......... 78%  223M 0s\n",
            " 45050K .......... .......... .......... .......... .......... 78%  222M 0s\n",
            " 45100K .......... .......... .......... .......... .......... 79%  226M 0s\n",
            " 45150K .......... .......... .......... .......... .......... 79%  203M 0s\n",
            " 45200K .......... .......... .......... .......... .......... 79%  237M 0s\n",
            " 45250K .......... .......... .......... .......... .......... 79%  240M 0s\n",
            " 45300K .......... .......... .......... .......... .......... 79%  172M 0s\n",
            " 45350K .......... .......... .......... .......... .......... 79%  130M 0s\n",
            " 45400K .......... .......... .......... .......... .......... 79%  179M 0s\n",
            " 45450K .......... .......... .......... .......... .......... 79%  159M 0s\n",
            " 45500K .......... .......... .......... .......... .......... 79%  177M 0s\n",
            " 45550K .......... .......... .......... .......... .......... 79%  207M 0s\n",
            " 45600K .......... .......... .......... .......... .......... 79%  261M 0s\n",
            " 45650K .......... .......... .......... .......... .......... 80%  250M 0s\n",
            " 45700K .......... .......... .......... .......... .......... 80%  244M 0s\n",
            " 45750K .......... .......... .......... .......... .......... 80%  213M 0s\n",
            " 45800K .......... .......... .......... .......... .......... 80%  252M 0s\n",
            " 45850K .......... .......... .......... .......... .......... 80%  227M 0s\n",
            " 45900K .......... .......... .......... .......... .......... 80% 48.6M 0s\n",
            " 45950K .......... .......... .......... .......... .......... 80% 14.3M 0s\n",
            " 46000K .......... .......... .......... .......... .......... 80%  135M 0s\n",
            " 46050K .......... .......... .......... .......... .......... 80%  191M 0s\n",
            " 46100K .......... .......... .......... .......... .......... 80%  160M 0s\n",
            " 46150K .......... .......... .......... .......... .......... 80%  169M 0s\n",
            " 46200K .......... .......... .......... .......... .......... 81%  202M 0s\n",
            " 46250K .......... .......... .......... .......... .......... 81%  189M 0s\n",
            " 46300K .......... .......... .......... .......... .......... 81%  154M 0s\n",
            " 46350K .......... .......... .......... .......... .......... 81%  143M 0s\n",
            " 46400K .......... .......... .......... .......... .......... 81%  250M 0s\n",
            " 46450K .......... .......... .......... .......... .......... 81%  177M 0s\n",
            " 46500K .......... .......... .......... .......... .......... 81%  254M 0s\n",
            " 46550K .......... .......... .......... .......... .......... 81%  167M 0s\n",
            " 46600K .......... .......... .......... .......... .......... 81%  191M 0s\n",
            " 46650K .......... .......... .......... .......... .......... 81%  241M 0s\n",
            " 46700K .......... .......... .......... .......... .......... 81%  254M 0s\n",
            " 46750K .......... .......... .......... .......... .......... 81%  204M 0s\n",
            " 46800K .......... .......... .......... .......... .......... 82%  257M 0s\n",
            " 46850K .......... .......... .......... .......... .......... 82%  243M 0s\n",
            " 46900K .......... .......... .......... .......... .......... 82%  242M 0s\n",
            " 46950K .......... .......... .......... .......... .......... 82%  219M 0s\n",
            " 47000K .......... .......... .......... .......... .......... 82%  252M 0s\n",
            " 47050K .......... .......... .......... .......... .......... 82%  252M 0s\n",
            " 47100K .......... .......... .......... .......... .......... 82% 27.7M 0s\n",
            " 47150K .......... .......... .......... .......... .......... 82%  171M 0s\n",
            " 47200K .......... .......... .......... .......... .......... 82%  172M 0s\n",
            " 47250K .......... .......... .......... .......... .......... 82%  150M 0s\n",
            " 47300K .......... .......... .......... .......... .......... 82%  173M 0s\n",
            " 47350K .......... .......... .......... .......... .......... 83%  186M 0s\n",
            " 47400K .......... .......... .......... .......... .......... 83%  248M 0s\n",
            " 47450K .......... .......... .......... .......... .......... 83%  243M 0s\n",
            " 47500K .......... .......... .......... .......... .......... 83%  258M 0s\n",
            " 47550K .......... .......... .......... .......... .......... 83%  226M 0s\n",
            " 47600K .......... .......... .......... .......... .......... 83%  239M 0s\n",
            " 47650K .......... .......... .......... .......... .......... 83%  219M 0s\n",
            " 47700K .......... .......... .......... .......... .......... 83%  246M 0s\n",
            " 47750K .......... .......... .......... .......... .......... 83%  125M 0s\n",
            " 47800K .......... .......... .......... .......... .......... 83%  187M 0s\n",
            " 47850K .......... .......... .......... .......... .......... 83%  222M 0s\n",
            " 47900K .......... .......... .......... .......... .......... 83%  260M 0s\n",
            " 47950K .......... .......... .......... .......... .......... 84%  206M 0s\n",
            " 48000K .......... .......... .......... .......... .......... 84% 43.7M 0s\n",
            " 48050K .......... .......... .......... .......... .......... 84%  177M 0s\n",
            " 48100K .......... .......... .......... .......... .......... 84%  152M 0s\n",
            " 48150K .......... .......... .......... .......... .......... 84%  143M 0s\n",
            " 48200K .......... .......... .......... .......... .......... 84%  171M 0s\n",
            " 48250K .......... .......... .......... .......... .......... 84%  214M 0s\n",
            " 48300K .......... .......... .......... .......... .......... 84%  236M 0s\n",
            " 48350K .......... .......... .......... .......... .......... 84%  205M 0s\n",
            " 48400K .......... .......... .......... .......... .......... 84%  135M 0s\n",
            " 48450K .......... .......... .......... .......... .......... 84%  158M 0s\n",
            " 48500K .......... .......... .......... .......... .......... 85%  254M 0s\n",
            " 48550K .......... .......... .......... .......... .......... 85%  150M 0s\n",
            " 48600K .......... .......... .......... .......... .......... 85%  176M 0s\n",
            " 48650K .......... .......... .......... .......... .......... 85%  192M 0s\n",
            " 48700K .......... .......... .......... .......... .......... 85%  240M 0s\n",
            " 48750K .......... .......... .......... .......... .......... 85%  208M 0s\n",
            " 48800K .......... .......... .......... .......... .......... 85% 11.1M 0s\n",
            " 48850K .......... .......... .......... .......... .......... 85%  146M 0s\n",
            " 48900K .......... .......... .......... .......... .......... 85%  166M 0s\n",
            " 48950K .......... .......... .......... .......... .......... 85%  159M 0s\n",
            " 49000K .......... .......... .......... .......... .......... 85%  148M 0s\n",
            " 49050K .......... .......... .......... .......... .......... 85%  192M 0s\n",
            " 49100K .......... .......... .......... .......... .......... 86%  230M 0s\n",
            " 49150K .......... .......... .......... .......... .......... 86%  174M 0s\n",
            " 49200K .......... .......... .......... .......... .......... 86%  262M 0s\n",
            " 49250K .......... .......... .......... .......... .......... 86%  255M 0s\n",
            " 49300K .......... .......... .......... .......... .......... 86%  260M 0s\n",
            " 49350K .......... .......... .......... .......... .......... 86%  180M 0s\n",
            " 49400K .......... .......... .......... .......... .......... 86%  231M 0s\n",
            " 49450K .......... .......... .......... .......... .......... 86%  243M 0s\n",
            " 49500K .......... .......... .......... .......... .......... 86%  154M 0s\n",
            " 49550K .......... .......... .......... .......... .......... 86%  161M 0s\n",
            " 49600K .......... .......... .......... .......... .......... 86%  175M 0s\n",
            " 49650K .......... .......... .......... .......... .......... 87%  205M 0s\n",
            " 49700K .......... .......... .......... .......... .......... 87%  250M 0s\n",
            " 49750K .......... .......... .......... .......... .......... 87%  208M 0s\n",
            " 49800K .......... .......... .......... .......... .......... 87%  236M 0s\n",
            " 49850K .......... .......... .......... .......... .......... 87%  261M 0s\n",
            " 49900K .......... .......... .......... .......... .......... 87%  256M 0s\n",
            " 49950K .......... .......... .......... .......... .......... 87% 24.8M 0s\n",
            " 50000K .......... .......... .......... .......... .......... 87%  167M 0s\n",
            " 50050K .......... .......... .......... .......... .......... 87%  168M 0s\n",
            " 50100K .......... .......... .......... .......... .......... 87%  177M 0s\n",
            " 50150K .......... .......... .......... .......... .......... 87%  201M 0s\n",
            " 50200K .......... .......... .......... .......... .......... 88%  199M 0s\n",
            " 50250K .......... .......... .......... .......... .......... 88%  217M 0s\n",
            " 50300K .......... .......... .......... .......... .......... 88%  192M 0s\n",
            " 50350K .......... .......... .......... .......... .......... 88%  165M 0s\n",
            " 50400K .......... .......... .......... .......... .......... 88%  249M 0s\n",
            " 50450K .......... .......... .......... .......... .......... 88%  245M 0s\n",
            " 50500K .......... .......... .......... .......... .......... 88%  204M 0s\n",
            " 50550K .......... .......... .......... .......... .......... 88%  157M 0s\n",
            " 50600K .......... .......... .......... .......... .......... 88%  200M 0s\n",
            " 50650K .......... .......... .......... .......... .......... 88%  180M 0s\n",
            " 50700K .......... .......... .......... .......... .......... 88%  203M 0s\n",
            " 50750K .......... .......... .......... .......... .......... 88%  170M 0s\n",
            " 50800K .......... .......... .......... .......... .......... 89%  192M 0s\n",
            " 50850K .......... .......... .......... .......... .......... 89%  196M 0s\n",
            " 50900K .......... .......... .......... .......... .......... 89%  207M 0s\n",
            " 50950K .......... .......... .......... .......... .......... 89%  178M 0s\n",
            " 51000K .......... .......... .......... .......... .......... 89%  234M 0s\n",
            " 51050K .......... .......... .......... .......... .......... 89%  233M 0s\n",
            " 51100K .......... .......... .......... .......... .......... 89% 76.9M 0s\n",
            " 51150K .......... .......... .......... .......... .......... 89% 12.8M 0s\n",
            " 51200K .......... .......... .......... .......... .......... 89%  183M 0s\n",
            " 51250K .......... .......... .......... .......... .......... 89%  173M 0s\n",
            " 51300K .......... .......... .......... .......... .......... 89%  192M 0s\n",
            " 51350K .......... .......... .......... .......... .......... 90%  173M 0s\n",
            " 51400K .......... .......... .......... .......... .......... 90%  185M 0s\n",
            " 51450K .......... .......... .......... .......... .......... 90%  213M 0s\n",
            " 51500K .......... .......... .......... .......... .......... 90%  234M 0s\n",
            " 51550K .......... .......... .......... .......... .......... 90%  199M 0s\n",
            " 51600K .......... .......... .......... .......... .......... 90%  249M 0s\n",
            " 51650K .......... .......... .......... .......... .......... 90%  258M 0s\n",
            " 51700K .......... .......... .......... .......... .......... 90%  243M 0s\n",
            " 51750K .......... .......... .......... .......... .......... 90%  211M 0s\n",
            " 51800K .......... .......... .......... .......... .......... 90% 22.4M 0s\n",
            " 51850K .......... .......... .......... .......... .......... 90%  144M 0s\n",
            " 51900K .......... .......... .......... .......... .......... 90%  133M 0s\n",
            " 51950K .......... .......... .......... .......... .......... 91%  127M 0s\n",
            " 52000K .......... .......... .......... .......... .......... 91%  211M 0s\n",
            " 52050K .......... .......... .......... .......... .......... 91%  245M 0s\n",
            " 52100K .......... .......... .......... .......... .......... 91%  181M 0s\n",
            " 52150K .......... .......... .......... .......... .......... 91%  231M 0s\n",
            " 52200K .......... .......... .......... .......... .......... 91%  252M 0s\n",
            " 52250K .......... .......... .......... .......... .......... 91%  244M 0s\n",
            " 52300K .......... .......... .......... .......... .......... 91%  244M 0s\n",
            " 52350K .......... .......... .......... .......... .......... 91%  208M 0s\n",
            " 52400K .......... .......... .......... .......... .......... 91%  258M 0s\n",
            " 52450K .......... .......... .......... .......... .......... 91%  259M 0s\n",
            " 52500K .......... .......... .......... .......... .......... 92%  264M 0s\n",
            " 52550K .......... .......... .......... .......... .......... 92% 21.0M 0s\n",
            " 52600K .......... .......... .......... .......... .......... 92%  153M 0s\n",
            " 52650K .......... .......... .......... .......... .......... 92%  164M 0s\n",
            " 52700K .......... .......... .......... .......... .......... 92%  163M 0s\n",
            " 52750K .......... .......... .......... .......... .......... 92%  157M 0s\n",
            " 52800K .......... .......... .......... .......... .......... 92%  234M 0s\n",
            " 52850K .......... .......... .......... .......... .......... 92%  214M 0s\n",
            " 52900K .......... .......... .......... .......... .......... 92%  247M 0s\n",
            " 52950K .......... .......... .......... .......... .......... 92%  230M 0s\n",
            " 53000K .......... .......... .......... .......... .......... 92%  251M 0s\n",
            " 53050K .......... .......... .......... .......... .......... 92%  249M 0s\n",
            " 53100K .......... .......... .......... .......... .......... 93%  245M 0s\n",
            " 53150K .......... .......... .......... .......... .......... 93%  128M 0s\n",
            " 53200K .......... .......... .......... .......... .......... 93%  191M 0s\n",
            " 53250K .......... .......... .......... .......... .......... 93%  172M 0s\n",
            " 53300K .......... .......... .......... .......... .......... 93%  240M 0s\n",
            " 53350K .......... .......... .......... .......... .......... 93%  219M 0s\n",
            " 53400K .......... .......... .......... .......... .......... 93%  253M 0s\n",
            " 53450K .......... .......... .......... .......... .......... 93%  258M 0s\n",
            " 53500K .......... .......... .......... .......... .......... 93%  250M 0s\n",
            " 53550K .......... .......... .......... .......... .......... 93% 17.2M 0s\n",
            " 53600K .......... .......... .......... .......... .......... 93%  157M 0s\n",
            " 53650K .......... .......... .......... .......... .......... 94%  166M 0s\n",
            " 53700K .......... .......... .......... .......... .......... 94%  193M 0s\n",
            " 53750K .......... .......... .......... .......... .......... 94%  162M 0s\n",
            " 53800K .......... .......... .......... .......... .......... 94%  164M 0s\n",
            " 53850K .......... .......... .......... .......... .......... 94%  240M 0s\n",
            " 53900K .......... .......... .......... .......... .......... 94%  259M 0s\n",
            " 53950K .......... .......... .......... .......... .......... 94%  210M 0s\n",
            " 54000K .......... .......... .......... .......... .......... 94%  256M 0s\n",
            " 54050K .......... .......... .......... .......... .......... 94%  214M 0s\n",
            " 54100K .......... .......... .......... .......... .......... 94%  239M 0s\n",
            " 54150K .......... .......... .......... .......... .......... 94%  213M 0s\n",
            " 54200K .......... .......... .......... .......... .......... 95%  257M 0s\n",
            " 54250K .......... .......... .......... .......... .......... 95% 22.9M 0s\n",
            " 54300K .......... .......... .......... .......... .......... 95%  192M 0s\n",
            " 54350K .......... .......... .......... .......... .......... 95%  153M 0s\n",
            " 54400K .......... .......... .......... .......... .......... 95%  168M 0s\n",
            " 54450K .......... .......... .......... .......... .......... 95%  180M 0s\n",
            " 54500K .......... .......... .......... .......... .......... 95%  204M 0s\n",
            " 54550K .......... .......... .......... .......... .......... 95%  147M 0s\n",
            " 54600K .......... .......... .......... .......... .......... 95%  228M 0s\n",
            " 54650K .......... .......... .......... .......... .......... 95%  242M 0s\n",
            " 54700K .......... .......... .......... .......... .......... 95%  261M 0s\n",
            " 54750K .......... .......... .......... .......... .......... 95%  221M 0s\n",
            " 54800K .......... .......... .......... .......... .......... 96%  255M 0s\n",
            " 54850K .......... .......... .......... .......... .......... 96%  247M 0s\n",
            " 54900K .......... .......... .......... .......... .......... 96%  250M 0s\n",
            " 54950K .......... .......... .......... .......... .......... 96%  235M 0s\n",
            " 55000K .......... .......... .......... .......... .......... 96%  259M 0s\n",
            " 55050K .......... .......... .......... .......... .......... 96% 14.1M 0s\n",
            " 55100K .......... .......... .......... .......... .......... 96%  136M 0s\n",
            " 55150K .......... .......... .......... .......... .......... 96%  120M 0s\n",
            " 55200K .......... .......... .......... .......... .......... 96%  123M 0s\n",
            " 55250K .......... .......... .......... .......... .......... 96%  134M 0s\n",
            " 55300K .......... .......... .......... .......... .......... 96%  235M 0s\n",
            " 55350K .......... .......... .......... .......... .......... 97%  207M 0s\n",
            " 55400K .......... .......... .......... .......... .......... 97%  237M 0s\n",
            " 55450K .......... .......... .......... .......... .......... 97%  245M 0s\n",
            " 55500K .......... .......... .......... .......... .......... 97%  246M 0s\n",
            " 55550K .......... .......... .......... .......... .......... 97%  205M 0s\n",
            " 55600K .......... .......... .......... .......... .......... 97%  225M 0s\n",
            " 55650K .......... .......... .......... .......... .......... 97%  246M 0s\n",
            " 55700K .......... .......... .......... .......... .......... 97%  235M 0s\n",
            " 55750K .......... .......... .......... .......... .......... 97%  218M 0s\n",
            " 55800K .......... .......... .......... .......... .......... 97%  161M 0s\n",
            " 55850K .......... .......... .......... .......... .......... 97%  148M 0s\n",
            " 55900K .......... .......... .......... .......... .......... 97%  176M 0s\n",
            " 55950K .......... .......... .......... .......... .......... 98%  214M 0s\n",
            " 56000K .......... .......... .......... .......... .......... 98%  250M 0s\n",
            " 56050K .......... .......... .......... .......... .......... 98%  246M 0s\n",
            " 56100K .......... .......... .......... .......... .......... 98%  250M 0s\n",
            " 56150K .......... .......... .......... .......... .......... 98%  219M 0s\n",
            " 56200K .......... .......... .......... .......... .......... 98%  258M 0s\n",
            " 56250K .......... .......... .......... .......... .......... 98%  228M 0s\n",
            " 56300K .......... .......... .......... .......... .......... 98%  156M 0s\n",
            " 56350K .......... .......... .......... .......... .......... 98%  187M 0s\n",
            " 56400K .......... .......... .......... .......... .......... 98%  210M 0s\n",
            " 56450K .......... .......... .......... .......... .......... 98%  141M 0s\n",
            " 56500K .......... .......... .......... .......... .......... 99%  193M 0s\n",
            " 56550K .......... .......... .......... .......... .......... 99%  210M 0s\n",
            " 56600K .......... .......... .......... .......... .......... 99%  246M 0s\n",
            " 56650K .......... .......... .......... .......... .......... 99%  259M 0s\n",
            " 56700K .......... .......... .......... .......... .......... 99%  249M 0s\n",
            " 56750K .......... .......... .......... .......... .......... 99% 17.3M 0s\n",
            " 56800K .......... .......... .......... .......... .......... 99%  203M 0s\n",
            " 56850K .......... .......... .......... .......... .......... 99%  262M 0s\n",
            " 56900K .......... .......... .......... .......... .......... 99%  180M 0s\n",
            " 56950K .......... .......... .......... .......... .......... 99%  180M 0s\n",
            " 57000K .......... .......... .......... .......... .......... 99%  240M 0s\n",
            " 57050K .......... .......... .......... .......... ........  100%  253M=0.4s\n",
            "\n",
            "2022-04-17 18:22:00 (139 MB/s) - ‘Miniconda3-4.5.4-Linux-x86_64.sh’ saved [58468498/58468498]\n",
            "\n",
            "Python 3.6.5 :: Anaconda, Inc.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "conda install --channel defaults conda python=3.7 --yes\n",
        "conda update --channel defaults --all --yes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "svkqJx4w-_Rj",
        "outputId": "cbc5c50b-987c-4c06-8028-ccd9a930adc0"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Solving environment: ...working... done\n",
            "\n",
            "## Package Plan ##\n",
            "\n",
            "  environment location: /usr/local\n",
            "\n",
            "  added / updated specs: \n",
            "    - conda\n",
            "    - python=3.7\n",
            "\n",
            "\n",
            "The following packages will be downloaded:\n",
            "\n",
            "    package                    |            build\n",
            "    ---------------------------|-----------------\n",
            "    _libgcc_mutex-0.1          |             main           3 KB\n",
            "    libgcc-ng-9.1.0            |       hdf63c60_0         8.1 MB\n",
            "    yaml-0.2.5                 |       h7b6447c_0          87 KB\n",
            "    pycosat-0.6.3              |   py37h27cfd23_0         108 KB\n",
            "    pyopenssl-22.0.0           |     pyhd3eb1b0_0          49 KB\n",
            "    zlib-1.2.11                |       h7f8727e_4         125 KB\n",
            "    pycparser-2.21             |     pyhd3eb1b0_0          94 KB\n",
            "    tqdm-4.63.0                |     pyhd3eb1b0_0          80 KB\n",
            "    setuptools-61.2.0          |   py37h06a4308_0         1.3 MB\n",
            "    brotlipy-0.7.0             |py37h27cfd23_1003         350 KB\n",
            "    cryptography-36.0.0        |   py37h9ce1e76_0         1.5 MB\n",
            "    ld_impl_linux-64-2.35.1    |       h7274673_9         637 KB\n",
            "    ca-certificates-2022.3.29  |       h06a4308_0         124 KB\n",
            "    certifi-2021.10.8          |   py37h06a4308_2         156 KB\n",
            "    urllib3-1.26.8             |     pyhd3eb1b0_0         100 KB\n",
            "    libffi-3.3                 |       he6710b0_2          54 KB\n",
            "    ruamel_yaml-0.15.100       |   py37h27cfd23_0         267 KB\n",
            "    conda-package-handling-1.8.1|   py37h7f8727e_0         954 KB\n",
            "    libstdcxx-ng-9.1.0         |       hdf63c60_0         4.0 MB\n",
            "    requests-2.27.1            |     pyhd3eb1b0_0          52 KB\n",
            "    cffi-1.15.0                |   py37hd667e15_1         224 KB\n",
            "    idna-3.3                   |     pyhd3eb1b0_0          55 KB\n",
            "    tk-8.6.11                  |       h1ccaba5_0         3.2 MB\n",
            "    pysocks-1.7.1              |           py37_1          27 KB\n",
            "    colorama-0.4.4             |     pyhd3eb1b0_0          21 KB\n",
            "    xz-5.2.5                   |       h7b6447c_0         438 KB\n",
            "    python-3.7.13              |       h12debd9_0        53.5 MB\n",
            "    ncurses-6.3                |       h7f8727e_2         1.0 MB\n",
            "    sqlite-3.38.2              |       hc218d9a_0         1.5 MB\n",
            "    conda-4.12.0               |   py37h06a4308_0        16.9 MB\n",
            "    readline-8.1.2             |       h7f8727e_1         423 KB\n",
            "    openssl-1.1.1n             |       h7f8727e_0         3.8 MB\n",
            "    wheel-0.37.1               |     pyhd3eb1b0_0          31 KB\n",
            "    pip-21.2.2                 |   py37h06a4308_0         2.0 MB\n",
            "    charset-normalizer-2.0.4   |     pyhd3eb1b0_0          33 KB\n",
            "    ------------------------------------------------------------\n",
            "                                           Total:       101.4 MB\n",
            "\n",
            "The following NEW packages will be INSTALLED:\n",
            "\n",
            "    _libgcc_mutex:          0.1-main               \n",
            "    brotlipy:               0.7.0-py37h27cfd23_1003\n",
            "    charset-normalizer:     2.0.4-pyhd3eb1b0_0     \n",
            "    colorama:               0.4.4-pyhd3eb1b0_0     \n",
            "    conda-package-handling: 1.8.1-py37h7f8727e_0   \n",
            "    ld_impl_linux-64:       2.35.1-h7274673_9      \n",
            "    tqdm:                   4.63.0-pyhd3eb1b0_0    \n",
            "\n",
            "The following packages will be UPDATED:\n",
            "\n",
            "    ca-certificates:        2018.03.07-0            --> 2022.3.29-h06a4308_0    \n",
            "    certifi:                2018.4.16-py36_0        --> 2021.10.8-py37h06a4308_2\n",
            "    cffi:                   1.11.5-py36h9745a5d_0   --> 1.15.0-py37hd667e15_1   \n",
            "    conda:                  4.5.4-py36_0            --> 4.12.0-py37h06a4308_0   \n",
            "    cryptography:           2.2.2-py36h14c3975_0    --> 36.0.0-py37h9ce1e76_0   \n",
            "    idna:                   2.6-py36h82fb2a8_1      --> 3.3-pyhd3eb1b0_0        \n",
            "    libffi:                 3.2.1-hd88cf55_4        --> 3.3-he6710b0_2          \n",
            "    libgcc-ng:              7.2.0-hdf63c60_3        --> 9.1.0-hdf63c60_0        \n",
            "    libstdcxx-ng:           7.2.0-hdf63c60_3        --> 9.1.0-hdf63c60_0        \n",
            "    ncurses:                6.1-hf484d3e_0          --> 6.3-h7f8727e_2          \n",
            "    openssl:                1.0.2o-h20670df_0       --> 1.1.1n-h7f8727e_0       \n",
            "    pip:                    10.0.1-py36_0           --> 21.2.2-py37h06a4308_0   \n",
            "    pycosat:                0.6.3-py36h0a5515d_0    --> 0.6.3-py37h27cfd23_0    \n",
            "    pycparser:              2.18-py36hf9f622e_1     --> 2.21-pyhd3eb1b0_0       \n",
            "    pyopenssl:              18.0.0-py36_0           --> 22.0.0-pyhd3eb1b0_0     \n",
            "    pysocks:                1.6.8-py36_0            --> 1.7.1-py37_1            \n",
            "    python:                 3.6.5-hc3d631a_2        --> 3.7.13-h12debd9_0       \n",
            "    readline:               7.0-ha6073c6_4          --> 8.1.2-h7f8727e_1        \n",
            "    requests:               2.18.4-py36he2e5f8d_1   --> 2.27.1-pyhd3eb1b0_0     \n",
            "    ruamel_yaml:            0.15.37-py36h14c3975_2  --> 0.15.100-py37h27cfd23_0 \n",
            "    setuptools:             39.2.0-py36_0           --> 61.2.0-py37h06a4308_0   \n",
            "    sqlite:                 3.23.1-he433501_0       --> 3.38.2-hc218d9a_0       \n",
            "    tk:                     8.6.7-hc745277_3        --> 8.6.11-h1ccaba5_0       \n",
            "    urllib3:                1.22-py36hbe7ace6_0     --> 1.26.8-pyhd3eb1b0_0     \n",
            "    wheel:                  0.31.1-py36_0           --> 0.37.1-pyhd3eb1b0_0     \n",
            "    xz:                     5.2.4-h14c3975_4        --> 5.2.5-h7b6447c_0        \n",
            "    yaml:                   0.1.7-had09818_2        --> 0.2.5-h7b6447c_0        \n",
            "    zlib:                   1.2.11-ha838bed_2       --> 1.2.11-h7f8727e_4       \n",
            "\n",
            "\n",
            "Downloading and Extracting Packages\n",
            "Preparing transaction: ...working... done\n",
            "Verifying transaction: ...working... done\n",
            "Executing transaction: ...working... done\n",
            "Collecting package metadata (current_repodata.json): ...working... done\n",
            "Solving environment: ...working... done\n",
            "\n",
            "## Package Plan ##\n",
            "\n",
            "  environment location: /usr/local\n",
            "\n",
            "\n",
            "The following packages will be downloaded:\n",
            "\n",
            "    package                    |            build\n",
            "    ---------------------------|-----------------\n",
            "    _openmp_mutex-4.5          |            1_gnu          22 KB\n",
            "    libgcc-ng-9.3.0            |      h5101ec6_17         4.8 MB\n",
            "    libgomp-9.3.0              |      h5101ec6_17         311 KB\n",
            "    libstdcxx-ng-9.3.0         |      hd4cf53a_17         3.1 MB\n",
            "    ------------------------------------------------------------\n",
            "                                           Total:         8.2 MB\n",
            "\n",
            "The following NEW packages will be INSTALLED:\n",
            "\n",
            "  _openmp_mutex      pkgs/main/linux-64::_openmp_mutex-4.5-1_gnu\n",
            "  libgomp            pkgs/main/linux-64::libgomp-9.3.0-h5101ec6_17\n",
            "\n",
            "The following packages will be UPDATED:\n",
            "\n",
            "  libgcc-ng                                9.1.0-hdf63c60_0 --> 9.3.0-h5101ec6_17\n",
            "  libstdcxx-ng                             9.1.0-hdf63c60_0 --> 9.3.0-hd4cf53a_17\n",
            "\n",
            "\n",
            "\n",
            "Downloading and Extracting Packages\n",
            "\r_openmp_mutex-4.5    | 22 KB     |            |   0% \r_openmp_mutex-4.5    | 22 KB     | ########## | 100% \n",
            "\rlibgcc-ng-9.3.0      | 4.8 MB    |            |   0% \rlibgcc-ng-9.3.0      | 4.8 MB    | ########## | 100% \rlibgcc-ng-9.3.0      | 4.8 MB    | ########## | 100% \n",
            "\rlibgomp-9.3.0        | 311 KB    |            |   0% \rlibgomp-9.3.0        | 311 KB    | ########## | 100% \n",
            "\rlibstdcxx-ng-9.3.0   | 3.1 MB    |            |   0% \rlibstdcxx-ng-9.3.0   | 3.1 MB    | ########## | 100% \rlibstdcxx-ng-9.3.0   | 3.1 MB    | ########## | 100% \n",
            "Preparing transaction: ...working... done\n",
            "Verifying transaction: ...working... done\n",
            "Executing transaction: ...working... done\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r_libgcc_mutex-0.1    |    3 KB |            |   0% \r_libgcc_mutex-0.1    |    3 KB | ########## | 100% \n",
            "\rlibgcc-ng-9.1.0      |  8.1 MB |            |   0% \rlibgcc-ng-9.1.0      |  8.1 MB | #######5   |  75% \rlibgcc-ng-9.1.0      |  8.1 MB | #########7 |  98% \rlibgcc-ng-9.1.0      |  8.1 MB | ########## | 100% \n",
            "\ryaml-0.2.5           |   87 KB |            |   0% \ryaml-0.2.5           |   87 KB | ########## | 100% \n",
            "\rpycosat-0.6.3        |  108 KB |            |   0% \rpycosat-0.6.3        |  108 KB | ########## | 100% \n",
            "\rpyopenssl-22.0.0     |   49 KB |            |   0% \rpyopenssl-22.0.0     |   49 KB | ########## | 100% \n",
            "\rzlib-1.2.11          |  125 KB |            |   0% \rzlib-1.2.11          |  125 KB | ########## | 100% \n",
            "\rpycparser-2.21       |   94 KB |            |   0% \rpycparser-2.21       |   94 KB | ########## | 100% \n",
            "\rtqdm-4.63.0          |   80 KB |            |   0% \rtqdm-4.63.0          |   80 KB | ########## | 100% \n",
            "\rsetuptools-61.2.0    |  1.3 MB |            |   0% \rsetuptools-61.2.0    |  1.3 MB | #######9   |  80% \rsetuptools-61.2.0    |  1.3 MB | ########## | 100% \n",
            "\rbrotlipy-0.7.0       |  350 KB |            |   0% \rbrotlipy-0.7.0       |  350 KB | ########## | 100% \n",
            "\rcryptography-36.0.0  |  1.5 MB |            |   0% \rcryptography-36.0.0  |  1.5 MB | #######7   |  78% \rcryptography-36.0.0  |  1.5 MB | #########9 |  99% \rcryptography-36.0.0  |  1.5 MB | ########## | 100% \n",
            "\rld_impl_linux-64-2.3 |  637 KB |            |   0% \rld_impl_linux-64-2.3 |  637 KB | #########7 |  98% \rld_impl_linux-64-2.3 |  637 KB | ########## | 100% \n",
            "\rca-certificates-2022 |  124 KB |            |   0% \rca-certificates-2022 |  124 KB | ########## | 100% \n",
            "\rcertifi-2021.10.8    |  156 KB |            |   0% \rcertifi-2021.10.8    |  156 KB | ########## | 100% \n",
            "\rurllib3-1.26.8       |  100 KB |            |   0% \rurllib3-1.26.8       |  100 KB | ########## | 100% \n",
            "\rlibffi-3.3           |   54 KB |            |   0% \rlibffi-3.3           |   54 KB | ########## | 100% \n",
            "\rruamel_yaml-0.15.100 |  267 KB |            |   0% \rruamel_yaml-0.15.100 |  267 KB | ########## | 100% \n",
            "\rconda-package-handli |  954 KB |            |   0% \rconda-package-handli |  954 KB | ########5  |  85% \rconda-package-handli |  954 KB | ########## | 100% \n",
            "\rlibstdcxx-ng-9.1.0   |  4.0 MB |            |   0% \rlibstdcxx-ng-9.1.0   |  4.0 MB | #######7   |  77% \rlibstdcxx-ng-9.1.0   |  4.0 MB | ########## | 100% \n",
            "\rrequests-2.27.1      |   52 KB |            |   0% \rrequests-2.27.1      |   52 KB | ########## | 100% \n",
            "\rcffi-1.15.0          |  224 KB |            |   0% \rcffi-1.15.0          |  224 KB | ########## | 100% \n",
            "\ridna-3.3             |   55 KB |            |   0% \ridna-3.3             |   55 KB | ########## | 100% \n",
            "\rtk-8.6.11            |  3.2 MB |            |   0% \rtk-8.6.11            |  3.2 MB | #######6   |  77% \rtk-8.6.11            |  3.2 MB | #########5 |  96% \rtk-8.6.11            |  3.2 MB | ########## | 100% \n",
            "\rpysocks-1.7.1        |   27 KB |            |   0% \rpysocks-1.7.1        |   27 KB | ########## | 100% \n",
            "\rcolorama-0.4.4       |   21 KB |            |   0% \rcolorama-0.4.4       |   21 KB | ########## | 100% \n",
            "\rxz-5.2.5             |  438 KB |            |   0% \rxz-5.2.5             |  438 KB | ########## | 100% \n",
            "\rpython-3.7.13        | 53.5 MB |            |   0% \rpython-3.7.13        | 53.5 MB | ##1        |  21% \rpython-3.7.13        | 53.5 MB | ####6      |  46% \rpython-3.7.13        | 53.5 MB | #####8     |  59% \rpython-3.7.13        | 53.5 MB | #######1   |  72% \rpython-3.7.13        | 53.5 MB | ########3  |  84% \rpython-3.7.13        | 53.5 MB | #########2 |  93% \rpython-3.7.13        | 53.5 MB | #########8 |  99% \rpython-3.7.13        | 53.5 MB | ########## | 100% \n",
            "\rncurses-6.3          |  1.0 MB |            |   0% \rncurses-6.3          |  1.0 MB | ########5  |  86% \rncurses-6.3          |  1.0 MB | ########## | 100% \n",
            "\rsqlite-3.38.2        |  1.5 MB |            |   0% \rsqlite-3.38.2        |  1.5 MB | ########5  |  85% \rsqlite-3.38.2        |  1.5 MB | ########## | 100% \n",
            "\rconda-4.12.0         | 16.9 MB |            |   0% \rconda-4.12.0         | 16.9 MB | #####6     |  57% \rconda-4.12.0         | 16.9 MB | #######5   |  75% \rconda-4.12.0         | 16.9 MB | #########1 |  91% \rconda-4.12.0         | 16.9 MB | ########## | 100% \n",
            "\rreadline-8.1.2       |  423 KB |            |   0% \rreadline-8.1.2       |  423 KB | ########## | 100% \n",
            "\ropenssl-1.1.1n       |  3.8 MB |            |   0% \ropenssl-1.1.1n       |  3.8 MB | #######7   |  77% \ropenssl-1.1.1n       |  3.8 MB | #########9 | 100% \ropenssl-1.1.1n       |  3.8 MB | ########## | 100% \n",
            "\rwheel-0.37.1         |   31 KB |            |   0% \rwheel-0.37.1         |   31 KB | ########## | 100% \n",
            "\rpip-21.2.2           |  2.0 MB |            |   0% \rpip-21.2.2           |  2.0 MB | #######7   |  78% \rpip-21.2.2           |  2.0 MB | #########6 |  96% \rpip-21.2.2           |  2.0 MB | ########## | 100% \n",
            "\rcharset-normalizer-2 |   33 KB |            |   0% \rcharset-normalizer-2 |   33 KB | ########## | 100% \n",
            "\n",
            "The environment is inconsistent, please check the package plan carefully\n",
            "The following packages are causing the inconsistency:\n",
            "\n",
            "  - defaults/linux-64::asn1crypto==0.24.0=py36_0\n",
            "  - defaults/linux-64::six==1.11.0=py36h372c433_1\n",
            "  - defaults/linux-64::chardet==3.0.4=py36h0f667ec_1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mTZMR43S_pbs",
        "outputId": "76f8e23b-2d64-4837-bf0b-baa64f0af771"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['',\n",
              " '/content',\n",
              " '/env/python',\n",
              " '/usr/lib/python37.zip',\n",
              " '/usr/lib/python3.7',\n",
              " '/usr/lib/python3.7/lib-dynload',\n",
              " '/usr/local/lib/python3.7/dist-packages',\n",
              " '/usr/lib/python3/dist-packages',\n",
              " '/usr/local/lib/python3.7/dist-packages/IPython/extensions',\n",
              " '/root/.ipython']"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "_ = (sys.path\n",
        "        .append(\"/usr/local/lib/python3.7/site-packages\"))"
      ],
      "metadata": {
        "id": "WGxCzFAZ_4Mx"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/ozanozyegen/evaluation-local-explanation-time-series"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tKp3QPYJ5Xo2",
        "outputId": "0dee25e0-eaa8-463e-8eda-a1be1ca1bb0a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'evaluation-local-explanation-time-series'...\n",
            "remote: Enumerating objects: 65, done.\u001b[K\n",
            "remote: Counting objects: 100% (65/65), done.\u001b[K\n",
            "remote: Compressing objects: 100% (63/63), done.\u001b[K\n",
            "remote: Total 65 (delta 4), reused 52 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (65/65), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd evaluation-local-explanation-time-series/src"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CGsWsrLtQdAy",
        "outputId": "d01c8f6c-577c-4043-bcba-85e3596af93a"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/evaluation-local-explanation-time-series/src\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!conda env create -n eval-metric -f xai.yml"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3sUC2W7sC1CK",
        "outputId": "a2845d13-2f91-468e-ec31-944ef15e9838"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting package metadata (repodata.json): - \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\bdone\n",
            "Solving environment: \\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\bdone\n",
            "\n",
            "Downloading and Extracting Packages\n",
            "matplotlib-3.3.4     | 26 KB     | : 100% 1.0/1 [00:00<00:00,  8.24it/s]\n",
            "pandocfilters-1.4.3  | 14 KB     | : 100% 1.0/1 [00:00<00:00, 10.19it/s]\n",
            "sqlite-3.33.0        | 1.1 MB    | : 100% 1.0/1 [00:00<00:00,  7.70it/s]\n",
            "deprecated-1.2.10    | 11 KB     | : 100% 1.0/1 [00:00<00:00,  7.77it/s]\n",
            "pyqt-5.9.2           | 4.5 MB    | : 100% 1.0/1 [00:00<00:00,  3.73it/s]\n",
            "nbconvert-6.0.7      | 484 KB    | : 100% 1.0/1 [00:00<00:00,  7.38it/s]\n",
            "chardet-3.0.4        | 175 KB    | : 100% 1.0/1 [00:00<00:00,  8.89it/s]\n",
            "blas-1.0             | 1 KB      | : 100% 1.0/1 [00:00<00:00, 29.19it/s]\n",
            "icu-58.2             | 10.5 MB   | : 100% 1.0/1 [00:00<00:00,  2.31it/s]\n",
            "cupti-10.1.168       | 1.4 MB    | : 100% 1.0/1 [00:00<00:00,  7.33it/s]\n",
            "async-timeout-3.0.1  | 13 KB     | : 100% 1.0/1 [00:00<00:00,  8.99it/s]\n",
            "pandoc-2.11          | 9.6 MB    | : 100% 1.0/1 [00:00<00:00,  2.17it/s]               \n",
            "urllib3-1.26.3       | 105 KB    | : 100% 1.0/1 [00:00<00:00,  7.98it/s]\n",
            "pyasn1-0.4.8         | 53 KB     | : 100% 1.0/1 [00:00<00:00, 22.68it/s]\n",
            "cffi-1.14.5          | 224 KB    | : 100% 1.0/1 [00:00<00:00,  8.24it/s]\n",
            "readline-8.1         | 362 KB    | : 100% 1.0/1 [00:00<00:00,  8.88it/s]\n",
            "entrypoints-0.3      | 12 KB     | : 100% 1.0/1 [00:00<00:00, 10.03it/s]\n",
            "tornado-6.1          | 589 KB    | : 100% 1.0/1 [00:00<00:00,  7.67it/s]\n",
            "_tflow_select-2.1.0  | 2 KB      | : 100% 1.0/1 [00:00<00:00, 10.94it/s]\n",
            "mccabe-0.6.1         | 14 KB     | : 100% 1.0/1 [00:00<00:00, 10.66it/s]\n",
            "backcall-0.2.0       | 13 KB     | : 100% 1.0/1 [00:00<00:00, 41.65it/s]\n",
            "decorator-4.4.2      | 12 KB     | : 100% 1.0/1 [00:00<00:00, 52.76it/s]\n",
            "intel-openmp-2020.2  | 786 KB    | : 100% 1.0/1 [00:00<00:00,  8.26it/s]\n",
            "ld_impl_linux-64-2.3 | 653 KB    | : 100% 1.0/1 [00:00<00:00,  6.17it/s]\n",
            "qt-5.9.7             | 68.5 MB   | : 100% 1.0/1 [00:02<00:00,  2.47s/it]               \n",
            "google-pasta-0.2.0   | 46 KB     | : 100% 1.0/1 [00:00<00:00,  8.44it/s]\n",
            "pyparsing-2.4.7      | 59 KB     | : 100% 1.0/1 [00:00<00:00, 24.27it/s]\n",
            "absl-py-0.11.0       | 103 KB    | : 100% 1.0/1 [00:00<00:00,  6.92it/s]\n",
            "freetype-2.10.4      | 596 KB    | : 100% 1.0/1 [00:00<00:00,  7.93it/s]\n",
            "pycodestyle-2.6.0    | 38 KB     | : 100% 1.0/1 [00:00<00:00, 40.16it/s]\n",
            "jupyter_client-6.1.7 | 76 KB     | : 100% 1.0/1 [00:00<00:00, 14.32it/s]\n",
            "parso-0.7.0          | 72 KB     | : 100% 1.0/1 [00:00<00:00, 10.15it/s]\n",
            "mkl_random-1.1.1     | 322 KB    | : 100% 1.0/1 [00:00<00:00,  9.66it/s]\n",
            "pygments-2.7.4       | 676 KB    | : 100% 1.0/1 [00:00<00:00,  7.77it/s]\n",
            "packaging-20.9       | 37 KB     | : 100% 1.0/1 [00:00<00:00, 10.43it/s]\n",
            "pycparser-2.20       | 94 KB     | : 100% 1.0/1 [00:00<00:00,  9.23it/s]\n",
            "yarl-1.6.3           | 133 KB    | : 100% 1.0/1 [00:00<00:00,  9.56it/s]\n",
            "gst-plugins-base-1.1 | 4.9 MB    | : 100% 1.0/1 [00:00<00:00,  4.46it/s]\n",
            "libsodium-1.0.18     | 244 KB    | : 100% 1.0/1 [00:00<00:00,  7.66it/s]\n",
            "grpcio-1.35.0        | 1.8 MB    | : 100% 1.0/1 [00:00<00:00,  5.87it/s]\n",
            "pandas-1.2.2         | 8.5 MB    | : 100% 1.0/1 [00:00<00:00,  1.65it/s]\n",
            "send2trash-1.5.0     | 14 KB     | : 100% 1.0/1 [00:00<00:00,  9.30it/s]\n",
            "python_abi-3.7       | 4 KB      | : 100% 1.0/1 [00:00<00:00, 31.78it/s]\n",
            "nbclient-0.5.2       | 58 KB     | : 100% 1.0/1 [00:00<00:00,  8.78it/s]\n",
            "threadpoolctl-2.1.0  | 15 KB     | : 100% 1.0/1 [00:00<00:00, 33.10it/s]\n",
            "protobuf-3.14.0      | 303 KB    | : 100% 1.0/1 [00:00<00:00,  7.90it/s]\n",
            "prometheus_client-0. | 45 KB     | : 100% 1.0/1 [00:00<00:00,  8.59it/s]\n",
            "opt_einsum-3.1.0     | 48 KB     | : 100% 1.0/1 [00:00<00:00, 19.83it/s]\n",
            "dbus-1.13.18         | 504 KB    | : 100% 1.0/1 [00:00<00:00,  8.36it/s]\n",
            "lz4-c-1.9.3          | 186 KB    | : 100% 1.0/1 [00:00<00:00,  9.56it/s]\n",
            "jpeg-9b              | 214 KB    | : 100% 1.0/1 [00:00<00:00, 10.13it/s]\n",
            "astunparse-1.6.3     | 17 KB     | : 100% 1.0/1 [00:00<00:00, 10.47it/s]\n",
            "markupsafe-1.1.1     | 26 KB     | : 100% 1.0/1 [00:00<00:00, 10.86it/s]\n",
            "multidict-4.7.6      | 65 KB     | : 100% 1.0/1 [00:00<00:00,  4.67it/s]                \n",
            "libxcb-1.14          | 505 KB    | : 100% 1.0/1 [00:00<00:00,  8.81it/s]\n",
            "idna-2.10            | 52 KB     | : 100% 1.0/1 [00:00<00:00, 38.71it/s]\n",
            "oauthlib-3.1.0       | 91 KB     | : 100% 1.0/1 [00:00<00:00,  9.86it/s]\n",
            "glib-2.67.4          | 3.0 MB    | : 100% 1.0/1 [00:00<00:00,  4.87it/s]\n",
            "click-7.1.2          | 64 KB     | : 100% 1.0/1 [00:00<00:00, 28.44it/s]\n",
            "expat-2.2.10         | 153 KB    | : 100% 1.0/1 [00:00<00:00,  8.74it/s]\n",
            "ipython_genutils-0.2 | 27 KB     | : 100% 1.0/1 [00:00<00:00, 10.61it/s]\n",
            "jupyter-1.0.0        | 6 KB      | : 100% 1.0/1 [00:00<00:00, 10.50it/s]\n",
            "coverage-5.4         | 259 KB    | : 100% 1.0/1 [00:00<00:00,  7.44it/s]\n",
            "argon2-cffi-20.1.0   | 46 KB     | : 100% 1.0/1 [00:00<00:00,  9.69it/s]\n",
            "rsa-4.7              | 30 KB     | : 100% 1.0/1 [00:00<00:00,  8.93it/s]\n",
            "importlib_metadata-2 | 3 KB      | : 100% 1.0/1 [00:00<00:00, 19.41it/s]\n",
            "tensorflow-gpu-2.2.0 | 3 KB      | : 100% 1.0/1 [00:00<00:00,  8.68it/s]\n",
            "mkl-service-2.3.0    | 52 KB     | : 100% 1.0/1 [00:00<00:00, 10.31it/s]\n",
            "autopep8-1.5.5       | 42 KB     | : 100% 1.0/1 [00:00<00:00, 10.41it/s]\n",
            "prompt-toolkit-3.0.8 | 235 KB    | : 100% 1.0/1 [00:00<00:00,  9.44it/s]\n",
            "libtiff-4.1.0        | 449 KB    | : 100% 1.0/1 [00:00<00:00,  8.97it/s]\n",
            "traitlets-5.0.5      | 81 KB     | : 100% 1.0/1 [00:00<00:00, 27.97it/s]\n",
            "prompt_toolkit-3.0.8 | 4 KB      | : 100% 1.0/1 [00:00<00:00, 26.04it/s]\n",
            "jupyter_console-6.2. | 22 KB     | : 100% 1.0/1 [00:00<00:00, 26.02it/s]\n",
            "cudatoolkit-10.1.243 | 347.4 MB  | : 100% 1.0/1 [00:07<00:00,  7.86s/it]               \n",
            "async_generator-1.10 | 39 KB     | : 100% 1.0/1 [00:00<00:00,  9.42it/s]\n",
            "bleach-3.3.0         | 113 KB    | : 100% 1.0/1 [00:00<00:00,  9.37it/s]\n",
            "certifi-2020.12.5    | 141 KB    | : 100% 1.0/1 [00:00<00:00,  8.39it/s]\n",
            "tensorflow-estimator | 254 KB    | : 100% 1.0/1 [00:00<00:00,  7.32it/s]\n",
            "matplotlib-base-3.3. | 5.1 MB    | : 100% 1.0/1 [00:00<00:00,  3.69it/s]\n",
            "cython-0.29.21       | 1.9 MB    | : 100% 1.0/1 [00:00<00:00,  5.22it/s]\n",
            "libxml2-2.9.10       | 1.2 MB    | : 100% 1.0/1 [00:00<00:00,  6.53it/s]\n",
            "jedi-0.17.2          | 918 KB    | : 100% 1.0/1 [00:00<00:00,  3.42it/s]\n",
            "jupyter_core-4.7.1   | 68 KB     | : 100% 1.0/1 [00:00<00:00,  7.94it/s]\n",
            "lazy-object-proxy-1. | 28 KB     | : 100% 1.0/1 [00:00<00:00,  9.44it/s]\n",
            "attrs-20.3.0         | 43 KB     | : 100% 1.0/1 [00:00<00:00, 10.46it/s]\n",
            "google-auth-oauthlib | 18 KB     | : 100% 1.0/1 [00:00<00:00, 10.94it/s]\n",
            "wheel-0.36.2         | 33 KB     | : 100% 1.0/1 [00:00<00:00, 10.21it/s]\n",
            "ca-certificates-2021 | 118 KB    | : 100% 1.0/1 [00:00<00:00,  9.67it/s]\n",
            "cudnn-7.6.5          | 179.9 MB  | : 100% 1.0/1 [00:03<00:00,  3.61s/it]               \n",
            "gstreamer-1.14.0     | 3.2 MB    | : 100% 1.0/1 [00:00<00:00,  1.32it/s]\n",
            "gast-0.3.3           | 12 KB     | : 100% 1.0/1 [00:00<00:00, 24.03it/s]\n",
            "testpath-0.4.4       | 85 KB     | : 100% 1.0/1 [00:00<00:00, 40.53it/s]\n",
            "jupyterlab_pygments- | 8 KB      | : 100% 1.0/1 [00:00<00:00, 10.65it/s]\n",
            "toml-0.10.1          | 20 KB     | : 100% 1.0/1 [00:00<00:00,  9.86it/s]\n",
            "olefile-0.46         | 50 KB     | : 100% 1.0/1 [00:00<00:00, 10.87it/s]\n",
            "pylint-2.6.0         | 436 KB    | : 100% 1.0/1 [00:00<00:00,  8.48it/s]\n",
            "libuuid-1.0.3        | 15 KB     | : 100% 1.0/1 [00:00<00:00, 10.47it/s]\n",
            "tensorflow-base-2.2. | 181.7 MB  | : 100% 1.0/1 [00:09<00:00,  9.12s/it]               \n",
            "libedit-3.1.20191231 | 116 KB    | : 100% 1.0/1 [00:00<00:00, 12.30it/s]\n",
            "astroid-2.5          | 278 KB    | : 100% 1.0/1 [00:00<00:00, 11.63it/s]\n",
            "scikit-learn-0.23.2  | 5.0 MB    | : 100% 1.0/1 [00:00<00:00,  3.12it/s]\n",
            "tk-8.6.10            | 3.0 MB    | : 100% 1.0/1 [00:00<00:00,  6.26it/s]\n",
            "scipy-1.6.0          | 15.4 MB   | : 100% 1.0/1 [00:01<00:00,  1.17s/it]\n",
            "numpy-base-1.19.2    | 4.1 MB    | : 100% 1.0/1 [00:00<00:00,  3.87it/s]\n",
            "python-3.7.9         | 45.3 MB   | : 100% 1.0/1 [00:01<00:00,  1.10s/it]               \n",
            "pyopenssl-20.0.1     | 49 KB     | : 100% 1.0/1 [00:00<00:00, 14.46it/s]\n",
            "jinja2-2.11.3        | 101 KB    | : 100% 1.0/1 [00:00<00:00, 14.22it/s]\n",
            "_libgcc_mutex-0.1    | 2 KB      | : 100% 1.0/1 [00:00<00:00, 18.84it/s]\n",
            "cryptography-3.3.1   | 572 KB    | : 100% 1.0/1 [00:00<00:00,  3.77it/s]\n",
            "pillow-8.1.0         | 623 KB    | : 100% 1.0/1 [00:00<00:00, 10.13it/s]\n",
            "defusedxml-0.6.0     | 23 KB     | : 100% 1.0/1 [00:00<00:00, 46.79it/s]\n",
            "tensorboard-2.3.0    | 5.2 MB    | : 100% 1.0/1 [00:00<00:00,  4.34it/s]               \n",
            "qtpy-1.9.0           | 34 KB     | : 100% 1.0/1 [00:00<00:00, 23.06it/s]\n",
            "importlib-metadata-2 | 28 KB     | : 100% 1.0/1 [00:00<00:00, 18.60it/s]\n",
            "lcms2-2.11           | 307 KB    | : 100% 1.0/1 [00:00<00:00, 12.81it/s]\n",
            "requests-oauthlib-1. | 23 KB     | : 100% 1.0/1 [00:00<00:00, 14.38it/s]\n",
            "keras-preprocessing- | 35 KB     | : 100% 1.0/1 [00:00<00:00, 14.38it/s]\n",
            "hdf5-1.10.6          | 3.7 MB    | : 100% 1.0/1 [00:00<00:00,  5.91it/s]\n",
            "numpy-1.19.2         | 22 KB     | : 100% 1.0/1 [00:00<00:00, 14.24it/s]\n",
            "blinker-1.4          | 23 KB     | : 100% 1.0/1 [00:00<00:00, 15.07it/s]\n",
            "tensorflow-2.2.0     | 4 KB      | : 100% 1.0/1 [00:00<00:00, 15.16it/s]\n",
            "cachetools-4.2.1     | 13 KB     | : 100% 1.0/1 [00:00<00:00, 11.63it/s]\n",
            "sip-4.19.8           | 274 KB    | : 100% 1.0/1 [00:00<00:00, 11.86it/s]\n",
            "mkl_fft-1.2.1        | 148 KB    | : 100% 1.0/1 [00:00<00:00, 11.20it/s]\n",
            "aiohttp-3.6.3        | 544 KB    | : 100% 1.0/1 [00:00<00:00,  3.78it/s]                 \n",
            "joblib-1.0.1         | 208 KB    | : 100% 1.0/1 [00:00<00:00, 12.70it/s]\n",
            "ipywidgets-7.6.3     | 105 KB    | : 100% 1.0/1 [00:00<00:00, 12.61it/s]\n",
            "cycler-0.10.0        | 13 KB     | : 100% 1.0/1 [00:00<00:00, 14.53it/s]\n",
            "pickleshare-0.7.5    | 13 KB     | : 100% 1.0/1 [00:00<00:00, 15.40it/s]\n",
            "setuptools-52.0.0    | 710 KB    | : 100% 1.0/1 [00:00<00:00,  9.31it/s]\n",
            "zeromq-4.3.3         | 500 KB    | : 100% 1.0/1 [00:00<00:00, 10.51it/s]\n",
            "zstd-1.4.5           | 619 KB    | : 100% 1.0/1 [00:00<00:00, 12.19it/s]\n",
            "google-auth-1.27.0   | 72 KB     | : 100% 1.0/1 [00:00<00:00, 12.11it/s]\n",
            "c-ares-1.17.1        | 108 KB    | : 100% 1.0/1 [00:00<00:00, 12.84it/s]\n",
            "libgfortran-ng-7.3.0 | 1006 KB   | : 100% 1.0/1 [00:00<00:00,  9.61it/s]\n",
            "mistune-0.8.4        | 54 KB     | : 100% 1.0/1 [00:00<00:00, 13.26it/s]\n",
            "jupyterlab_widgets-1 | 109 KB    | : 100% 1.0/1 [00:00<00:00, 13.81it/s]\n",
            "python-dateutil-2.8. | 221 KB    | : 100% 1.0/1 [00:00<00:00, 13.58it/s]\n",
            "libpng-1.6.37        | 278 KB    | : 100% 1.0/1 [00:00<00:00, 13.35it/s]\n",
            "notebook-6.2.0       | 4.1 MB    | : 100% 1.0/1 [00:00<00:00,  2.10it/s]\n",
            "pytz-2021.1          | 181 KB    | : 100% 1.0/1 [00:00<00:00,  8.56it/s]\n",
            "jsonschema-3.2.0     | 45 KB     | : 100% 1.0/1 [00:00<00:00, 27.59it/s]\n",
            "ipython-7.20.0       | 991 KB    | : 100% 1.0/1 [00:00<00:00,  5.64it/s]\n",
            "pyzmq-20.0.0         | 439 KB    | : 100% 1.0/1 [00:00<00:00,  9.55it/s]\n",
            "pcre-8.44            | 212 KB    | : 100% 1.0/1 [00:00<00:00, 13.07it/s]\n",
            "termcolor-1.1.0      | 9 KB      | : 100% 1.0/1 [00:00<00:00, 12.69it/s]\n",
            "pyasn1-modules-0.2.8 | 72 KB     | : 100% 1.0/1 [00:00<00:00, 13.53it/s]\n",
            "requests-2.25.1      | 52 KB     | : 100% 1.0/1 [00:00<00:00, 15.04it/s]\n",
            "ipykernel-5.3.4      | 179 KB    | : 100% 1.0/1 [00:00<00:00, 11.74it/s]\n",
            "ptyprocess-0.7.0     | 17 KB     | : 100% 1.0/1 [00:00<00:00, 15.15it/s]\n",
            "six-1.15.0           | 27 KB     | : 100% 1.0/1 [00:00<00:00, 15.39it/s]\n",
            "pyrsistent-0.17.3    | 89 KB     | : 100% 1.0/1 [00:00<00:00,  9.75it/s]\n",
            "zipp-3.4.0           | 15 KB     | : 100% 1.0/1 [00:00<00:00, 13.54it/s]\n",
            "typed-ast-1.4.2      | 185 KB    | : 100% 1.0/1 [00:00<00:00, 13.26it/s]\n",
            "qtconsole-5.0.2      | 97 KB     | : 100% 1.0/1 [00:00<00:00, 11.55it/s]\n",
            "kiwisolver-1.3.1     | 80 KB     | : 100% 1.0/1 [00:00<00:00, 14.35it/s]\n",
            "isort-5.7.0          | 82 KB     | : 100% 1.0/1 [00:00<00:00, 10.63it/s]\n",
            "pexpect-4.8.0        | 53 KB     | : 100% 1.0/1 [00:00<00:00, 12.81it/s]\n",
            "h5py-2.10.0          | 902 KB    | : 100% 1.0/1 [00:00<00:00,  9.51it/s]\n",
            "werkzeug-1.0.1       | 239 KB    | : 100% 1.0/1 [00:00<00:00, 16.17it/s]\n",
            "pyjwt-1.7.1          | 33 KB     | : 100% 1.0/1 [00:00<00:00, 14.44it/s]\n",
            "widgetsnbextension-3 | 1.8 MB    | : 100% 1.0/1 [00:00<00:00,  2.81it/s]\n",
            "mkl-2020.2           | 138.3 MB  | : 100% 1.0/1 [00:04<00:00,  4.70s/it]               \n",
            "wrapt-1.12.1         | 49 KB     | : 100% 1.0/1 [00:00<00:00, 11.30it/s]\n",
            "nbformat-5.1.2       | 68 KB     | : 100% 1.0/1 [00:00<00:00, 10.57it/s]\n",
            "tensorboard-plugin-w | 630 KB    | : 100% 1.0/1 [00:00<00:00, 11.90it/s]\n",
            "fontconfig-2.13.1    | 250 KB    | : 100% 1.0/1 [00:00<00:00, 12.48it/s]\n",
            "zlib-1.2.11          | 103 KB    | : 100% 1.0/1 [00:00<00:00, 13.14it/s]\n",
            "libprotobuf-3.14.0   | 2.0 MB    | : 100% 1.0/1 [00:00<00:00,  7.75it/s]\n",
            "ncurses-6.2          | 817 KB    | : 100% 1.0/1 [00:00<00:00,  3.91it/s]\n",
            "nest-asyncio-1.5.1   | 10 KB     | : 100% 1.0/1 [00:00<00:00, 14.27it/s]\n",
            "webencodings-0.5.1   | 19 KB     | : 100% 1.0/1 [00:00<00:00, 11.68it/s]\n",
            "tqdm-4.56.0          | 80 KB     | : 100% 1.0/1 [00:00<00:00, 11.53it/s]\n",
            "wcwidth-0.2.5        | 29 KB     | : 100% 1.0/1 [00:00<00:00,  9.20it/s]\n",
            "terminado-0.9.2      | 25 KB     | : 100% 1.0/1 [00:00<00:00, 14.22it/s]\n",
            "pip-21.0.1           | 1.8 MB    | : 100% 1.0/1 [00:00<00:00,  3.36it/s]               \n",
            "openssl-1.1.1j       | 2.5 MB    | : 100% 1.0/1 [00:00<00:00,  6.87it/s]\n",
            "markdown-3.3.3       | 127 KB    | : 100% 1.0/1 [00:00<00:00, 10.96it/s]\n",
            "seaborn-0.11.1       | 212 KB    | : 100% 1.0/1 [00:00<00:00, 12.95it/s]\n",
            "Preparing transaction: | \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\bdone\n",
            "Verifying transaction: \\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\bdone\n",
            "Executing transaction: / \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ Enabling notebook extension jupyter-js-widgets/extension...\n",
            "Paths used for configuration of notebook: \n",
            "    \t/usr/local/envs/eval-metric/etc/jupyter/nbconfig/notebook.d/widgetsnbextension.json\n",
            "    \t/usr/local/envs/eval-metric/etc/jupyter/nbconfig/notebook.json\n",
            "Paths used for configuration of notebook: \n",
            "    \t/usr/local/envs/eval-metric/etc/jupyter/nbconfig/notebook.d/widgetsnbextension.json\n",
            "      - Validating: \u001b[32mOK\u001b[0m\n",
            "Paths used for configuration of notebook: \n",
            "    \t/usr/local/envs/eval-metric/etc/jupyter/nbconfig/notebook.d/widgetsnbextension.json\n",
            "    \t/usr/local/envs/eval-metric/etc/jupyter/nbconfig/notebook.json\n",
            "\n",
            "\b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\bdone\n",
            "Installing pip dependencies: | \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| Ran pip subprocess with arguments:\n",
            "['/usr/local/envs/eval-metric/bin/python', '-m', 'pip', 'install', '-U', '-r', '/content/evaluation-local-explanation-time-series/src/condaenv.dmzk5ity.requirements.txt']\n",
            "Pip subprocess output:\n",
            "Collecting cloudpickle==1.6.0\n",
            "  Downloading cloudpickle-1.6.0-py3-none-any.whl (23 kB)\n",
            "Collecting configparser==5.0.1\n",
            "  Downloading configparser-5.0.1-py3-none-any.whl (22 kB)\n",
            "Collecting docker-pycreds==0.4.0\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Collecting gitdb==4.0.5\n",
            "  Downloading gitdb-4.0.5-py3-none-any.whl (63 kB)\n",
            "Collecting gitpython==3.1.13\n",
            "  Downloading GitPython-3.1.13-py3-none-any.whl (159 kB)\n",
            "Collecting gql==0.2.0\n",
            "  Downloading gql-0.2.0.tar.gz (18 kB)\n",
            "Collecting graphql-core==1.1\n",
            "  Downloading graphql-core-1.1.tar.gz (70 kB)\n",
            "Collecting llvmlite==0.35.0\n",
            "  Downloading llvmlite-0.35.0-cp37-cp37m-manylinux2010_x86_64.whl (25.3 MB)\n",
            "Collecting numba==0.52.0\n",
            "  Downloading numba-0.52.0-cp37-cp37m-manylinux2014_x86_64.whl (3.2 MB)\n",
            "Collecting nvidia-ml-py3==7.352.0\n",
            "  Downloading nvidia-ml-py3-7.352.0.tar.gz (19 kB)\n",
            "Collecting pathtools==0.1.2\n",
            "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
            "Collecting promise==2.3\n",
            "  Downloading promise-2.3.tar.gz (19 kB)\n",
            "Collecting psutil==5.8.0\n",
            "  Downloading psutil-5.8.0-cp37-cp37m-manylinux2010_x86_64.whl (296 kB)\n",
            "Collecting pyyaml==5.4.1\n",
            "  Downloading PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636 kB)\n",
            "Collecting sentry-sdk==0.20.3\n",
            "  Downloading sentry_sdk-0.20.3-py2.py3-none-any.whl (131 kB)\n",
            "Collecting shap==0.39.0\n",
            "  Downloading shap-0.39.0.tar.gz (356 kB)\n",
            "Collecting shortuuid==1.0.1\n",
            "  Downloading shortuuid-1.0.1-py3-none-any.whl (7.5 kB)\n",
            "Collecting slicer==0.0.7\n",
            "  Downloading slicer-0.0.7-py3-none-any.whl (14 kB)\n",
            "Collecting smmap==3.0.5\n",
            "  Downloading smmap-3.0.5-py2.py3-none-any.whl (25 kB)\n",
            "Collecting subprocess32==3.5.4\n",
            "  Downloading subprocess32-3.5.4.tar.gz (97 kB)\n",
            "Collecting wandb==0.9.7\n",
            "  Downloading wandb-0.9.7-py2.py3-none-any.whl (1.4 MB)\n",
            "Collecting watchdog==2.0.2\n",
            "  Downloading watchdog-2.0.2-py3-none-manylinux2014_x86_64.whl (74 kB)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/envs/eval-metric/lib/python3.7/site-packages (from docker-pycreds==0.4.0->-r /content/evaluation-local-explanation-time-series/src/condaenv.dmzk5ity.requirements.txt (line 3)) (1.15.0)\n",
            "Requirement already satisfied: requests<3,>=2.12 in /usr/local/envs/eval-metric/lib/python3.7/site-packages (from gql==0.2.0->-r /content/evaluation-local-explanation-time-series/src/condaenv.dmzk5ity.requirements.txt (line 6)) (2.25.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/envs/eval-metric/lib/python3.7/site-packages (from numba==0.52.0->-r /content/evaluation-local-explanation-time-series/src/condaenv.dmzk5ity.requirements.txt (line 9)) (52.0.0.post20210125)\n",
            "Requirement already satisfied: numpy>=1.15 in /usr/local/envs/eval-metric/lib/python3.7/site-packages (from numba==0.52.0->-r /content/evaluation-local-explanation-time-series/src/condaenv.dmzk5ity.requirements.txt (line 9)) (1.19.2)\n",
            "Requirement already satisfied: certifi in /usr/local/envs/eval-metric/lib/python3.7/site-packages (from sentry-sdk==0.20.3->-r /content/evaluation-local-explanation-time-series/src/condaenv.dmzk5ity.requirements.txt (line 15)) (2020.12.5)\n",
            "Requirement already satisfied: urllib3>=1.10.0 in /usr/local/envs/eval-metric/lib/python3.7/site-packages (from sentry-sdk==0.20.3->-r /content/evaluation-local-explanation-time-series/src/condaenv.dmzk5ity.requirements.txt (line 15)) (1.26.3)\n",
            "Requirement already satisfied: scipy in /usr/local/envs/eval-metric/lib/python3.7/site-packages (from shap==0.39.0->-r /content/evaluation-local-explanation-time-series/src/condaenv.dmzk5ity.requirements.txt (line 16)) (1.6.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/envs/eval-metric/lib/python3.7/site-packages (from shap==0.39.0->-r /content/evaluation-local-explanation-time-series/src/condaenv.dmzk5ity.requirements.txt (line 16)) (0.23.2)\n",
            "Requirement already satisfied: pandas in /usr/local/envs/eval-metric/lib/python3.7/site-packages (from shap==0.39.0->-r /content/evaluation-local-explanation-time-series/src/condaenv.dmzk5ity.requirements.txt (line 16)) (1.2.2)\n",
            "Requirement already satisfied: tqdm>4.25.0 in /usr/local/envs/eval-metric/lib/python3.7/site-packages (from shap==0.39.0->-r /content/evaluation-local-explanation-time-series/src/condaenv.dmzk5ity.requirements.txt (line 16)) (4.56.0)\n",
            "Requirement already satisfied: Click>=7.0 in /usr/local/envs/eval-metric/lib/python3.7/site-packages (from wandb==0.9.7->-r /content/evaluation-local-explanation-time-series/src/condaenv.dmzk5ity.requirements.txt (line 21)) (7.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/envs/eval-metric/lib/python3.7/site-packages (from wandb==0.9.7->-r /content/evaluation-local-explanation-time-series/src/condaenv.dmzk5ity.requirements.txt (line 21)) (2.8.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/envs/eval-metric/lib/python3.7/site-packages (from requests<3,>=2.12->gql==0.2.0->-r /content/evaluation-local-explanation-time-series/src/condaenv.dmzk5ity.requirements.txt (line 6)) (2.10)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/envs/eval-metric/lib/python3.7/site-packages (from requests<3,>=2.12->gql==0.2.0->-r /content/evaluation-local-explanation-time-series/src/condaenv.dmzk5ity.requirements.txt (line 6)) (3.0.4)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/envs/eval-metric/lib/python3.7/site-packages (from pandas->shap==0.39.0->-r /content/evaluation-local-explanation-time-series/src/condaenv.dmzk5ity.requirements.txt (line 16)) (2021.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/envs/eval-metric/lib/python3.7/site-packages (from scikit-learn->shap==0.39.0->-r /content/evaluation-local-explanation-time-series/src/condaenv.dmzk5ity.requirements.txt (line 16)) (2.1.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/envs/eval-metric/lib/python3.7/site-packages (from scikit-learn->shap==0.39.0->-r /content/evaluation-local-explanation-time-series/src/condaenv.dmzk5ity.requirements.txt (line 16)) (1.0.1)\n",
            "Building wheels for collected packages: gql, graphql-core, nvidia-ml-py3, pathtools, promise, shap, subprocess32\n",
            "  Building wheel for gql (setup.py): started\n",
            "  Building wheel for gql (setup.py): finished with status 'done'\n",
            "  Created wheel for gql: filename=gql-0.2.0-py3-none-any.whl size=7630 sha256=afd744594e1a4e9db132853a272c3b7a78832bafbb46224efd3be501184a097f\n",
            "  Stored in directory: /root/.cache/pip/wheels/b6/9a/56/5456fd32264a8fc53eefcb2f74e24e99a7ef4eb40a9af5c905\n",
            "  Building wheel for graphql-core (setup.py): started\n",
            "  Building wheel for graphql-core (setup.py): finished with status 'done'\n",
            "  Created wheel for graphql-core: filename=graphql_core-1.1-py3-none-any.whl size=104649 sha256=1360218af197d4eb5552a34ac4c30e6dfa64b33998a4286dc3ba60d810b5a118\n",
            "  Stored in directory: /root/.cache/pip/wheels/6b/fd/8c/a20dd591c1a554070cc33fb58042867e6ac1c85395abe2e57a\n",
            "  Building wheel for nvidia-ml-py3 (setup.py): started\n",
            "  Building wheel for nvidia-ml-py3 (setup.py): finished with status 'done'\n",
            "  Created wheel for nvidia-ml-py3: filename=nvidia_ml_py3-7.352.0-py3-none-any.whl size=19191 sha256=109e8958fef74de40165502f673ad6ff4a0f9cac2236c5a870b760024a72c358\n",
            "  Stored in directory: /root/.cache/pip/wheels/df/99/da/c34f202dc8fd1dffd35e0ecf1a7d7f8374ca05fbcbaf974b83\n",
            "  Building wheel for pathtools (setup.py): started\n",
            "  Building wheel for pathtools (setup.py): finished with status 'done'\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8784 sha256=dc8dbece0bc3fe8f958b2c8b91dfca872a11f35c3e6ddf5d03739894dfceb1fc\n",
            "  Stored in directory: /root/.cache/pip/wheels/3e/31/09/fa59cef12cdcfecc627b3d24273699f390e71828921b2cbba2\n",
            "  Building wheel for promise (setup.py): started\n",
            "  Building wheel for promise (setup.py): finished with status 'done'\n",
            "  Created wheel for promise: filename=promise-2.3-py3-none-any.whl size=21494 sha256=351761f4c0f3c1aa870cf3a336da0534975215ebf6d8c865ad2c1e37e677a3d0\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/93/c6/762e359f8cb6a5b69c72235d798804cae523bbe41c2aa8333d\n",
            "  Building wheel for shap (setup.py): started\n",
            "  Building wheel for shap (setup.py): finished with status 'done'\n",
            "  Created wheel for shap: filename=shap-0.39.0-cp37-cp37m-linux_x86_64.whl size=494321 sha256=ca304121caff9c575d6a9af85bc5b91c5df4af49ec55a9c242e3381092826450\n",
            "  Stored in directory: /root/.cache/pip/wheels/ca/25/8f/6ae5df62c32651cd719e972e738a8aaa4a87414c4d2b14c9c0\n",
            "  Building wheel for subprocess32 (setup.py): started\n",
            "  Building wheel for subprocess32 (setup.py): finished with status 'done'\n",
            "  Created wheel for subprocess32: filename=subprocess32-3.5.4-py3-none-any.whl size=6488 sha256=aa2de11bbc548440ac5d019ec586c774a9022b158ba6bff497fc485dfab2bd24\n",
            "  Stored in directory: /root/.cache/pip/wheels/50/ca/fa/8fca8d246e64f19488d07567547ddec8eb084e8c0d7a59226a\n",
            "Successfully built gql graphql-core nvidia-ml-py3 pathtools promise shap subprocess32\n",
            "Installing collected packages: smmap, promise, llvmlite, graphql-core, gitdb, watchdog, subprocess32, slicer, shortuuid, sentry-sdk, pyyaml, psutil, nvidia-ml-py3, numba, gql, gitpython, docker-pycreds, configparser, cloudpickle, wandb, shap, pathtools\n",
            "Successfully installed cloudpickle-1.6.0 configparser-5.0.1 docker-pycreds-0.4.0 gitdb-4.0.5 gitpython-3.1.13 gql-0.2.0 graphql-core-1.1 llvmlite-0.35.0 numba-0.52.0 nvidia-ml-py3-7.352.0 pathtools-0.1.2 promise-2.3 psutil-5.8.0 pyyaml-5.4.1 sentry-sdk-0.20.3 shap-0.39.0 shortuuid-1.0.1 slicer-0.0.7 smmap-3.0.5 subprocess32-3.5.4 wandb-0.9.7 watchdog-2.0.2\n",
            "\n",
            "\b\bdone\n",
            "#\n",
            "# To activate this environment, use\n",
            "#\n",
            "#     $ conda activate eval-metric\n",
            "#\n",
            "# To deactivate an active environment, use\n",
            "#\n",
            "#     $ conda deactivate\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!conda info --envs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yM-SQWHfD_XB",
        "outputId": "828d3344-618c-44fc-b73a-8021f6d26155"
      },
      "execution_count": 134,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# conda environments:\n",
            "#\n",
            "base                  *  /usr/local\n",
            "eval-metric              /usr/local/envs/eval-metric\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!conda list -n eval-metric"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X0L7C_kOA23a",
        "outputId": "2cd8ab71-e463-4942-c37b-a295903e5cb6"
      },
      "execution_count": 135,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# packages in environment at /usr/local/envs/eval-metric:\n",
            "#\n",
            "# Name                    Version                   Build  Channel\n",
            "_libgcc_mutex             0.1                        main    conda-forge\n",
            "_tflow_select             2.1.0                       gpu  \n",
            "absl-py                   0.11.0             pyhd3eb1b0_1  \n",
            "aiohttp                   3.6.3            py37h7b6447c_0  \n",
            "argon2-cffi               20.1.0           py37h27cfd23_1  \n",
            "astroid                   2.5              py37h06a4308_1  \n",
            "astunparse                1.6.3                      py_0  \n",
            "async-timeout             3.0.1            py37h06a4308_0  \n",
            "async_generator           1.10             py37h28b3542_0  \n",
            "attrs                     20.3.0             pyhd3eb1b0_0  \n",
            "autopep8                  1.5.5              pyhd3eb1b0_0  \n",
            "backcall                  0.2.0              pyhd3eb1b0_0  \n",
            "blas                      1.0                         mkl    conda-forge\n",
            "bleach                    3.3.0              pyhd3eb1b0_0  \n",
            "blinker                   1.4              py37h06a4308_0  \n",
            "brotlipy                  0.7.0           py37h27cfd23_1003  \n",
            "c-ares                    1.17.1               h27cfd23_0  \n",
            "ca-certificates           2021.1.19            h06a4308_1  \n",
            "cachetools                4.2.1              pyhd3eb1b0_0  \n",
            "certifi                   2020.12.5        py37h06a4308_0  \n",
            "cffi                      1.14.5           py37h261ae71_0  \n",
            "chardet                   3.0.4           py37h06a4308_1003  \n",
            "click                     7.1.2              pyhd3eb1b0_0  \n",
            "cloudpickle               1.6.0                    pypi_0    pypi\n",
            "configparser              5.0.1                    pypi_0    pypi\n",
            "coverage                  5.4              py37h27cfd23_2  \n",
            "cryptography              3.3.1            py37h3c74f83_1  \n",
            "cudatoolkit               10.1.243             h6bb024c_0  \n",
            "cudnn                     7.6.5                cuda10.1_0  \n",
            "cupti                     10.1.168                      0  \n",
            "cycler                    0.10.0                   py37_0  \n",
            "cython                    0.29.21          py37h2531618_0  \n",
            "dbus                      1.13.18              hb2f20db_0  \n",
            "decorator                 4.4.2              pyhd3eb1b0_0  \n",
            "defusedxml                0.6.0              pyhd3eb1b0_0  \n",
            "deprecated                1.2.10             pyh9f0ad1d_0    conda-forge\n",
            "docker-pycreds            0.4.0                    pypi_0    pypi\n",
            "entrypoints               0.3                      py37_0  \n",
            "expat                     2.2.10               he6710b0_2  \n",
            "fontconfig                2.13.1               h6c09931_0  \n",
            "freetype                  2.10.4               h5ab3b9f_0  \n",
            "gast                      0.3.3                      py_0    conda-forge\n",
            "gitdb                     4.0.5                    pypi_0    pypi\n",
            "gitpython                 3.1.13                   pypi_0    pypi\n",
            "glib                      2.67.4               h36276a3_1  \n",
            "google-auth               1.27.0             pyhd3eb1b0_0  \n",
            "google-auth-oauthlib      0.4.2              pyhd3eb1b0_2  \n",
            "google-pasta              0.2.0                      py_0  \n",
            "gql                       0.2.0                    pypi_0    pypi\n",
            "graphql-core              1.1                      pypi_0    pypi\n",
            "grpcio                    1.35.0           py37h2157cd5_1  \n",
            "gst-plugins-base          1.14.0               h8213a91_2  \n",
            "gstreamer                 1.14.0               h28cd5cc_2  \n",
            "h5py                      2.10.0           py37hd6299e0_1  \n",
            "hdf5                      1.10.6               hb1b8bf9_0  \n",
            "icu                       58.2                 he6710b0_3  \n",
            "idna                      2.10               pyhd3eb1b0_0  \n",
            "importlib-metadata        2.0.0                      py_1    conda-forge\n",
            "importlib_metadata        2.0.0                         1    conda-forge\n",
            "intel-openmp              2020.2                      254  \n",
            "ipykernel                 5.3.4            py37h5ca1d4c_0  \n",
            "ipython                   7.20.0           py37hb070fc8_1  \n",
            "ipython_genutils          0.2.0              pyhd3eb1b0_1  \n",
            "ipywidgets                7.6.3              pyhd3eb1b0_1  \n",
            "isort                     5.7.0              pyhd3eb1b0_0  \n",
            "jedi                      0.17.2           py37h06a4308_1  \n",
            "jinja2                    2.11.3             pyhd3eb1b0_0  \n",
            "joblib                    1.0.1              pyhd3eb1b0_0  \n",
            "jpeg                      9b                   h024ee3a_2  \n",
            "jsonschema                3.2.0                      py_2    conda-forge\n",
            "jupyter                   1.0.0                    py37_7  \n",
            "jupyter_client            6.1.7                      py_0    conda-forge\n",
            "jupyter_console           6.2.0                      py_0    conda-forge\n",
            "jupyter_core              4.7.1            py37h06a4308_0  \n",
            "jupyterlab_pygments       0.1.2                      py_0  \n",
            "jupyterlab_widgets        1.0.0              pyhd3eb1b0_1  \n",
            "keras-preprocessing       1.1.2              pyhd3eb1b0_0  \n",
            "kiwisolver                1.3.1            py37h2531618_0  \n",
            "lazy-object-proxy         1.5.2            py37h27cfd23_0  \n",
            "lcms2                     2.11                 h396b838_0  \n",
            "ld_impl_linux-64          2.33.1               h53a641e_7    conda-forge\n",
            "libedit                   3.1.20191231         h14c3975_1  \n",
            "libffi                    3.3                  he6710b0_2  \n",
            "libgcc-ng                 9.1.0                hdf63c60_0  \n",
            "libgfortran-ng            7.3.0                hdf63c60_0  \n",
            "libpng                    1.6.37               hbc83047_0  \n",
            "libprotobuf               3.14.0               h8c45485_0  \n",
            "libsodium                 1.0.18               h7b6447c_0  \n",
            "libstdcxx-ng              9.1.0                hdf63c60_0  \n",
            "libtiff                   4.1.0                h2733197_1  \n",
            "libuuid                   1.0.3                h1bed415_2  \n",
            "libxcb                    1.14                 h7b6447c_0  \n",
            "libxml2                   2.9.10               hb55368b_3  \n",
            "llvmlite                  0.35.0                   pypi_0    pypi\n",
            "lz4-c                     1.9.3                h2531618_0  \n",
            "markdown                  3.3.3            py37h06a4308_0  \n",
            "markupsafe                1.1.1            py37h14c3975_1  \n",
            "matplotlib                3.3.4            py37h06a4308_0  \n",
            "matplotlib-base           3.3.4            py37h62a2d02_0  \n",
            "mccabe                    0.6.1                    py37_1  \n",
            "mistune                   0.8.4           py37h14c3975_1001  \n",
            "mkl                       2020.2                      256  \n",
            "mkl-service               2.3.0            py37he8ac12f_0  \n",
            "mkl_fft                   1.2.1            py37h54f3939_0  \n",
            "mkl_random                1.1.1            py37h0573a6f_0  \n",
            "multidict                 4.7.6            py37h7b6447c_1  \n",
            "nbclient                  0.5.2              pyhd3eb1b0_0  \n",
            "nbconvert                 6.0.7                    py37_0  \n",
            "nbformat                  5.1.2              pyhd3eb1b0_1  \n",
            "ncurses                   6.2                  he6710b0_1  \n",
            "nest-asyncio              1.5.1              pyhd3eb1b0_0  \n",
            "notebook                  6.2.0            py37h06a4308_0  \n",
            "numba                     0.52.0                   pypi_0    pypi\n",
            "numpy                     1.19.2           py37h54aff64_0  \n",
            "numpy-base                1.19.2           py37hfa32c7d_0  \n",
            "nvidia-ml-py3             7.352.0                  pypi_0    pypi\n",
            "oauthlib                  3.1.0                      py_0  \n",
            "olefile                   0.46                     py37_0  \n",
            "openssl                   1.1.1j               h27cfd23_0  \n",
            "opt_einsum                3.1.0                      py_0    conda-forge\n",
            "packaging                 20.9               pyhd3eb1b0_0  \n",
            "pandas                    1.2.2            py37ha9443f7_0  \n",
            "pandoc                    2.11                 hb0f4dca_0  \n",
            "pandocfilters             1.4.3            py37h06a4308_1  \n",
            "parso                     0.7.0                      py_0  \n",
            "pathtools                 0.1.2                    pypi_0    pypi\n",
            "pcre                      8.44                 he6710b0_0  \n",
            "pexpect                   4.8.0              pyhd3eb1b0_3  \n",
            "pickleshare               0.7.5           pyhd3eb1b0_1003  \n",
            "pillow                    8.1.0            py37he98fc37_0  \n",
            "pip                       21.0.1           py37h06a4308_0  \n",
            "prometheus_client         0.9.0              pyhd3eb1b0_0  \n",
            "promise                   2.3                      pypi_0    pypi\n",
            "prompt-toolkit            3.0.8                      py_0    conda-forge\n",
            "prompt_toolkit            3.0.8                         0    conda-forge\n",
            "protobuf                  3.14.0           py37h2531618_1  \n",
            "psutil                    5.8.0                    pypi_0    pypi\n",
            "ptyprocess                0.7.0              pyhd3eb1b0_2  \n",
            "pyasn1                    0.4.8                      py_0    conda-forge\n",
            "pyasn1-modules            0.2.8                      py_0  \n",
            "pycodestyle               2.6.0              pyhd3eb1b0_0  \n",
            "pycparser                 2.20                       py_2  \n",
            "pygments                  2.7.4              pyhd3eb1b0_0  \n",
            "pyjwt                     1.7.1                    py37_0  \n",
            "pylint                    2.6.0                    py37_0  \n",
            "pyopenssl                 20.0.1             pyhd3eb1b0_1  \n",
            "pyparsing                 2.4.7              pyhd3eb1b0_0  \n",
            "pyqt                      5.9.2            py37h05f1152_2  \n",
            "pyrsistent                0.17.3           py37h7b6447c_0  \n",
            "pysocks                   1.7.1                    py37_1  \n",
            "python                    3.7.9                h7579374_0  \n",
            "python-dateutil           2.8.1              pyhd3eb1b0_0  \n",
            "python_abi                3.7                     1_cp37m    conda-forge\n",
            "pytz                      2021.1             pyhd3eb1b0_0  \n",
            "pyyaml                    5.4.1                    pypi_0    pypi\n",
            "pyzmq                     20.0.0           py37h2531618_1  \n",
            "qt                        5.9.7                h5867ecd_1  \n",
            "qtconsole                 5.0.2              pyhd3eb1b0_0  \n",
            "qtpy                      1.9.0                      py_0    conda-forge\n",
            "readline                  8.1                  h27cfd23_0  \n",
            "requests                  2.25.1             pyhd3eb1b0_0  \n",
            "requests-oauthlib         1.3.0                      py_0  \n",
            "rsa                       4.7                pyhd3eb1b0_1  \n",
            "scikit-learn              0.23.2           py37h0573a6f_0  \n",
            "scipy                     1.6.0            py37h91f5cce_0  \n",
            "seaborn                   0.11.1             pyhd3eb1b0_0  \n",
            "send2trash                1.5.0              pyhd3eb1b0_1  \n",
            "sentry-sdk                0.20.3                   pypi_0    pypi\n",
            "setuptools                52.0.0           py37h06a4308_0  \n",
            "shap                      0.39.0                   pypi_0    pypi\n",
            "shortuuid                 1.0.1                    pypi_0    pypi\n",
            "sip                       4.19.8           py37hf484d3e_0  \n",
            "six                       1.15.0           py37h06a4308_0  \n",
            "slicer                    0.0.7                    pypi_0    pypi\n",
            "smmap                     3.0.5                    pypi_0    pypi\n",
            "sqlite                    3.33.0               h62c20be_0  \n",
            "subprocess32              3.5.4                    pypi_0    pypi\n",
            "tensorboard               2.3.0              pyh4dce500_0  \n",
            "tensorboard-plugin-wit    1.6.0                      py_0  \n",
            "tensorflow                2.2.0           gpu_py37h1a511ff_0  \n",
            "tensorflow-base           2.2.0           gpu_py37h8a81be8_0  \n",
            "tensorflow-estimator      2.2.0              pyh208ff02_0  \n",
            "tensorflow-gpu            2.2.0                h0d30ee6_0  \n",
            "termcolor                 1.1.0            py37h06a4308_1  \n",
            "terminado                 0.9.2            py37h06a4308_0  \n",
            "testpath                  0.4.4              pyhd3eb1b0_0  \n",
            "threadpoolctl             2.1.0              pyh5ca1d4c_0    conda-forge\n",
            "tk                        8.6.10               hbc83047_0  \n",
            "toml                      0.10.1                     py_0  \n",
            "tornado                   6.1              py37h27cfd23_0  \n",
            "tqdm                      4.56.0             pyhd3eb1b0_0  \n",
            "traitlets                 5.0.5              pyhd3eb1b0_0  \n",
            "typed-ast                 1.4.2            py37h27cfd23_1  \n",
            "urllib3                   1.26.3             pyhd3eb1b0_0  \n",
            "wandb                     0.9.7                    pypi_0    pypi\n",
            "watchdog                  2.0.2                    pypi_0    pypi\n",
            "wcwidth                   0.2.5                      py_0  \n",
            "webencodings              0.5.1                    py37_1  \n",
            "werkzeug                  1.0.1              pyhd3eb1b0_0  \n",
            "wheel                     0.36.2             pyhd3eb1b0_0  \n",
            "widgetsnbextension        3.5.1                    py37_0    conda-forge\n",
            "wrapt                     1.12.1           py37h7b6447c_1  \n",
            "xz                        5.2.5                h7b6447c_0  \n",
            "yarl                      1.6.3            py37h27cfd23_0  \n",
            "zeromq                    4.3.3                he6710b0_3  \n",
            "zipp                      3.4.0              pyhd3eb1b0_0  \n",
            "zlib                      1.2.11               h7b6447c_3  \n",
            "zstd                      1.4.5                h9ceee32_0  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!source activate eval-metric && python raw_data_process.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MhT75466xr_E",
        "outputId": "8b03030c-52b5-42cf-a357-fb8a2dc3cec2"
      },
      "execution_count": 142,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Store           False\n",
            "Dept            False\n",
            "Date            False\n",
            "Weekly_Sales    False\n",
            "year            False\n",
            "month           False\n",
            "weekofmonth     False\n",
            "day             False\n",
            "Type            False\n",
            "Size            False\n",
            "Temperature     False\n",
            "Fuel_Price      False\n",
            "MarkDown1        True\n",
            "MarkDown2        True\n",
            "MarkDown3        True\n",
            "MarkDown4        True\n",
            "MarkDown5        True\n",
            "CPI             False\n",
            "Unemployment    False\n",
            "IsHoliday       False\n",
            "dtype: bool\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!source activate eval-metric && python train_models.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AH2tG7IQFH6d",
        "outputId": "53333572-5c01-41cf-e73b-9410274ee466"
      },
      "execution_count": 143,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "Epoch 8/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4798 - nd: 0.3251 - val_loss: 0.2852 - val_nd: 0.3185\n",
            "Epoch 9/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.4667 - nd: 0.3005 - val_loss: 0.1525 - val_nd: 0.1712\n",
            "Epoch 10/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4370 - nd: 0.2331 - val_loss: 0.1560 - val_nd: 0.1718\n",
            "Epoch 11/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4091 - nd: 0.2139 - val_loss: 0.1530 - val_nd: 0.1501\n",
            "Epoch 12/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.4133 - nd: 0.2174 - val_loss: 0.1852 - val_nd: 0.1752\n",
            "Epoch 13/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4120 - nd: 0.2304 - val_loss: 0.1676 - val_nd: 0.1457\n",
            "Epoch 14/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4056 - nd: 0.1997 - val_loss: 0.1723 - val_nd: 0.1489\n",
            "Epoch 15/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4058 - nd: 0.1987 - val_loss: 0.1892 - val_nd: 0.1832\n",
            "Epoch 16/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.3991 - nd: 0.2014 - val_loss: 0.1706 - val_nd: 0.1467\n",
            "Epoch 17/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.3899 - nd: 0.1980 - val_loss: 0.1742 - val_nd: 0.1401\n",
            "Epoch 18/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.3935 - nd: 0.1920 - val_loss: 0.1765 - val_nd: 0.1483\n",
            "Epoch 19/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.3867 - nd: 0.1907 - val_loss: 0.1687 - val_nd: 0.1346\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 60500\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                   _step 19\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                    loss 0.3866862654685974\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                   epoch 18\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                _runtime 5270.726792812347\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              _timestamp 1650247646.9543753\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                  val_nd 0.13460199534893036\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                      nd 0.19070982933044434\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                val_loss 0.16869722306728363\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           best_val_loss 0.1524564027786255\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              best_epoch 8\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              best-nd_95 0.0011376126345852098\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       best-nrmse_maxmin 0.026972796152673402\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    best-nrmse_maxmin_95 0.00018787203913891933\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 best-nd 0.20540142375460252\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   best-nrmse_maxmin_std 0.0031949429473148057\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     best-nrmse_mean_std 0.03061774543633056\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      best-nrmse_mean_95 0.0018004134545795954\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         best-nrmse_mean 0.2584854314856151\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             best-nd_std 0.019346186267540706\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 7 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced fiery-eon-613: https://app.wandb.ai/matintavakoli/Interpreting_TS/runs/1jf1e9zf\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.9.7\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Wandb version 0.12.14 is available!  To upgrade, please run:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in wandb/run-20220418_020731-3icg6uro\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mgallant-cloud-614\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://app.wandb.ai/matintavakoli/Interpreting_TS\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://app.wandb.ai/matintavakoli/Interpreting_TS/runs/3icg6uro\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb off` to turn off syncing.\n",
            "\n",
            "/usr/local/envs/eval-metric/lib/python3.7/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  return f(**kwargs)\n",
            "/usr/local/envs/eval-metric/lib/python3.7/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  return f(**kwargs)\n",
            "  4%|█▍                                     | 101/2660 [00:00<00:10, 240.04it/s]\n",
            "(9090, 30, 10) (9090, 6) (1111, 30, 10) (1111, 6)\n",
            "(9090, 30, 10) (9090, 6) (1111, 30, 10) (1111, 6)\n",
            "Model: \"sequential_67\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm_63 (LSTM)               (None, 30, 128)           71168     \n",
            "_________________________________________________________________\n",
            "lstm_64 (LSTM)               (None, 30, 128)           131584    \n",
            "_________________________________________________________________\n",
            "lstm_65 (LSTM)               (None, 128)               131584    \n",
            "_________________________________________________________________\n",
            "dropout_99 (Dropout)         (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_133 (Dense)            (None, 6)                 774       \n",
            "=================================================================\n",
            "Total params: 335,110\n",
            "Trainable params: 335,110\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/200\n",
            "18/18 [==============================] - 1s 67ms/step - loss: 0.9005 - nd: 0.7678 - val_loss: 0.4148 - val_nd: 0.4562\n",
            "Epoch 2/200\n",
            "18/18 [==============================] - 0s 21ms/step - loss: 0.6007 - nd: 0.4691 - val_loss: 0.2939 - val_nd: 0.3329\n",
            "Epoch 3/200\n",
            "18/18 [==============================] - 0s 22ms/step - loss: 0.4718 - nd: 0.3366 - val_loss: 0.2647 - val_nd: 0.2504\n",
            "Epoch 4/200\n",
            "18/18 [==============================] - 0s 22ms/step - loss: 0.4329 - nd: 0.2801 - val_loss: 0.2530 - val_nd: 0.2348\n",
            "Epoch 5/200\n",
            "18/18 [==============================] - 0s 21ms/step - loss: 0.4335 - nd: 0.2666 - val_loss: 0.2362 - val_nd: 0.2143\n",
            "Epoch 6/200\n",
            "18/18 [==============================] - 0s 22ms/step - loss: 0.4186 - nd: 0.2533 - val_loss: 0.2260 - val_nd: 0.2038\n",
            "Epoch 7/200\n",
            "18/18 [==============================] - 0s 21ms/step - loss: 0.4036 - nd: 0.2518 - val_loss: 0.2065 - val_nd: 0.1937\n",
            "Epoch 8/200\n",
            "18/18 [==============================] - 0s 19ms/step - loss: 0.4213 - nd: 0.2387 - val_loss: 0.2405 - val_nd: 0.2203\n",
            "Epoch 9/200\n",
            "18/18 [==============================] - 0s 20ms/step - loss: 0.4090 - nd: 0.2205 - val_loss: 0.2148 - val_nd: 0.1764\n",
            "Epoch 10/200\n",
            "18/18 [==============================] - 0s 19ms/step - loss: 0.4246 - nd: 0.2656 - val_loss: 0.2159 - val_nd: 0.2044\n",
            "Epoch 11/200\n",
            "18/18 [==============================] - 0s 19ms/step - loss: 0.4000 - nd: 0.2243 - val_loss: 0.2078 - val_nd: 0.1810\n",
            "Epoch 12/200\n",
            "18/18 [==============================] - 0s 22ms/step - loss: 0.3987 - nd: 0.2284 - val_loss: 0.1979 - val_nd: 0.1900\n",
            "Epoch 13/200\n",
            "18/18 [==============================] - 0s 22ms/step - loss: 0.3870 - nd: 0.1866 - val_loss: 0.1770 - val_nd: 0.1631\n",
            "Epoch 14/200\n",
            "18/18 [==============================] - 0s 22ms/step - loss: 0.3756 - nd: 0.1805 - val_loss: 0.1682 - val_nd: 0.1655\n",
            "Epoch 15/200\n",
            "18/18 [==============================] - 0s 23ms/step - loss: 0.3701 - nd: 0.1760 - val_loss: 0.1407 - val_nd: 0.1554\n",
            "Epoch 16/200\n",
            "18/18 [==============================] - 0s 19ms/step - loss: 0.3579 - nd: 0.1747 - val_loss: 0.1511 - val_nd: 0.1519\n",
            "Epoch 17/200\n",
            "18/18 [==============================] - 0s 20ms/step - loss: 0.3610 - nd: 0.1771 - val_loss: 0.2520 - val_nd: 0.2634\n",
            "Epoch 18/200\n",
            "18/18 [==============================] - 0s 20ms/step - loss: 0.3740 - nd: 0.2128 - val_loss: 0.1440 - val_nd: 0.1486\n",
            "Epoch 19/200\n",
            "18/18 [==============================] - 0s 19ms/step - loss: 0.3822 - nd: 0.1904 - val_loss: 0.1518 - val_nd: 0.1753\n",
            "Epoch 20/200\n",
            "18/18 [==============================] - 0s 19ms/step - loss: 0.3755 - nd: 0.1954 - val_loss: 0.1518 - val_nd: 0.1684\n",
            "Epoch 21/200\n",
            "18/18 [==============================] - 0s 19ms/step - loss: 0.3673 - nd: 0.1784 - val_loss: 0.1898 - val_nd: 0.1508\n",
            "Epoch 22/200\n",
            "18/18 [==============================] - 0s 19ms/step - loss: 0.3699 - nd: 0.1784 - val_loss: 0.1583 - val_nd: 0.1544\n",
            "Epoch 23/200\n",
            "18/18 [==============================] - 0s 19ms/step - loss: 0.3597 - nd: 0.1727 - val_loss: 0.1420 - val_nd: 0.1375\n",
            "Epoch 24/200\n",
            "18/18 [==============================] - 0s 23ms/step - loss: 0.3555 - nd: 0.1811 - val_loss: 0.1356 - val_nd: 0.1405\n",
            "Epoch 25/200\n",
            "18/18 [==============================] - 0s 19ms/step - loss: 0.3632 - nd: 0.1716 - val_loss: 0.2022 - val_nd: 0.1469\n",
            "Epoch 26/200\n",
            "18/18 [==============================] - 0s 21ms/step - loss: 0.3491 - nd: 0.1595 - val_loss: 0.1325 - val_nd: 0.1373\n",
            "Epoch 27/200\n",
            "18/18 [==============================] - 0s 20ms/step - loss: 0.3493 - nd: 0.1643 - val_loss: 0.1995 - val_nd: 0.1364\n",
            "Epoch 28/200\n",
            "18/18 [==============================] - 0s 21ms/step - loss: 0.3485 - nd: 0.1660 - val_loss: 0.1268 - val_nd: 0.1321\n",
            "Epoch 29/200\n",
            "18/18 [==============================] - 0s 20ms/step - loss: 0.3636 - nd: 0.1913 - val_loss: 0.1312 - val_nd: 0.1394\n",
            "Epoch 30/200\n",
            "18/18 [==============================] - 0s 19ms/step - loss: 0.3720 - nd: 0.1776 - val_loss: 0.1869 - val_nd: 0.1336\n",
            "Epoch 31/200\n",
            "18/18 [==============================] - 0s 19ms/step - loss: 0.3422 - nd: 0.1617 - val_loss: 0.1472 - val_nd: 0.1531\n",
            "Epoch 32/200\n",
            "18/18 [==============================] - 0s 22ms/step - loss: 0.3797 - nd: 0.1839 - val_loss: 0.1261 - val_nd: 0.1191\n",
            "Epoch 33/200\n",
            "18/18 [==============================] - 0s 22ms/step - loss: 0.3680 - nd: 0.1576 - val_loss: 0.1200 - val_nd: 0.1192\n",
            "Epoch 34/200\n",
            "18/18 [==============================] - 0s 19ms/step - loss: 0.3478 - nd: 0.1508 - val_loss: 0.1284 - val_nd: 0.1364\n",
            "Epoch 35/200\n",
            "18/18 [==============================] - 0s 19ms/step - loss: 0.3494 - nd: 0.1558 - val_loss: 0.1524 - val_nd: 0.1233\n",
            "Epoch 36/200\n",
            "18/18 [==============================] - 0s 20ms/step - loss: 0.3377 - nd: 0.1688 - val_loss: 0.1208 - val_nd: 0.1198\n",
            "Epoch 37/200\n",
            "18/18 [==============================] - 0s 19ms/step - loss: 0.3714 - nd: 0.1646 - val_loss: 0.2080 - val_nd: 0.1744\n",
            "Epoch 38/200\n",
            "18/18 [==============================] - 0s 19ms/step - loss: 0.3585 - nd: 0.1557 - val_loss: 0.1598 - val_nd: 0.1304\n",
            "Epoch 39/200\n",
            "18/18 [==============================] - 0s 20ms/step - loss: 0.3258 - nd: 0.1488 - val_loss: 0.1206 - val_nd: 0.1160\n",
            "Epoch 40/200\n",
            "18/18 [==============================] - 0s 19ms/step - loss: 0.3425 - nd: 0.1505 - val_loss: 0.1209 - val_nd: 0.1252\n",
            "Epoch 41/200\n",
            "18/18 [==============================] - 0s 19ms/step - loss: 0.3461 - nd: 0.1501 - val_loss: 0.1356 - val_nd: 0.1118\n",
            "Epoch 42/200\n",
            "18/18 [==============================] - 0s 19ms/step - loss: 0.3399 - nd: 0.1443 - val_loss: 0.1332 - val_nd: 0.1359\n",
            "Epoch 43/200\n",
            "18/18 [==============================] - 0s 20ms/step - loss: 0.3425 - nd: 0.1424 - val_loss: 0.1202 - val_nd: 0.1143\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 60614\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                   _step 43\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                    loss 0.3425021469593048\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                   epoch 42\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                _runtime 5299.103495121002\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              _timestamp 1650247675.3310776\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                  val_nd 0.11426868289709091\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                      nd 0.14242127537727356\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                val_loss 0.120209701359272\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           best_val_loss 0.11999034136533737\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              best_epoch 32\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              best-nd_95 0.0015864808741400229\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       best-nrmse_maxmin 0.021514407153047865\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    best-nrmse_maxmin_95 8.539966788644882e-05\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 best-nd 0.14734778526139497\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   best-nrmse_maxmin_std 0.001452302683610537\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     best-nrmse_mean_std 0.013917692615030538\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      best-nrmse_mean_95 0.0008184012468491908\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         best-nrmse_mean 0.20617665238098015\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             best-nd_std 0.02697961816518904\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 7 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced gallant-cloud-614: https://app.wandb.ai/matintavakoli/Interpreting_TS/runs/3icg6uro\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.9.7\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Wandb version 0.12.14 is available!  To upgrade, please run:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in wandb/run-20220418_020800-1bvs9s0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mpolar-aardvark-615\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://app.wandb.ai/matintavakoli/Interpreting_TS\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://app.wandb.ai/matintavakoli/Interpreting_TS/runs/1bvs9s0m\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb off` to turn off syncing.\n",
            "\n",
            "/usr/local/envs/eval-metric/lib/python3.7/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  return f(**kwargs)\n",
            "/usr/local/envs/eval-metric/lib/python3.7/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  return f(**kwargs)\n",
            "  4%|█▍                                     | 101/2660 [00:00<00:10, 236.81it/s]\n",
            "(9090, 30, 10) (9090, 6) (1111, 30, 10) (1111, 6)\n",
            "(9090, 30, 10) (9090, 6) (1111, 30, 10) (1111, 6)\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 60804\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              _timestamp 1650247931.8447087\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                   _step 2\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                    loss 0.005115541285338472\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                      nd 0.1203764173657505\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                _runtime 5555.617126226425\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                  val_nd 0.09920215786569062\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                val_loss 0.020238968985170937\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              best-nd_95 0.00080527355494608\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       best-nrmse_maxmin 0.01979911055967519\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    best-nrmse_maxmin_95 0.00023916535706191474\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 best-nd 0.11667802664088264\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   best-nrmse_maxmin_std 0.0040672346682838435\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     best-nrmse_mean_std 0.038977082770130254\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      best-nrmse_mean_95 0.002291967068102166\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         best-nrmse_mean 0.18973863914890385\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             best-nd_std 0.013694443711933511\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 8 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced polar-aardvark-615: https://app.wandb.ai/matintavakoli/Interpreting_TS/runs/1bvs9s0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.9.7\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Wandb version 0.12.14 is available!  To upgrade, please run:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in wandb/run-20220418_021215-1apee3rs\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mmagic-disco-616\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://app.wandb.ai/matintavakoli/Interpreting_TS\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://app.wandb.ai/matintavakoli/Interpreting_TS/runs/1apee3rs\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb off` to turn off syncing.\n",
            "\n",
            "/usr/local/envs/eval-metric/lib/python3.7/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  return f(**kwargs)\n",
            "/usr/local/envs/eval-metric/lib/python3.7/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  return f(**kwargs)\n",
            "  4%|█▍                                     | 101/2660 [00:00<00:12, 199.90it/s]\n",
            "(9090, 30, 10) (9090, 6) (1111, 30, 10) (1111, 6)\n",
            "(9090, 30, 10) (9090, 6) (1111, 30, 10) (1111, 6)\n",
            "Model: \"sequential_68\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "reshape_34 (Reshape)         (None, 300)               0         \n",
            "_________________________________________________________________\n",
            "dense_134 (Dense)            (None, 128)               38528     \n",
            "_________________________________________________________________\n",
            "dropout_100 (Dropout)        (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_135 (Dense)            (None, 128)               16512     \n",
            "_________________________________________________________________\n",
            "dropout_101 (Dropout)        (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_136 (Dense)            (None, 128)               16512     \n",
            "_________________________________________________________________\n",
            "dropout_102 (Dropout)        (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_137 (Dense)            (None, 6)                 774       \n",
            "=================================================================\n",
            "Total params: 72,326\n",
            "Trainable params: 72,326\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/200\n",
            "18/18 [==============================] - 0s 10ms/step - loss: 1.1122 - nd: 1.0644 - val_loss: 0.8321 - val_nd: 0.8849\n",
            "Epoch 2/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 1.0333 - nd: 0.9612 - val_loss: 0.8144 - val_nd: 0.9196\n",
            "Epoch 3/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 1.0141 - nd: 0.9086 - val_loss: 0.7855 - val_nd: 0.8687\n",
            "Epoch 4/200\n",
            "18/18 [==============================] - 0s 8ms/step - loss: 0.9668 - nd: 0.8553 - val_loss: 0.6926 - val_nd: 0.7584\n",
            "Epoch 5/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.8545 - nd: 0.7224 - val_loss: 0.4649 - val_nd: 0.4998\n",
            "Epoch 6/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.6921 - nd: 0.5633 - val_loss: 0.2751 - val_nd: 0.2710\n",
            "Epoch 7/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.6076 - nd: 0.4812 - val_loss: 0.2228 - val_nd: 0.2249\n",
            "Epoch 8/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.5653 - nd: 0.4369 - val_loss: 0.1996 - val_nd: 0.2083\n",
            "Epoch 9/200\n",
            "18/18 [==============================] - 0s 8ms/step - loss: 0.5273 - nd: 0.3908 - val_loss: 0.1760 - val_nd: 0.2140\n",
            "Epoch 10/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5151 - nd: 0.3671 - val_loss: 0.1879 - val_nd: 0.2261\n",
            "Epoch 11/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4855 - nd: 0.3533 - val_loss: 0.1785 - val_nd: 0.1973\n",
            "Epoch 12/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.4746 - nd: 0.3420 - val_loss: 0.1798 - val_nd: 0.1984\n",
            "Epoch 13/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.4672 - nd: 0.3293 - val_loss: 0.1726 - val_nd: 0.1914\n",
            "Epoch 14/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4651 - nd: 0.3294 - val_loss: 0.1732 - val_nd: 0.1649\n",
            "Epoch 15/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4564 - nd: 0.3122 - val_loss: 0.1760 - val_nd: 0.1670\n",
            "Epoch 16/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4474 - nd: 0.3049 - val_loss: 0.1749 - val_nd: 0.1767\n",
            "Epoch 17/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4482 - nd: 0.3064 - val_loss: 0.2146 - val_nd: 0.2040\n",
            "Epoch 18/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.4639 - nd: 0.2992 - val_loss: 0.1788 - val_nd: 0.1674\n",
            "Epoch 19/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4554 - nd: 0.2996 - val_loss: 0.1782 - val_nd: 0.1642\n",
            "Epoch 20/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.4513 - nd: 0.2921 - val_loss: 0.1812 - val_nd: 0.1614\n",
            "Epoch 21/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4306 - nd: 0.2874 - val_loss: 0.1804 - val_nd: 0.1546\n",
            "Epoch 22/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4389 - nd: 0.2866 - val_loss: 0.1809 - val_nd: 0.1474\n",
            "Epoch 23/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4565 - nd: 0.2870 - val_loss: 0.1815 - val_nd: 0.1460\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 60898\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                   _step 23\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                    loss 0.4565295875072479\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                   epoch 22\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                _runtime 5564.58123588562\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              _timestamp 1650247940.8088183\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                  val_nd 0.14599718153476715\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                      nd 0.2869754135608673\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                val_loss 0.18151973187923431\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           best_val_loss 0.17262013256549835\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              best_epoch 12\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              best-nd_95 0.001592777124924093\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       best-nrmse_maxmin 0.02956392853843691\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    best-nrmse_maxmin_95 0.0004323455760845775\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 best-nd 0.22043490011088587\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   best-nrmse_maxmin_std 0.00735244826981827\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     best-nrmse_mean_std 0.07045990904103105\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      best-nrmse_mean_95 0.004143249819282911\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         best-nrmse_mean 0.2833167455614473\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             best-nd_std 0.027086691906067616\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 7 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced magic-disco-616: https://app.wandb.ai/matintavakoli/Interpreting_TS/runs/1apee3rs\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.9.7\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Wandb version 0.12.14 is available!  To upgrade, please run:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in wandb/run-20220418_021225-2kx17uwr\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mwobbly-cloud-617\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://app.wandb.ai/matintavakoli/Interpreting_TS\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://app.wandb.ai/matintavakoli/Interpreting_TS/runs/2kx17uwr\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb off` to turn off syncing.\n",
            "\n",
            "/usr/local/envs/eval-metric/lib/python3.7/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  return f(**kwargs)\n",
            "/usr/local/envs/eval-metric/lib/python3.7/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  return f(**kwargs)\n",
            "  4%|█▍                                     | 101/2660 [00:00<00:11, 219.02it/s]\n",
            "(9090, 30, 10) (9090, 6) (1111, 30, 10) (1111, 6)\n",
            "(9090, 30, 10) (9090, 6) (1111, 30, 10) (1111, 6)\n",
            "Model: \"sequential_69\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm_66 (LSTM)               (None, 30, 128)           71168     \n",
            "_________________________________________________________________\n",
            "lstm_67 (LSTM)               (None, 30, 128)           131584    \n",
            "_________________________________________________________________\n",
            "lstm_68 (LSTM)               (None, 128)               131584    \n",
            "_________________________________________________________________\n",
            "dropout_103 (Dropout)        (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_138 (Dense)            (None, 6)                 774       \n",
            "=================================================================\n",
            "Total params: 335,110\n",
            "Trainable params: 335,110\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/200\n",
            "18/18 [==============================] - 1s 70ms/step - loss: 0.8930 - nd: 0.7772 - val_loss: 0.6519 - val_nd: 0.6773\n",
            "Epoch 2/200\n",
            "18/18 [==============================] - 0s 22ms/step - loss: 0.6311 - nd: 0.5162 - val_loss: 0.5028 - val_nd: 0.5172\n",
            "Epoch 3/200\n",
            "18/18 [==============================] - 0s 23ms/step - loss: 0.5305 - nd: 0.4244 - val_loss: 0.3625 - val_nd: 0.4241\n",
            "Epoch 4/200\n",
            "18/18 [==============================] - 0s 24ms/step - loss: 0.4996 - nd: 0.3772 - val_loss: 0.3062 - val_nd: 0.3669\n",
            "Epoch 5/200\n",
            "18/18 [==============================] - 0s 21ms/step - loss: 0.4825 - nd: 0.3467 - val_loss: 0.3229 - val_nd: 0.3497\n",
            "Epoch 6/200\n",
            "18/18 [==============================] - 0s 22ms/step - loss: 0.4661 - nd: 0.3391 - val_loss: 0.2670 - val_nd: 0.2888\n",
            "Epoch 7/200\n",
            "18/18 [==============================] - 0s 23ms/step - loss: 0.4502 - nd: 0.3105 - val_loss: 0.2568 - val_nd: 0.3001\n",
            "Epoch 8/200\n",
            "18/18 [==============================] - 0s 21ms/step - loss: 0.4376 - nd: 0.2891 - val_loss: 0.2761 - val_nd: 0.3283\n",
            "Epoch 9/200\n",
            "18/18 [==============================] - 0s 22ms/step - loss: 0.4309 - nd: 0.2849 - val_loss: 0.2010 - val_nd: 0.2334\n",
            "Epoch 10/200\n",
            "18/18 [==============================] - 0s 20ms/step - loss: 0.4271 - nd: 0.2815 - val_loss: 0.3745 - val_nd: 0.4201\n",
            "Epoch 11/200\n",
            "18/18 [==============================] - 0s 20ms/step - loss: 0.4821 - nd: 0.3814 - val_loss: 0.2394 - val_nd: 0.2803\n",
            "Epoch 12/200\n",
            "18/18 [==============================] - 0s 20ms/step - loss: 0.4324 - nd: 0.2961 - val_loss: 0.2452 - val_nd: 0.2811\n",
            "Epoch 13/200\n",
            "18/18 [==============================] - 0s 20ms/step - loss: 0.4081 - nd: 0.2661 - val_loss: 0.2171 - val_nd: 0.2423\n",
            "Epoch 14/200\n",
            "18/18 [==============================] - 0s 20ms/step - loss: 0.4037 - nd: 0.2532 - val_loss: 0.2093 - val_nd: 0.2431\n",
            "Epoch 15/200\n",
            "18/18 [==============================] - 0s 21ms/step - loss: 0.4071 - nd: 0.2547 - val_loss: 0.2623 - val_nd: 0.2940\n",
            "Epoch 16/200\n",
            "18/18 [==============================] - 0s 20ms/step - loss: 0.4107 - nd: 0.2555 - val_loss: 0.2332 - val_nd: 0.2678\n",
            "Epoch 17/200\n",
            "18/18 [==============================] - 0s 20ms/step - loss: 0.3987 - nd: 0.2462 - val_loss: 0.2233 - val_nd: 0.2427\n",
            "Epoch 18/200\n",
            "18/18 [==============================] - 0s 23ms/step - loss: 0.4124 - nd: 0.2472 - val_loss: 0.1985 - val_nd: 0.2048\n",
            "Epoch 19/200\n",
            "18/18 [==============================] - 0s 23ms/step - loss: 0.4042 - nd: 0.2453 - val_loss: 0.1570 - val_nd: 0.1725\n",
            "Epoch 20/200\n",
            "18/18 [==============================] - 0s 22ms/step - loss: 0.3893 - nd: 0.2363 - val_loss: 0.1642 - val_nd: 0.1841\n",
            "Epoch 21/200\n",
            "18/18 [==============================] - 0s 22ms/step - loss: 0.4009 - nd: 0.2361 - val_loss: 0.1898 - val_nd: 0.2184\n",
            "Epoch 22/200\n",
            "18/18 [==============================] - 0s 21ms/step - loss: 0.3993 - nd: 0.2442 - val_loss: 0.1674 - val_nd: 0.1902\n",
            "Epoch 23/200\n",
            "18/18 [==============================] - 0s 20ms/step - loss: 0.3894 - nd: 0.2359 - val_loss: 0.1688 - val_nd: 0.1921\n",
            "Epoch 24/200\n",
            "18/18 [==============================] - 0s 23ms/step - loss: 0.3979 - nd: 0.2288 - val_loss: 0.1545 - val_nd: 0.1634\n",
            "Epoch 25/200\n",
            "18/18 [==============================] - 0s 20ms/step - loss: 0.4063 - nd: 0.2430 - val_loss: 0.2279 - val_nd: 0.2430\n",
            "Epoch 26/200\n",
            "18/18 [==============================] - 0s 20ms/step - loss: 0.4031 - nd: 0.2410 - val_loss: 0.1929 - val_nd: 0.2187\n",
            "Epoch 27/200\n",
            "18/18 [==============================] - 0s 20ms/step - loss: 0.3873 - nd: 0.2224 - val_loss: 0.1918 - val_nd: 0.2061\n",
            "Epoch 28/200\n",
            "18/18 [==============================] - 0s 21ms/step - loss: 0.3824 - nd: 0.2286 - val_loss: 0.1982 - val_nd: 0.2270\n",
            "Epoch 29/200\n",
            "18/18 [==============================] - 0s 20ms/step - loss: 0.3892 - nd: 0.2297 - val_loss: 0.1679 - val_nd: 0.1809\n",
            "Epoch 30/200\n",
            "18/18 [==============================] - 0s 21ms/step - loss: 0.3757 - nd: 0.2146 - val_loss: 0.1966 - val_nd: 0.1978\n",
            "Epoch 31/200\n",
            "18/18 [==============================] - 0s 23ms/step - loss: 0.3776 - nd: 0.2154 - val_loss: 0.1475 - val_nd: 0.1574\n",
            "Epoch 32/200\n",
            "18/18 [==============================] - 0s 20ms/step - loss: 0.3842 - nd: 0.2171 - val_loss: 0.1901 - val_nd: 0.1875\n",
            "Epoch 33/200\n",
            "18/18 [==============================] - 0s 20ms/step - loss: 0.3923 - nd: 0.2195 - val_loss: 0.1522 - val_nd: 0.1541\n",
            "Epoch 34/200\n",
            "18/18 [==============================] - 0s 21ms/step - loss: 0.3874 - nd: 0.2186 - val_loss: 0.1945 - val_nd: 0.1998\n",
            "Epoch 35/200\n",
            "18/18 [==============================] - 0s 20ms/step - loss: 0.3719 - nd: 0.2161 - val_loss: 0.1555 - val_nd: 0.1466\n",
            "Epoch 36/200\n",
            "18/18 [==============================] - 0s 20ms/step - loss: 0.3495 - nd: 0.2073 - val_loss: 0.2104 - val_nd: 0.1884\n",
            "Epoch 37/200\n",
            "18/18 [==============================] - 0s 20ms/step - loss: 0.3867 - nd: 0.2078 - val_loss: 0.1693 - val_nd: 0.1583\n",
            "Epoch 38/200\n",
            "18/18 [==============================] - 0s 20ms/step - loss: 0.3738 - nd: 0.2023 - val_loss: 0.1828 - val_nd: 0.1694\n",
            "Epoch 39/200\n",
            "18/18 [==============================] - 0s 21ms/step - loss: 0.3737 - nd: 0.2020 - val_loss: 0.1764 - val_nd: 0.1715\n",
            "Epoch 40/200\n",
            "18/18 [==============================] - 0s 21ms/step - loss: 0.3815 - nd: 0.2085 - val_loss: 0.1876 - val_nd: 0.1806\n",
            "Epoch 41/200\n",
            "18/18 [==============================] - 0s 20ms/step - loss: 0.3787 - nd: 0.2405 - val_loss: 0.2057 - val_nd: 0.1964\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 61024\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                   _step 41\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                    loss 0.37871965765953064\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                   epoch 40\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                _runtime 5593.594382286072\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              _timestamp 1650247969.8219647\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                  val_nd 0.19644659757614136\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                      nd 0.2404654324054718\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                val_loss 0.2057422399520874\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           best_val_loss 0.14749112725257874\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              best_epoch 30\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              best-nd_95 0.0020750701062286782\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       best-nrmse_maxmin 0.025822109704821353\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    best-nrmse_maxmin_95 0.00015303166672123858\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 best-nd 0.18646760180595123\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   best-nrmse_maxmin_std 0.0026024492337857713\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     best-nrmse_mean_std 0.024939765581101954\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      best-nrmse_mean_95 0.00146653154457928\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         best-nrmse_mean 0.24745818457750376\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             best-nd_std 0.035288543369547606\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 7 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced wobbly-cloud-617: https://app.wandb.ai/matintavakoli/Interpreting_TS/runs/2kx17uwr\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.9.7\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Wandb version 0.12.14 is available!  To upgrade, please run:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in wandb/run-20220418_021255-qeebn84y\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mamber-waterfall-618\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://app.wandb.ai/matintavakoli/Interpreting_TS\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://app.wandb.ai/matintavakoli/Interpreting_TS/runs/qeebn84y\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb off` to turn off syncing.\n",
            "\n",
            "/usr/local/envs/eval-metric/lib/python3.7/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  return f(**kwargs)\n",
            "/usr/local/envs/eval-metric/lib/python3.7/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  return f(**kwargs)\n",
            "  4%|█▍                                     | 101/2660 [00:00<00:10, 235.82it/s]\n",
            "(9090, 30, 10) (9090, 6) (1111, 30, 10) (1111, 6)\n",
            "(9090, 30, 10) (9090, 6) (1111, 30, 10) (1111, 6)\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 61208\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              _timestamp 1650248226.8422787\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                   _step 2\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                    loss 0.0051247821290048975\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                      nd 0.12052057611674336\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                _runtime 5850.614696264267\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                  val_nd 0.09935574530025659\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                val_loss 0.020213027303978778\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              best-nd_95 0.0007988273409403468\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       best-nrmse_maxmin 0.019790316655533092\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    best-nrmse_maxmin_95 0.00023593268356853793\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 best-nd 0.11693237904087665\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   best-nrmse_maxmin_std 0.004012259976861035\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     best-nrmse_mean_std 0.038450249854744704\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      best-nrmse_mean_95 0.0022609877436725483\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         best-nrmse_mean 0.1896543654943021\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             best-nd_std 0.013584819703651626\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 8 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced amber-waterfall-618: https://app.wandb.ai/matintavakoli/Interpreting_TS/runs/qeebn84y\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.9.7\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Wandb version 0.12.14 is available!  To upgrade, please run:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in wandb/run-20220418_021712-2r8cnshn\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mfragrant-salad-619\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://app.wandb.ai/matintavakoli/Interpreting_TS\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://app.wandb.ai/matintavakoli/Interpreting_TS/runs/2r8cnshn\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb off` to turn off syncing.\n",
            "\n",
            "/usr/local/envs/eval-metric/lib/python3.7/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  return f(**kwargs)\n",
            "/usr/local/envs/eval-metric/lib/python3.7/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  return f(**kwargs)\n",
            "  4%|█▍                                     | 101/2660 [00:00<00:10, 242.31it/s]\n",
            "(9090, 30, 10) (9090, 6) (1111, 30, 10) (1111, 6)\n",
            "(9090, 30, 10) (9090, 6) (1111, 30, 10) (1111, 6)\n",
            "Model: \"sequential_70\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "reshape_35 (Reshape)         (None, 300)               0         \n",
            "_________________________________________________________________\n",
            "dense_139 (Dense)            (None, 128)               38528     \n",
            "_________________________________________________________________\n",
            "dropout_104 (Dropout)        (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_140 (Dense)            (None, 128)               16512     \n",
            "_________________________________________________________________\n",
            "dropout_105 (Dropout)        (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_141 (Dense)            (None, 128)               16512     \n",
            "_________________________________________________________________\n",
            "dropout_106 (Dropout)        (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_142 (Dense)            (None, 6)                 774       \n",
            "=================================================================\n",
            "Total params: 72,326\n",
            "Trainable params: 72,326\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/200\n",
            "18/18 [==============================] - 0s 10ms/step - loss: 1.2328 - nd: 1.2709 - val_loss: 0.8262 - val_nd: 0.9000\n",
            "Epoch 2/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 1.1036 - nd: 1.0727 - val_loss: 0.8176 - val_nd: 0.8975\n",
            "Epoch 3/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 1.0477 - nd: 0.9612 - val_loss: 0.8138 - val_nd: 0.9055\n",
            "Epoch 4/200\n",
            "18/18 [==============================] - 0s 8ms/step - loss: 1.0165 - nd: 0.9248 - val_loss: 0.8056 - val_nd: 0.8979\n",
            "Epoch 5/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.9930 - nd: 0.8839 - val_loss: 0.7817 - val_nd: 0.8603\n",
            "Epoch 6/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.9636 - nd: 0.8473 - val_loss: 0.7056 - val_nd: 0.7633\n",
            "Epoch 7/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.8939 - nd: 0.7659 - val_loss: 0.5182 - val_nd: 0.5191\n",
            "Epoch 8/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.7769 - nd: 0.6663 - val_loss: 0.3498 - val_nd: 0.3349\n",
            "Epoch 9/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.7145 - nd: 0.5929 - val_loss: 0.2866 - val_nd: 0.3048\n",
            "Epoch 10/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.6481 - nd: 0.5305 - val_loss: 0.2416 - val_nd: 0.2667\n",
            "Epoch 11/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.6194 - nd: 0.5059 - val_loss: 0.2529 - val_nd: 0.2964\n",
            "Epoch 12/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.6084 - nd: 0.4955 - val_loss: 0.2293 - val_nd: 0.2654\n",
            "Epoch 13/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.5909 - nd: 0.4778 - val_loss: 0.2146 - val_nd: 0.2696\n",
            "Epoch 14/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.5716 - nd: 0.4591 - val_loss: 0.2450 - val_nd: 0.3008\n",
            "Epoch 15/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.5727 - nd: 0.4537 - val_loss: 0.2423 - val_nd: 0.2945\n",
            "Epoch 16/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.5566 - nd: 0.4503 - val_loss: 0.2435 - val_nd: 0.2948\n",
            "Epoch 17/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5380 - nd: 0.4429 - val_loss: 0.2259 - val_nd: 0.2738\n",
            "Epoch 18/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5577 - nd: 0.4386 - val_loss: 0.2461 - val_nd: 0.2925\n",
            "Epoch 19/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.5505 - nd: 0.4371 - val_loss: 0.2701 - val_nd: 0.3271\n",
            "Epoch 20/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5456 - nd: 0.4249 - val_loss: 0.2393 - val_nd: 0.2849\n",
            "Epoch 21/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.5470 - nd: 0.4134 - val_loss: 0.2298 - val_nd: 0.2677\n",
            "Epoch 22/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5342 - nd: 0.4157 - val_loss: 0.2419 - val_nd: 0.2709\n",
            "Epoch 23/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.5130 - nd: 0.3979 - val_loss: 0.2058 - val_nd: 0.2438\n",
            "Epoch 24/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.5114 - nd: 0.3956 - val_loss: 0.2181 - val_nd: 0.2386\n",
            "Epoch 25/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5188 - nd: 0.3960 - val_loss: 0.2252 - val_nd: 0.2567\n",
            "Epoch 26/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5171 - nd: 0.3884 - val_loss: 0.2148 - val_nd: 0.2315\n",
            "Epoch 27/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.5238 - nd: 0.3965 - val_loss: 0.2220 - val_nd: 0.2349\n",
            "Epoch 28/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5174 - nd: 0.3802 - val_loss: 0.2222 - val_nd: 0.2374\n",
            "Epoch 29/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.5087 - nd: 0.3840 - val_loss: 0.2208 - val_nd: 0.2375\n",
            "Epoch 30/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.5149 - nd: 0.3784 - val_loss: 0.1987 - val_nd: 0.2301\n",
            "Epoch 31/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.5091 - nd: 0.3696 - val_loss: 0.2191 - val_nd: 0.2287\n",
            "Epoch 32/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5058 - nd: 0.3701 - val_loss: 0.2149 - val_nd: 0.2129\n",
            "Epoch 33/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.5088 - nd: 0.3775 - val_loss: 0.2042 - val_nd: 0.2024\n",
            "Epoch 34/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5060 - nd: 0.3702 - val_loss: 0.2072 - val_nd: 0.2180\n",
            "Epoch 35/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5083 - nd: 0.3670 - val_loss: 0.2143 - val_nd: 0.2288\n",
            "Epoch 36/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4969 - nd: 0.3661 - val_loss: 0.2144 - val_nd: 0.2297\n",
            "Epoch 37/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4942 - nd: 0.3572 - val_loss: 0.2064 - val_nd: 0.2229\n",
            "Epoch 38/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4975 - nd: 0.3553 - val_loss: 0.2077 - val_nd: 0.2103\n",
            "Epoch 39/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5038 - nd: 0.3559 - val_loss: 0.2129 - val_nd: 0.2173\n",
            "Epoch 40/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4792 - nd: 0.3508 - val_loss: 0.2204 - val_nd: 0.2245\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 61302\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                   _step 40\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                    loss 0.47920867800712585\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                   epoch 39\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                _runtime 5863.148516654968\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              _timestamp 1650248239.376099\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                  val_nd 0.22451309859752655\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                      nd 0.35083743929862976\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                val_loss 0.22041794657707214\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           best_val_loss 0.198737233877182\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              best_epoch 29\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              best-nd_95 0.0016731005896186875\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       best-nrmse_maxmin 0.03308996823460918\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    best-nrmse_maxmin_95 0.000598950324861779\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 best-nd 0.24349309232206667\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   best-nrmse_maxmin_std 0.010185720690421948\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     best-nrmse_mean_std 0.0976116970874271\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      best-nrmse_mean_95 0.005739854788655324\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         best-nrmse_mean 0.31710745406424967\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             best-nd_std 0.02845266892002936\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 7 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced fragrant-salad-619: https://app.wandb.ai/matintavakoli/Interpreting_TS/runs/2r8cnshn\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.9.7\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Wandb version 0.12.14 is available!  To upgrade, please run:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in wandb/run-20220418_021724-8v79a5wg\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mdry-bush-620\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://app.wandb.ai/matintavakoli/Interpreting_TS\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://app.wandb.ai/matintavakoli/Interpreting_TS/runs/8v79a5wg\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb off` to turn off syncing.\n",
            "\n",
            "/usr/local/envs/eval-metric/lib/python3.7/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  return f(**kwargs)\n",
            "/usr/local/envs/eval-metric/lib/python3.7/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  return f(**kwargs)\n",
            "  4%|█▍                                     | 101/2660 [00:00<00:11, 228.58it/s]\n",
            "(9090, 30, 10) (9090, 6) (1111, 30, 10) (1111, 6)\n",
            "(9090, 30, 10) (9090, 6) (1111, 30, 10) (1111, 6)\n",
            "Model: \"sequential_71\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm_69 (LSTM)               (None, 30, 128)           71168     \n",
            "_________________________________________________________________\n",
            "lstm_70 (LSTM)               (None, 30, 128)           131584    \n",
            "_________________________________________________________________\n",
            "lstm_71 (LSTM)               (None, 128)               131584    \n",
            "_________________________________________________________________\n",
            "dropout_107 (Dropout)        (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_143 (Dense)            (None, 6)                 774       \n",
            "=================================================================\n",
            "Total params: 335,110\n",
            "Trainable params: 335,110\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/200\n",
            "18/18 [==============================] - 1s 70ms/step - loss: 0.9571 - nd: 0.8502 - val_loss: 0.9175 - val_nd: 0.8200\n",
            "Epoch 2/200\n",
            "18/18 [==============================] - 0s 22ms/step - loss: 0.7030 - nd: 0.5962 - val_loss: 0.5886 - val_nd: 0.6154\n",
            "Epoch 3/200\n",
            "18/18 [==============================] - 0s 23ms/step - loss: 0.6158 - nd: 0.5018 - val_loss: 0.5544 - val_nd: 0.5406\n",
            "Epoch 4/200\n",
            "18/18 [==============================] - 0s 23ms/step - loss: 0.5628 - nd: 0.4414 - val_loss: 0.5317 - val_nd: 0.5671\n",
            "Epoch 5/200\n",
            "18/18 [==============================] - 0s 23ms/step - loss: 0.5284 - nd: 0.4088 - val_loss: 0.5042 - val_nd: 0.5572\n",
            "Epoch 6/200\n",
            "18/18 [==============================] - 0s 23ms/step - loss: 0.5054 - nd: 0.3927 - val_loss: 0.4285 - val_nd: 0.4644\n",
            "Epoch 7/200\n",
            "18/18 [==============================] - 0s 21ms/step - loss: 0.5102 - nd: 0.3699 - val_loss: 0.4559 - val_nd: 0.5297\n",
            "Epoch 8/200\n",
            "18/18 [==============================] - 0s 20ms/step - loss: 0.5042 - nd: 0.3739 - val_loss: 0.4537 - val_nd: 0.4831\n",
            "Epoch 9/200\n",
            "18/18 [==============================] - 0s 21ms/step - loss: 0.5003 - nd: 0.3767 - val_loss: 0.4805 - val_nd: 0.5774\n",
            "Epoch 10/200\n",
            "18/18 [==============================] - 0s 21ms/step - loss: 0.4876 - nd: 0.3570 - val_loss: 0.4946 - val_nd: 0.5654\n",
            "Epoch 11/200\n",
            "18/18 [==============================] - 0s 20ms/step - loss: 0.4804 - nd: 0.3506 - val_loss: 0.4736 - val_nd: 0.5815\n",
            "Epoch 12/200\n",
            "18/18 [==============================] - 0s 21ms/step - loss: 0.4642 - nd: 0.3257 - val_loss: 0.4373 - val_nd: 0.5111\n",
            "Epoch 13/200\n",
            "18/18 [==============================] - 0s 22ms/step - loss: 0.4589 - nd: 0.3216 - val_loss: 0.4187 - val_nd: 0.4989\n",
            "Epoch 14/200\n",
            "18/18 [==============================] - 0s 23ms/step - loss: 0.4617 - nd: 0.3127 - val_loss: 0.3738 - val_nd: 0.4299\n",
            "Epoch 15/200\n",
            "18/18 [==============================] - 0s 21ms/step - loss: 0.4512 - nd: 0.3100 - val_loss: 0.3935 - val_nd: 0.4751\n",
            "Epoch 16/200\n",
            "18/18 [==============================] - 0s 22ms/step - loss: 0.4517 - nd: 0.3094 - val_loss: 0.3390 - val_nd: 0.4019\n",
            "Epoch 17/200\n",
            "18/18 [==============================] - 0s 21ms/step - loss: 0.4451 - nd: 0.3167 - val_loss: 0.3885 - val_nd: 0.4804\n",
            "Epoch 18/200\n",
            "18/18 [==============================] - 0s 21ms/step - loss: 0.4617 - nd: 0.3217 - val_loss: 0.3810 - val_nd: 0.4228\n",
            "Epoch 19/200\n",
            "18/18 [==============================] - 0s 22ms/step - loss: 0.4732 - nd: 0.3113 - val_loss: 0.3223 - val_nd: 0.3544\n",
            "Epoch 20/200\n",
            "18/18 [==============================] - 0s 20ms/step - loss: 0.4559 - nd: 0.3059 - val_loss: 0.3517 - val_nd: 0.4084\n",
            "Epoch 21/200\n",
            "18/18 [==============================] - 0s 20ms/step - loss: 0.4510 - nd: 0.2942 - val_loss: 0.3411 - val_nd: 0.3774\n",
            "Epoch 22/200\n",
            "18/18 [==============================] - 0s 20ms/step - loss: 0.4659 - nd: 0.2956 - val_loss: 0.3504 - val_nd: 0.3974\n",
            "Epoch 23/200\n",
            "18/18 [==============================] - 0s 20ms/step - loss: 0.4363 - nd: 0.2873 - val_loss: 0.3413 - val_nd: 0.3767\n",
            "Epoch 24/200\n",
            "18/18 [==============================] - 0s 20ms/step - loss: 0.4393 - nd: 0.2803 - val_loss: 0.3636 - val_nd: 0.4057\n",
            "Epoch 25/200\n",
            "18/18 [==============================] - 0s 21ms/step - loss: 0.4367 - nd: 0.2827 - val_loss: 0.3528 - val_nd: 0.3793\n",
            "Epoch 26/200\n",
            "18/18 [==============================] - 0s 21ms/step - loss: 0.4397 - nd: 0.2842 - val_loss: 0.3872 - val_nd: 0.4410\n",
            "Epoch 27/200\n",
            "18/18 [==============================] - 0s 20ms/step - loss: 0.4405 - nd: 0.2938 - val_loss: 0.3708 - val_nd: 0.4311\n",
            "Epoch 28/200\n",
            "18/18 [==============================] - 0s 23ms/step - loss: 0.4317 - nd: 0.2827 - val_loss: 0.3165 - val_nd: 0.2997\n",
            "Epoch 29/200\n",
            "18/18 [==============================] - 0s 21ms/step - loss: 0.4272 - nd: 0.2793 - val_loss: 0.3639 - val_nd: 0.4017\n",
            "Epoch 30/200\n",
            "18/18 [==============================] - 0s 20ms/step - loss: 0.4231 - nd: 0.2774 - val_loss: 0.3601 - val_nd: 0.4077\n",
            "Epoch 31/200\n",
            "18/18 [==============================] - 0s 20ms/step - loss: 0.4268 - nd: 0.2804 - val_loss: 0.3700 - val_nd: 0.3669\n",
            "Epoch 32/200\n",
            "18/18 [==============================] - 0s 21ms/step - loss: 0.4425 - nd: 0.2804 - val_loss: 0.3324 - val_nd: 0.3520\n",
            "Epoch 33/200\n",
            "18/18 [==============================] - 0s 23ms/step - loss: 0.4401 - nd: 0.2726 - val_loss: 0.3161 - val_nd: 0.3275\n",
            "Epoch 34/200\n",
            "18/18 [==============================] - 0s 23ms/step - loss: 0.4267 - nd: 0.2710 - val_loss: 0.3133 - val_nd: 0.3684\n",
            "Epoch 35/200\n",
            "18/18 [==============================] - 0s 23ms/step - loss: 0.4261 - nd: 0.2760 - val_loss: 0.3021 - val_nd: 0.3222\n",
            "Epoch 36/200\n",
            "18/18 [==============================] - 0s 21ms/step - loss: 0.4253 - nd: 0.2697 - val_loss: 0.3427 - val_nd: 0.3960\n",
            "Epoch 37/200\n",
            "18/18 [==============================] - 0s 21ms/step - loss: 0.4095 - nd: 0.2633 - val_loss: 0.3091 - val_nd: 0.3180\n",
            "Epoch 38/200\n",
            "18/18 [==============================] - 0s 23ms/step - loss: 0.4267 - nd: 0.2618 - val_loss: 0.2620 - val_nd: 0.2888\n",
            "Epoch 39/200\n",
            "18/18 [==============================] - 0s 21ms/step - loss: 0.4217 - nd: 0.2675 - val_loss: 0.2951 - val_nd: 0.3242\n",
            "Epoch 40/200\n",
            "18/18 [==============================] - 0s 20ms/step - loss: 0.4124 - nd: 0.2590 - val_loss: 0.3115 - val_nd: 0.3475\n",
            "Epoch 41/200\n",
            "18/18 [==============================] - 0s 20ms/step - loss: 0.4130 - nd: 0.2616 - val_loss: 0.2888 - val_nd: 0.3400\n",
            "Epoch 42/200\n",
            "18/18 [==============================] - 0s 21ms/step - loss: 0.4169 - nd: 0.2608 - val_loss: 0.2900 - val_nd: 0.3313\n",
            "Epoch 43/200\n",
            "18/18 [==============================] - 0s 20ms/step - loss: 0.4127 - nd: 0.2599 - val_loss: 0.3314 - val_nd: 0.3296\n",
            "Epoch 44/200\n",
            "18/18 [==============================] - 0s 21ms/step - loss: 0.4255 - nd: 0.2548 - val_loss: 0.3247 - val_nd: 0.2970\n",
            "Epoch 45/200\n",
            "18/18 [==============================] - 0s 21ms/step - loss: 0.4046 - nd: 0.2541 - val_loss: 0.3336 - val_nd: 0.3211\n",
            "Epoch 46/200\n",
            "18/18 [==============================] - 0s 21ms/step - loss: 0.3965 - nd: 0.2529 - val_loss: 0.3161 - val_nd: 0.3064\n",
            "Epoch 47/200\n",
            "18/18 [==============================] - 0s 21ms/step - loss: 0.4038 - nd: 0.2547 - val_loss: 0.3139 - val_nd: 0.3085\n",
            "Epoch 48/200\n",
            "18/18 [==============================] - 0s 21ms/step - loss: 0.4082 - nd: 0.2542 - val_loss: 0.3246 - val_nd: 0.3121\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 61479\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                   _step 48\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                    loss 0.40824881196022034\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                   epoch 47\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                _runtime 5894.92204451561\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              _timestamp 1650248271.149627\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                  val_nd 0.31206610798835754\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                      nd 0.2541729509830475\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                val_loss 0.3246353268623352\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           best_val_loss 0.2619648277759552\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              best_epoch 37\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              best-nd_95 0.003567638741499444\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       best-nrmse_maxmin 0.04611870485832173\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    best-nrmse_maxmin_95 0.0005001033687587714\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 best-nd 0.3577610600988054\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   best-nrmse_maxmin_std 0.008504734064033554\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     best-nrmse_mean_std 0.0815024827892837\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      best-nrmse_mean_95 0.004792585623282077\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         best-nrmse_mean 0.4419643131318249\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             best-nd_std 0.06067109447453397\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 7 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced dry-bush-620: https://app.wandb.ai/matintavakoli/Interpreting_TS/runs/8v79a5wg\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.9.7\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Wandb version 0.12.14 is available!  To upgrade, please run:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in wandb/run-20220418_021755-3hpv2nm6\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mautumn-feather-621\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://app.wandb.ai/matintavakoli/Interpreting_TS\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://app.wandb.ai/matintavakoli/Interpreting_TS/runs/3hpv2nm6\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb off` to turn off syncing.\n",
            "\n",
            "/usr/local/envs/eval-metric/lib/python3.7/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  return f(**kwargs)\n",
            "/usr/local/envs/eval-metric/lib/python3.7/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  return f(**kwargs)\n",
            "  4%|█▍                                     | 101/2660 [00:00<00:11, 227.17it/s]\n",
            "(9090, 30, 10) (9090, 6) (1111, 30, 10) (1111, 6)\n",
            "(9090, 30, 10) (9090, 6) (1111, 30, 10) (1111, 6)\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 61684\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              _timestamp 1650248526.1004176\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                   _step 2\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                    loss 0.005126140585181629\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                      nd 0.12055915824278177\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                _runtime 6149.872835159302\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                  val_nd 0.0992256905472076\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                val_loss 0.020239708274404234\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              best-nd_95 0.0008105360571421949\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       best-nrmse_maxmin 0.019800299175218517\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    best-nrmse_maxmin_95 0.0002399824839140962\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 best-nd 0.1167653144760943\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   best-nrmse_maxmin_std 0.004081130688603867\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     best-nrmse_mean_std 0.03911025097367961\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      best-nrmse_mean_95 0.0022997977500147527\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         best-nrmse_mean 0.18975002987754203\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             best-nd_std 0.013783937573573418\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 8 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced autumn-feather-621: https://app.wandb.ai/matintavakoli/Interpreting_TS/runs/3hpv2nm6\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.9.7\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Wandb version 0.12.14 is available!  To upgrade, please run:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in wandb/run-20220418_022210-w12wz5b5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33miconic-wind-622\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://app.wandb.ai/matintavakoli/Interpreting_TS\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://app.wandb.ai/matintavakoli/Interpreting_TS/runs/w12wz5b5\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb off` to turn off syncing.\n",
            "\n",
            "/usr/local/envs/eval-metric/lib/python3.7/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  return f(**kwargs)\n",
            "/usr/local/envs/eval-metric/lib/python3.7/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  return f(**kwargs)\n",
            "  4%|█▍                                     | 101/2660 [00:00<00:10, 234.12it/s]\n",
            "(9090, 30, 10) (9090, 6) (1111, 30, 10) (1111, 6)\n",
            "(9090, 30, 10) (9090, 6) (1111, 30, 10) (1111, 6)\n",
            "Model: \"sequential_72\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "reshape_36 (Reshape)         (None, 300)               0         \n",
            "_________________________________________________________________\n",
            "dense_144 (Dense)            (None, 16)                4816      \n",
            "_________________________________________________________________\n",
            "dropout_108 (Dropout)        (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_145 (Dense)            (None, 16)                272       \n",
            "_________________________________________________________________\n",
            "dropout_109 (Dropout)        (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_146 (Dense)            (None, 16)                272       \n",
            "_________________________________________________________________\n",
            "dropout_110 (Dropout)        (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_147 (Dense)            (None, 16)                272       \n",
            "_________________________________________________________________\n",
            "dropout_111 (Dropout)        (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_148 (Dense)            (None, 6)                 102       \n",
            "=================================================================\n",
            "Total params: 5,734\n",
            "Trainable params: 5,734\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/200\n",
            "18/18 [==============================] - 0s 11ms/step - loss: 1.3046 - nd: 1.4483 - val_loss: 1.0912 - val_nd: 1.2839\n",
            "Epoch 2/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 1.1663 - nd: 1.2685 - val_loss: 0.9729 - val_nd: 1.1557\n",
            "Epoch 3/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 1.0888 - nd: 1.1443 - val_loss: 0.9011 - val_nd: 1.0677\n",
            "Epoch 4/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 1.0463 - nd: 1.0517 - val_loss: 0.8620 - val_nd: 1.0103\n",
            "Epoch 5/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 1.0186 - nd: 0.9887 - val_loss: 0.8422 - val_nd: 0.9696\n",
            "Epoch 6/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 1.0115 - nd: 0.9465 - val_loss: 0.8336 - val_nd: 0.9461\n",
            "Epoch 7/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 1.0055 - nd: 0.9208 - val_loss: 0.8297 - val_nd: 0.9299\n",
            "Epoch 8/200\n",
            "18/18 [==============================] - 0s 8ms/step - loss: 1.0016 - nd: 0.9064 - val_loss: 0.8281 - val_nd: 0.9209\n",
            "Epoch 9/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 1.0018 - nd: 0.8935 - val_loss: 0.8274 - val_nd: 0.9154\n",
            "Epoch 10/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 1.0011 - nd: 0.8901 - val_loss: 0.8272 - val_nd: 0.9138\n",
            "Epoch 11/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 1.0023 - nd: 0.8904 - val_loss: 0.8273 - val_nd: 0.9143\n",
            "Epoch 12/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 1.0043 - nd: 0.8861 - val_loss: 0.8270 - val_nd: 0.9108\n",
            "Epoch 13/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 1.0014 - nd: 0.8873 - val_loss: 0.8270 - val_nd: 0.9118\n",
            "Epoch 14/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 1.0041 - nd: 0.8850 - val_loss: 0.8269 - val_nd: 0.9108\n",
            "Epoch 15/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 1.0017 - nd: 0.8837 - val_loss: 0.8268 - val_nd: 0.9094\n",
            "Epoch 16/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 1.0042 - nd: 0.8880 - val_loss: 0.8271 - val_nd: 0.9125\n",
            "Epoch 17/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 1.0004 - nd: 0.8858 - val_loss: 0.8268 - val_nd: 0.9095\n",
            "Epoch 18/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 1.0046 - nd: 0.8839 - val_loss: 0.8268 - val_nd: 0.9091\n",
            "Epoch 19/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 1.0049 - nd: 0.8866 - val_loss: 0.8269 - val_nd: 0.9109\n",
            "Epoch 20/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 1.0025 - nd: 0.8838 - val_loss: 0.8266 - val_nd: 0.9069\n",
            "Epoch 21/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.9985 - nd: 0.8821 - val_loss: 0.8267 - val_nd: 0.9082\n",
            "Epoch 22/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 1.0035 - nd: 0.8860 - val_loss: 0.8268 - val_nd: 0.9106\n",
            "Epoch 23/200\n",
            "18/18 [==============================] - 0s 8ms/step - loss: 0.9978 - nd: 0.8805 - val_loss: 0.8265 - val_nd: 0.9059\n",
            "Epoch 24/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 1.0038 - nd: 0.8832 - val_loss: 0.8269 - val_nd: 0.9118\n",
            "Epoch 25/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.9990 - nd: 0.8872 - val_loss: 0.8268 - val_nd: 0.9101\n",
            "Epoch 26/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.9985 - nd: 0.8821 - val_loss: 0.8267 - val_nd: 0.9094\n",
            "Epoch 27/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 1.0007 - nd: 0.8863 - val_loss: 0.8265 - val_nd: 0.9074\n",
            "Epoch 28/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 1.0019 - nd: 0.8801 - val_loss: 0.8255 - val_nd: 0.9054\n",
            "Epoch 29/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 1.0035 - nd: 0.8845 - val_loss: 0.8255 - val_nd: 0.9105\n",
            "Epoch 30/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 1.0028 - nd: 0.8859 - val_loss: 0.8235 - val_nd: 0.9060\n",
            "Epoch 31/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.9936 - nd: 0.8767 - val_loss: 0.8210 - val_nd: 0.9029\n",
            "Epoch 32/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.9935 - nd: 0.8817 - val_loss: 0.8181 - val_nd: 0.8976\n",
            "Epoch 33/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.9926 - nd: 0.8761 - val_loss: 0.8140 - val_nd: 0.8923\n",
            "Epoch 34/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.9865 - nd: 0.8685 - val_loss: 0.8081 - val_nd: 0.8798\n",
            "Epoch 35/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.9880 - nd: 0.8589 - val_loss: 0.8003 - val_nd: 0.8744\n",
            "Epoch 36/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.9675 - nd: 0.8460 - val_loss: 0.7785 - val_nd: 0.8371\n",
            "Epoch 37/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.9503 - nd: 0.8141 - val_loss: 0.7575 - val_nd: 0.8153\n",
            "Epoch 38/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.9312 - nd: 0.7927 - val_loss: 0.7278 - val_nd: 0.7727\n",
            "Epoch 39/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.9011 - nd: 0.7528 - val_loss: 0.6896 - val_nd: 0.7230\n",
            "Epoch 40/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.8687 - nd: 0.7066 - val_loss: 0.6439 - val_nd: 0.6642\n",
            "Epoch 41/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.8292 - nd: 0.6539 - val_loss: 0.5902 - val_nd: 0.5919\n",
            "Epoch 42/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.7919 - nd: 0.6028 - val_loss: 0.5432 - val_nd: 0.5605\n",
            "Epoch 43/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.7517 - nd: 0.5526 - val_loss: 0.5022 - val_nd: 0.5099\n",
            "Epoch 44/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.7176 - nd: 0.5175 - val_loss: 0.4671 - val_nd: 0.4892\n",
            "Epoch 45/200\n",
            "18/18 [==============================] - 0s 8ms/step - loss: 0.6873 - nd: 0.5048 - val_loss: 0.4464 - val_nd: 0.4816\n",
            "Epoch 46/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.6738 - nd: 0.4883 - val_loss: 0.4294 - val_nd: 0.4683\n",
            "Epoch 47/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.6620 - nd: 0.4766 - val_loss: 0.4123 - val_nd: 0.4508\n",
            "Epoch 48/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.6393 - nd: 0.4788 - val_loss: 0.4053 - val_nd: 0.4604\n",
            "Epoch 49/200\n",
            "18/18 [==============================] - 0s 8ms/step - loss: 0.6318 - nd: 0.4644 - val_loss: 0.3956 - val_nd: 0.4623\n",
            "Epoch 50/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.6214 - nd: 0.4557 - val_loss: 0.3526 - val_nd: 0.3952\n",
            "Epoch 51/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.6105 - nd: 0.4320 - val_loss: 0.3473 - val_nd: 0.4142\n",
            "Epoch 52/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.5950 - nd: 0.4290 - val_loss: 0.3468 - val_nd: 0.3958\n",
            "Epoch 53/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.5750 - nd: 0.4085 - val_loss: 0.3099 - val_nd: 0.3851\n",
            "Epoch 54/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.5587 - nd: 0.3873 - val_loss: 0.2991 - val_nd: 0.3668\n",
            "Epoch 55/200\n",
            "18/18 [==============================] - 0s 8ms/step - loss: 0.5512 - nd: 0.3764 - val_loss: 0.2870 - val_nd: 0.3580\n",
            "Epoch 56/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.5431 - nd: 0.3697 - val_loss: 0.2843 - val_nd: 0.3512\n",
            "Epoch 57/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.5409 - nd: 0.3654 - val_loss: 0.2772 - val_nd: 0.3480\n",
            "Epoch 58/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5322 - nd: 0.3614 - val_loss: 0.2773 - val_nd: 0.3466\n",
            "Epoch 59/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.5255 - nd: 0.3578 - val_loss: 0.2726 - val_nd: 0.3474\n",
            "Epoch 60/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.5346 - nd: 0.3565 - val_loss: 0.2647 - val_nd: 0.3380\n",
            "Epoch 61/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5133 - nd: 0.3526 - val_loss: 0.2724 - val_nd: 0.3250\n",
            "Epoch 62/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.5229 - nd: 0.3503 - val_loss: 0.2470 - val_nd: 0.3128\n",
            "Epoch 63/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.4995 - nd: 0.3413 - val_loss: 0.2423 - val_nd: 0.2973\n",
            "Epoch 64/200\n",
            "18/18 [==============================] - 0s 8ms/step - loss: 0.4946 - nd: 0.3381 - val_loss: 0.2406 - val_nd: 0.2858\n",
            "Epoch 65/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.4970 - nd: 0.3353 - val_loss: 0.2327 - val_nd: 0.2764\n",
            "Epoch 66/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.4954 - nd: 0.3252 - val_loss: 0.2292 - val_nd: 0.2729\n",
            "Epoch 67/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.4959 - nd: 0.3287 - val_loss: 0.2148 - val_nd: 0.2686\n",
            "Epoch 68/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.4640 - nd: 0.3147 - val_loss: 0.2068 - val_nd: 0.2513\n",
            "Epoch 69/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4716 - nd: 0.3062 - val_loss: 0.2156 - val_nd: 0.2502\n",
            "Epoch 70/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4489 - nd: 0.3015 - val_loss: 0.2162 - val_nd: 0.2487\n",
            "Epoch 71/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4658 - nd: 0.2998 - val_loss: 0.2094 - val_nd: 0.2470\n",
            "Epoch 72/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4476 - nd: 0.2923 - val_loss: 0.2200 - val_nd: 0.2548\n",
            "Epoch 73/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4492 - nd: 0.2931 - val_loss: 0.2119 - val_nd: 0.2480\n",
            "Epoch 74/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4528 - nd: 0.2894 - val_loss: 0.2072 - val_nd: 0.2393\n",
            "Epoch 75/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4517 - nd: 0.2913 - val_loss: 0.2132 - val_nd: 0.2452\n",
            "Epoch 76/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.4337 - nd: 0.2912 - val_loss: 0.2082 - val_nd: 0.2320\n",
            "Epoch 77/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4294 - nd: 0.2916 - val_loss: 0.2342 - val_nd: 0.2549\n",
            "Epoch 78/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.4398 - nd: 0.2995 - val_loss: 0.2189 - val_nd: 0.2413\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 61778\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                   _step 78\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                    loss 0.4398095905780792\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                   epoch 77\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                _runtime 6166.32000207901\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              _timestamp 1650248542.5475845\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                  val_nd 0.24132829904556274\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                      nd 0.2995416522026062\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                val_loss 0.2188955694437027\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           best_val_loss 0.2067880779504776\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              best_epoch 67\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              best-nd_95 0.0022463529056100887\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       best-nrmse_maxmin 0.03689007998330145\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    best-nrmse_maxmin_95 3.7105631083988816e-05\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 best-nd 0.30987115871096604\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   best-nrmse_maxmin_std 0.0006310165944906507\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     best-nrmse_mean_std 0.006047151944435419\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      best-nrmse_mean_95 0.00035559031429302745\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         best-nrmse_mean 0.3535246471314549\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             best-nd_std 0.038201370495862697\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 7 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced iconic-wind-622: https://app.wandb.ai/matintavakoli/Interpreting_TS/runs/w12wz5b5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.9.7\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Wandb version 0.12.14 is available!  To upgrade, please run:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in wandb/run-20220418_022227-i8s01fyx\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33miconic-universe-623\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://app.wandb.ai/matintavakoli/Interpreting_TS\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://app.wandb.ai/matintavakoli/Interpreting_TS/runs/i8s01fyx\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb off` to turn off syncing.\n",
            "\n",
            "/usr/local/envs/eval-metric/lib/python3.7/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  return f(**kwargs)\n",
            "/usr/local/envs/eval-metric/lib/python3.7/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  return f(**kwargs)\n",
            "  4%|█▍                                     | 101/2660 [00:00<00:10, 235.21it/s]\n",
            "(9090, 30, 10) (9090, 6) (1111, 30, 10) (1111, 6)\n",
            "(9090, 30, 10) (9090, 6) (1111, 30, 10) (1111, 6)\n",
            "Model: \"sequential_73\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm_72 (LSTM)               (None, 30, 16)            1728      \n",
            "_________________________________________________________________\n",
            "lstm_73 (LSTM)               (None, 30, 16)            2112      \n",
            "_________________________________________________________________\n",
            "lstm_74 (LSTM)               (None, 30, 16)            2112      \n",
            "_________________________________________________________________\n",
            "lstm_75 (LSTM)               (None, 16)                2112      \n",
            "_________________________________________________________________\n",
            "dropout_112 (Dropout)        (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_149 (Dense)            (None, 6)                 102       \n",
            "=================================================================\n",
            "Total params: 8,166\n",
            "Trainable params: 8,166\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/200\n",
            "18/18 [==============================] - 1s 69ms/step - loss: 0.9974 - nd: 0.9056 - val_loss: 0.8092 - val_nd: 0.9335\n",
            "Epoch 2/200\n",
            "18/18 [==============================] - 0s 14ms/step - loss: 0.9474 - nd: 0.8200 - val_loss: 0.7292 - val_nd: 0.7480\n",
            "Epoch 3/200\n",
            "18/18 [==============================] - 0s 15ms/step - loss: 0.8448 - nd: 0.6468 - val_loss: 0.5591 - val_nd: 0.5247\n",
            "Epoch 4/200\n",
            "18/18 [==============================] - 0s 13ms/step - loss: 0.7344 - nd: 0.5240 - val_loss: 0.4549 - val_nd: 0.4629\n",
            "Epoch 5/200\n",
            "18/18 [==============================] - 0s 14ms/step - loss: 0.6659 - nd: 0.5041 - val_loss: 0.4200 - val_nd: 0.4621\n",
            "Epoch 6/200\n",
            "18/18 [==============================] - 0s 13ms/step - loss: 0.6274 - nd: 0.4761 - val_loss: 0.3868 - val_nd: 0.4177\n",
            "Epoch 7/200\n",
            "18/18 [==============================] - 0s 14ms/step - loss: 0.5983 - nd: 0.4266 - val_loss: 0.3395 - val_nd: 0.3703\n",
            "Epoch 8/200\n",
            "18/18 [==============================] - 0s 12ms/step - loss: 0.5514 - nd: 0.3891 - val_loss: 0.2803 - val_nd: 0.3051\n",
            "Epoch 9/200\n",
            "18/18 [==============================] - 0s 14ms/step - loss: 0.5248 - nd: 0.3578 - val_loss: 0.2319 - val_nd: 0.2765\n",
            "Epoch 10/200\n",
            "18/18 [==============================] - 0s 14ms/step - loss: 0.4811 - nd: 0.3261 - val_loss: 0.2122 - val_nd: 0.2540\n",
            "Epoch 11/200\n",
            "18/18 [==============================] - 0s 11ms/step - loss: 0.4687 - nd: 0.3073 - val_loss: 0.2202 - val_nd: 0.2566\n",
            "Epoch 12/200\n",
            "18/18 [==============================] - 0s 12ms/step - loss: 0.4552 - nd: 0.2895 - val_loss: 0.2234 - val_nd: 0.2440\n",
            "Epoch 13/200\n",
            "18/18 [==============================] - 0s 11ms/step - loss: 0.4464 - nd: 0.2704 - val_loss: 0.2149 - val_nd: 0.2347\n",
            "Epoch 14/200\n",
            "18/18 [==============================] - 0s 14ms/step - loss: 0.4408 - nd: 0.2590 - val_loss: 0.2004 - val_nd: 0.2120\n",
            "Epoch 15/200\n",
            "18/18 [==============================] - 0s 12ms/step - loss: 0.4374 - nd: 0.2692 - val_loss: 0.2153 - val_nd: 0.2219\n",
            "Epoch 16/200\n",
            "18/18 [==============================] - 0s 12ms/step - loss: 0.4240 - nd: 0.2527 - val_loss: 0.2119 - val_nd: 0.2155\n",
            "Epoch 17/200\n",
            "18/18 [==============================] - 0s 11ms/step - loss: 0.4158 - nd: 0.2371 - val_loss: 0.2006 - val_nd: 0.1933\n",
            "Epoch 18/200\n",
            "18/18 [==============================] - 0s 11ms/step - loss: 0.4068 - nd: 0.2402 - val_loss: 0.2411 - val_nd: 0.2584\n",
            "Epoch 19/200\n",
            "18/18 [==============================] - 0s 11ms/step - loss: 0.4130 - nd: 0.2349 - val_loss: 0.2044 - val_nd: 0.1933\n",
            "Epoch 20/200\n",
            "18/18 [==============================] - 0s 12ms/step - loss: 0.3968 - nd: 0.2213 - val_loss: 0.2102 - val_nd: 0.2022\n",
            "Epoch 21/200\n",
            "18/18 [==============================] - 0s 11ms/step - loss: 0.4056 - nd: 0.2205 - val_loss: 0.2081 - val_nd: 0.2033\n",
            "Epoch 22/200\n",
            "18/18 [==============================] - 0s 11ms/step - loss: 0.3990 - nd: 0.2151 - val_loss: 0.2130 - val_nd: 0.1937\n",
            "Epoch 23/200\n",
            "18/18 [==============================] - 0s 14ms/step - loss: 0.3884 - nd: 0.2171 - val_loss: 0.1973 - val_nd: 0.1792\n",
            "Epoch 24/200\n",
            "18/18 [==============================] - 0s 11ms/step - loss: 0.4003 - nd: 0.2103 - val_loss: 0.2115 - val_nd: 0.1903\n",
            "Epoch 25/200\n",
            "18/18 [==============================] - 0s 12ms/step - loss: 0.3895 - nd: 0.2147 - val_loss: 0.2218 - val_nd: 0.2143\n",
            "Epoch 26/200\n",
            "18/18 [==============================] - 0s 12ms/step - loss: 0.4135 - nd: 0.2180 - val_loss: 0.2052 - val_nd: 0.1738\n",
            "Epoch 27/200\n",
            "18/18 [==============================] - 0s 11ms/step - loss: 0.3999 - nd: 0.2091 - val_loss: 0.2083 - val_nd: 0.1862\n",
            "Epoch 28/200\n",
            "18/18 [==============================] - 0s 11ms/step - loss: 0.3910 - nd: 0.2056 - val_loss: 0.2037 - val_nd: 0.1832\n",
            "Epoch 29/200\n",
            "18/18 [==============================] - 0s 12ms/step - loss: 0.3919 - nd: 0.2040 - val_loss: 0.2025 - val_nd: 0.1693\n",
            "Epoch 30/200\n",
            "18/18 [==============================] - 0s 13ms/step - loss: 0.3869 - nd: 0.2007 - val_loss: 0.2141 - val_nd: 0.2039\n",
            "Epoch 31/200\n",
            "18/18 [==============================] - 0s 11ms/step - loss: 0.3940 - nd: 0.2048 - val_loss: 0.2151 - val_nd: 0.1946\n",
            "Epoch 32/200\n",
            "18/18 [==============================] - 0s 12ms/step - loss: 0.4084 - nd: 0.2372 - val_loss: 0.2140 - val_nd: 0.1851\n",
            "Epoch 33/200\n",
            "18/18 [==============================] - 0s 12ms/step - loss: 0.4577 - nd: 0.3539 - val_loss: 0.3149 - val_nd: 0.3298\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 62069\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                   _step 33\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                    loss 0.4577474594116211\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                   epoch 32\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                _runtime 6187.9587326049805\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              _timestamp 1650248564.186315\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                  val_nd 0.32979539036750793\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                      nd 0.35392338037490845\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                val_loss 0.31491321325302124\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           best_val_loss 0.1972961723804474\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              best_epoch 22\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              best-nd_95 0.0019050238432457116\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       best-nrmse_maxmin 0.03472577358569937\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    best-nrmse_maxmin_95 0.0008120703746785749\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 best-nd 0.23552016159206554\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   best-nrmse_maxmin_std 0.013810030104501738\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     best-nrmse_mean_std 0.1323441429722761\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      best-nrmse_mean_95 0.007782224727734488\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         best-nrmse_mean 0.3327836876148865\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             best-nd_std 0.03239674472231548\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 7 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced iconic-universe-623: https://app.wandb.ai/matintavakoli/Interpreting_TS/runs/i8s01fyx\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.9.7\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Wandb version 0.12.14 is available!  To upgrade, please run:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in wandb/run-20220418_022249-3a958o38\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33msilver-pyramid-624\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://app.wandb.ai/matintavakoli/Interpreting_TS\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://app.wandb.ai/matintavakoli/Interpreting_TS/runs/3a958o38\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb off` to turn off syncing.\n",
            "\n",
            "/usr/local/envs/eval-metric/lib/python3.7/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  return f(**kwargs)\n",
            "/usr/local/envs/eval-metric/lib/python3.7/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  return f(**kwargs)\n",
            "  4%|█▍                                     | 101/2660 [00:00<00:11, 231.68it/s]\n",
            "(9090, 30, 10) (9090, 6) (1111, 30, 10) (1111, 6)\n",
            "(9090, 30, 10) (9090, 6) (1111, 30, 10) (1111, 6)\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 62229\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              _timestamp 1650248944.4060216\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                   _step 2\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                    loss 0.003946073567348792\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                      nd 0.09894343010216634\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                _runtime 6568.17843914032\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                  val_nd 0.09407629104522935\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                val_loss 0.01933580865754916\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              best-nd_95 0.0006795921239328122\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       best-nrmse_maxmin 0.019103508273548325\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    best-nrmse_maxmin_95 0.00020295777544750978\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 best-nd 0.11144859980481403\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   best-nrmse_maxmin_std 0.0034514902602895835\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     best-nrmse_mean_std 0.03307628709124117\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      best-nrmse_mean_95 0.001944982932542958\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         best-nrmse_mean 0.1830725502475475\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             best-nd_std 0.011557111283623859\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 8 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced silver-pyramid-624: https://app.wandb.ai/matintavakoli/Interpreting_TS/runs/3a958o38\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.9.7\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Wandb version 0.12.14 is available!  To upgrade, please run:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in wandb/run-20220418_022908-3svjtkzr\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mvital-dawn-625\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://app.wandb.ai/matintavakoli/Interpreting_TS\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://app.wandb.ai/matintavakoli/Interpreting_TS/runs/3svjtkzr\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb off` to turn off syncing.\n",
            "\n",
            "/usr/local/envs/eval-metric/lib/python3.7/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  return f(**kwargs)\n",
            "/usr/local/envs/eval-metric/lib/python3.7/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  return f(**kwargs)\n",
            "  4%|█▍                                     | 101/2660 [00:00<00:11, 223.75it/s]\n",
            "(9090, 30, 10) (9090, 6) (1111, 30, 10) (1111, 6)\n",
            "(9090, 30, 10) (9090, 6) (1111, 30, 10) (1111, 6)\n",
            "Model: \"sequential_74\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "reshape_37 (Reshape)         (None, 300)               0         \n",
            "_________________________________________________________________\n",
            "dense_150 (Dense)            (None, 16)                4816      \n",
            "_________________________________________________________________\n",
            "dropout_113 (Dropout)        (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_151 (Dense)            (None, 16)                272       \n",
            "_________________________________________________________________\n",
            "dropout_114 (Dropout)        (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_152 (Dense)            (None, 16)                272       \n",
            "_________________________________________________________________\n",
            "dropout_115 (Dropout)        (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_153 (Dense)            (None, 16)                272       \n",
            "_________________________________________________________________\n",
            "dropout_116 (Dropout)        (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_154 (Dense)            (None, 6)                 102       \n",
            "=================================================================\n",
            "Total params: 5,734\n",
            "Trainable params: 5,734\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/200\n",
            "18/18 [==============================] - 0s 11ms/step - loss: 1.1471 - nd: 1.0937 - val_loss: 0.8848 - val_nd: 0.9342\n",
            "Epoch 2/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 1.0771 - nd: 0.9941 - val_loss: 0.8436 - val_nd: 0.9176\n",
            "Epoch 3/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 1.0572 - nd: 0.9704 - val_loss: 0.8315 - val_nd: 0.9196\n",
            "Epoch 4/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 1.0391 - nd: 0.9581 - val_loss: 0.8286 - val_nd: 0.9187\n",
            "Epoch 5/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 1.0334 - nd: 0.9450 - val_loss: 0.8279 - val_nd: 0.9176\n",
            "Epoch 6/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 1.0311 - nd: 0.9407 - val_loss: 0.8278 - val_nd: 0.9175\n",
            "Epoch 7/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 1.0300 - nd: 0.9328 - val_loss: 0.8277 - val_nd: 0.9169\n",
            "Epoch 8/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 1.0302 - nd: 0.9301 - val_loss: 0.8275 - val_nd: 0.9156\n",
            "Epoch 9/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 1.0258 - nd: 0.9245 - val_loss: 0.8276 - val_nd: 0.9173\n",
            "Epoch 10/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 1.0240 - nd: 0.9220 - val_loss: 0.8276 - val_nd: 0.9164\n",
            "Epoch 11/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 1.0212 - nd: 0.9191 - val_loss: 0.8277 - val_nd: 0.9182\n",
            "Epoch 12/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 1.0145 - nd: 0.9148 - val_loss: 0.8275 - val_nd: 0.9162\n",
            "Epoch 13/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 1.0161 - nd: 0.9113 - val_loss: 0.8276 - val_nd: 0.9168\n",
            "Epoch 14/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 1.0196 - nd: 0.9127 - val_loss: 0.8279 - val_nd: 0.9200\n",
            "Epoch 15/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 1.0140 - nd: 0.9104 - val_loss: 0.8277 - val_nd: 0.9182\n",
            "Epoch 16/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 1.0052 - nd: 0.9069 - val_loss: 0.8276 - val_nd: 0.9178\n",
            "Epoch 17/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 1.0028 - nd: 0.9043 - val_loss: 0.8274 - val_nd: 0.9169\n",
            "Epoch 18/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 1.0061 - nd: 0.9018 - val_loss: 0.8271 - val_nd: 0.9161\n",
            "Epoch 19/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 1.0089 - nd: 0.8982 - val_loss: 0.8268 - val_nd: 0.9146\n",
            "Epoch 20/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 1.0066 - nd: 0.9018 - val_loss: 0.8270 - val_nd: 0.9176\n",
            "Epoch 21/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 1.0071 - nd: 0.9019 - val_loss: 0.8269 - val_nd: 0.9181\n",
            "Epoch 22/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 1.0062 - nd: 0.9022 - val_loss: 0.8266 - val_nd: 0.9171\n",
            "Epoch 23/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 1.0085 - nd: 0.8988 - val_loss: 0.8262 - val_nd: 0.9143\n",
            "Epoch 24/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 1.0095 - nd: 0.8979 - val_loss: 0.8263 - val_nd: 0.9166\n",
            "Epoch 25/200\n",
            "18/18 [==============================] - 0s 8ms/step - loss: 1.0057 - nd: 0.8979 - val_loss: 0.8259 - val_nd: 0.9157\n",
            "Epoch 26/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 1.0024 - nd: 0.8963 - val_loss: 0.8256 - val_nd: 0.9147\n",
            "Epoch 27/200\n",
            "18/18 [==============================] - 0s 8ms/step - loss: 1.0072 - nd: 0.8952 - val_loss: 0.8252 - val_nd: 0.9126\n",
            "Epoch 28/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 1.0048 - nd: 0.8940 - val_loss: 0.8248 - val_nd: 0.9124\n",
            "Epoch 29/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 1.0084 - nd: 0.8941 - val_loss: 0.8239 - val_nd: 0.9113\n",
            "Epoch 30/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 1.0005 - nd: 0.8906 - val_loss: 0.8228 - val_nd: 0.9082\n",
            "Epoch 31/200\n",
            "18/18 [==============================] - 0s 8ms/step - loss: 1.0060 - nd: 0.8870 - val_loss: 0.8218 - val_nd: 0.9061\n",
            "Epoch 32/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 1.0056 - nd: 0.8838 - val_loss: 0.8188 - val_nd: 0.9008\n",
            "Epoch 33/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.9994 - nd: 0.8790 - val_loss: 0.8159 - val_nd: 0.8952\n",
            "Epoch 34/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.9951 - nd: 0.8718 - val_loss: 0.8115 - val_nd: 0.8846\n",
            "Epoch 35/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.9916 - nd: 0.8590 - val_loss: 0.8051 - val_nd: 0.8707\n",
            "Epoch 36/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.9892 - nd: 0.8494 - val_loss: 0.7942 - val_nd: 0.8523\n",
            "Epoch 37/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.9747 - nd: 0.8296 - val_loss: 0.7775 - val_nd: 0.8249\n",
            "Epoch 38/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.9645 - nd: 0.8071 - val_loss: 0.7499 - val_nd: 0.7772\n",
            "Epoch 39/200\n",
            "18/18 [==============================] - 0s 8ms/step - loss: 0.9379 - nd: 0.7620 - val_loss: 0.7077 - val_nd: 0.7037\n",
            "Epoch 40/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.9156 - nd: 0.7178 - val_loss: 0.6615 - val_nd: 0.6320\n",
            "Epoch 41/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.8865 - nd: 0.6871 - val_loss: 0.6147 - val_nd: 0.5547\n",
            "Epoch 42/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.8578 - nd: 0.6698 - val_loss: 0.5745 - val_nd: 0.5120\n",
            "Epoch 43/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.8432 - nd: 0.6510 - val_loss: 0.5402 - val_nd: 0.4738\n",
            "Epoch 44/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.8309 - nd: 0.6350 - val_loss: 0.5146 - val_nd: 0.4616\n",
            "Epoch 45/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.8092 - nd: 0.6252 - val_loss: 0.4911 - val_nd: 0.4541\n",
            "Epoch 46/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.8003 - nd: 0.6122 - val_loss: 0.4715 - val_nd: 0.4186\n",
            "Epoch 47/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.7877 - nd: 0.5963 - val_loss: 0.4542 - val_nd: 0.4165\n",
            "Epoch 48/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.7762 - nd: 0.5898 - val_loss: 0.4345 - val_nd: 0.3900\n",
            "Epoch 49/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.7635 - nd: 0.5805 - val_loss: 0.4275 - val_nd: 0.3978\n",
            "Epoch 50/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.7630 - nd: 0.5789 - val_loss: 0.4108 - val_nd: 0.3829\n",
            "Epoch 51/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.7364 - nd: 0.5663 - val_loss: 0.4027 - val_nd: 0.3824\n",
            "Epoch 52/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.7415 - nd: 0.5534 - val_loss: 0.3934 - val_nd: 0.3794\n",
            "Epoch 53/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.7436 - nd: 0.5614 - val_loss: 0.3989 - val_nd: 0.4009\n",
            "Epoch 54/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.7503 - nd: 0.5688 - val_loss: 0.3851 - val_nd: 0.4078\n",
            "Epoch 55/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.7302 - nd: 0.5495 - val_loss: 0.3701 - val_nd: 0.3640\n",
            "Epoch 56/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.7122 - nd: 0.5429 - val_loss: 0.3707 - val_nd: 0.3750\n",
            "Epoch 57/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.7204 - nd: 0.5389 - val_loss: 0.3682 - val_nd: 0.3819\n",
            "Epoch 58/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.7006 - nd: 0.5353 - val_loss: 0.3566 - val_nd: 0.3708\n",
            "Epoch 59/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.7204 - nd: 0.5406 - val_loss: 0.3501 - val_nd: 0.3705\n",
            "Epoch 60/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.6908 - nd: 0.5354 - val_loss: 0.3457 - val_nd: 0.3654\n",
            "Epoch 61/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.7070 - nd: 0.5325 - val_loss: 0.3403 - val_nd: 0.3673\n",
            "Epoch 62/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.6976 - nd: 0.5412 - val_loss: 0.3307 - val_nd: 0.3542\n",
            "Epoch 63/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.6926 - nd: 0.5323 - val_loss: 0.3308 - val_nd: 0.3607\n",
            "Epoch 64/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.7000 - nd: 0.5301 - val_loss: 0.3283 - val_nd: 0.3649\n",
            "Epoch 65/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.6977 - nd: 0.5283 - val_loss: 0.3358 - val_nd: 0.3719\n",
            "Epoch 66/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.6954 - nd: 0.5339 - val_loss: 0.3244 - val_nd: 0.3584\n",
            "Epoch 67/200\n",
            "18/18 [==============================] - 0s 8ms/step - loss: 0.6908 - nd: 0.5289 - val_loss: 0.3222 - val_nd: 0.3560\n",
            "Epoch 68/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.6756 - nd: 0.5222 - val_loss: 0.3264 - val_nd: 0.3638\n",
            "Epoch 69/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.6865 - nd: 0.5245 - val_loss: 0.3136 - val_nd: 0.3442\n",
            "Epoch 70/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.6914 - nd: 0.5221 - val_loss: 0.3160 - val_nd: 0.3472\n",
            "Epoch 71/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.6809 - nd: 0.5211 - val_loss: 0.3210 - val_nd: 0.3547\n",
            "Epoch 72/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.6948 - nd: 0.5239 - val_loss: 0.3232 - val_nd: 0.3551\n",
            "Epoch 73/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.6708 - nd: 0.5068 - val_loss: 0.3187 - val_nd: 0.3547\n",
            "Epoch 74/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.6874 - nd: 0.5209 - val_loss: 0.3266 - val_nd: 0.3681\n",
            "Epoch 75/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.6819 - nd: 0.5232 - val_loss: 0.3219 - val_nd: 0.3515\n",
            "Epoch 76/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.6798 - nd: 0.5144 - val_loss: 0.3214 - val_nd: 0.3519\n",
            "Epoch 77/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.6751 - nd: 0.5137 - val_loss: 0.3100 - val_nd: 0.3273\n",
            "Epoch 78/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.6607 - nd: 0.5029 - val_loss: 0.3305 - val_nd: 0.3552\n",
            "Epoch 79/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.6672 - nd: 0.5076 - val_loss: 0.3266 - val_nd: 0.3483\n",
            "Epoch 80/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.6800 - nd: 0.5094 - val_loss: 0.3104 - val_nd: 0.3268\n",
            "Epoch 81/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.6481 - nd: 0.4927 - val_loss: 0.3048 - val_nd: 0.3154\n",
            "Epoch 82/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.6577 - nd: 0.4971 - val_loss: 0.2896 - val_nd: 0.2942\n",
            "Epoch 83/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.6603 - nd: 0.5022 - val_loss: 0.3069 - val_nd: 0.3151\n",
            "Epoch 84/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.6649 - nd: 0.4978 - val_loss: 0.2904 - val_nd: 0.2877\n",
            "Epoch 85/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.6703 - nd: 0.4982 - val_loss: 0.3055 - val_nd: 0.3071\n",
            "Epoch 86/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.6657 - nd: 0.4966 - val_loss: 0.3027 - val_nd: 0.3011\n",
            "Epoch 87/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.6667 - nd: 0.5041 - val_loss: 0.3028 - val_nd: 0.3101\n",
            "Epoch 88/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.6627 - nd: 0.5031 - val_loss: 0.3140 - val_nd: 0.3246\n",
            "Epoch 89/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.6440 - nd: 0.4943 - val_loss: 0.2862 - val_nd: 0.2961\n",
            "Epoch 90/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.6604 - nd: 0.4960 - val_loss: 0.3042 - val_nd: 0.3037\n",
            "Epoch 91/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.6530 - nd: 0.4890 - val_loss: 0.2822 - val_nd: 0.2624\n",
            "Epoch 92/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.6488 - nd: 0.4844 - val_loss: 0.2921 - val_nd: 0.2651\n",
            "Epoch 93/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.6439 - nd: 0.4786 - val_loss: 0.2802 - val_nd: 0.2613\n",
            "Epoch 94/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.6520 - nd: 0.4711 - val_loss: 0.2820 - val_nd: 0.2619\n",
            "Epoch 95/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.6318 - nd: 0.4699 - val_loss: 0.2820 - val_nd: 0.2559\n",
            "Epoch 96/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.6494 - nd: 0.4743 - val_loss: 0.2854 - val_nd: 0.2760\n",
            "Epoch 97/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.6632 - nd: 0.4837 - val_loss: 0.2714 - val_nd: 0.2492\n",
            "Epoch 98/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.6546 - nd: 0.4789 - val_loss: 0.2646 - val_nd: 0.2495\n",
            "Epoch 99/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.6398 - nd: 0.4687 - val_loss: 0.2806 - val_nd: 0.2587\n",
            "Epoch 100/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.6403 - nd: 0.4706 - val_loss: 0.2797 - val_nd: 0.2675\n",
            "Epoch 101/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.6458 - nd: 0.4753 - val_loss: 0.2778 - val_nd: 0.2463\n",
            "Epoch 102/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.6446 - nd: 0.4752 - val_loss: 0.2601 - val_nd: 0.2335\n",
            "Epoch 103/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.6415 - nd: 0.4685 - val_loss: 0.2842 - val_nd: 0.2573\n",
            "Epoch 104/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.6413 - nd: 0.4659 - val_loss: 0.2838 - val_nd: 0.2735\n",
            "Epoch 105/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.6325 - nd: 0.4573 - val_loss: 0.2629 - val_nd: 0.2386\n",
            "Epoch 106/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.6341 - nd: 0.4667 - val_loss: 0.2940 - val_nd: 0.3008\n",
            "Epoch 107/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.6438 - nd: 0.4713 - val_loss: 0.2718 - val_nd: 0.2527\n",
            "Epoch 108/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.6471 - nd: 0.4694 - val_loss: 0.2765 - val_nd: 0.2523\n",
            "Epoch 109/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.6296 - nd: 0.4575 - val_loss: 0.2674 - val_nd: 0.2410\n",
            "Epoch 110/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.6369 - nd: 0.4588 - val_loss: 0.2818 - val_nd: 0.2664\n",
            "Epoch 111/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.6436 - nd: 0.4602 - val_loss: 0.2756 - val_nd: 0.2375\n",
            "Epoch 112/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.6382 - nd: 0.4626 - val_loss: 0.2845 - val_nd: 0.2490\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 62311\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                   _step 112\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                    loss 0.6382156610488892\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                   epoch 111\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                _runtime 6587.522121429443\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              _timestamp 1650248963.749704\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                  val_nd 0.24901354312896729\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                      nd 0.4626443684101105\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                val_loss 0.2844516932964325\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           best_val_loss 0.26009854674339294\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              best_epoch 101\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              best-nd_95 0.0013789411722781846\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       best-nrmse_maxmin 0.04503644811513127\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    best-nrmse_maxmin_95 0.0007718302517818156\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 best-nd 0.2805710330611359\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   best-nrmse_maxmin_std 0.013125708491571266\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     best-nrmse_mean_std 0.12578615890595843\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      best-nrmse_mean_95 0.007396595982715699\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         best-nrmse_mean 0.4315928410879791\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             best-nd_std 0.02345020788258177\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 7 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced vital-dawn-625: https://app.wandb.ai/matintavakoli/Interpreting_TS/runs/3svjtkzr\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.9.7\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Wandb version 0.12.14 is available!  To upgrade, please run:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in wandb/run-20220418_022932-6kcasexy\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33matomic-yogurt-626\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://app.wandb.ai/matintavakoli/Interpreting_TS\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://app.wandb.ai/matintavakoli/Interpreting_TS/runs/6kcasexy\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb off` to turn off syncing.\n",
            "\n",
            "/usr/local/envs/eval-metric/lib/python3.7/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  return f(**kwargs)\n",
            "/usr/local/envs/eval-metric/lib/python3.7/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  return f(**kwargs)\n",
            "  4%|█▍                                     | 101/2660 [00:00<00:11, 232.55it/s]\n",
            "(9090, 30, 10) (9090, 6) (1111, 30, 10) (1111, 6)\n",
            "(9090, 30, 10) (9090, 6) (1111, 30, 10) (1111, 6)\n",
            "Model: \"sequential_75\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm_76 (LSTM)               (None, 30, 16)            1728      \n",
            "_________________________________________________________________\n",
            "lstm_77 (LSTM)               (None, 30, 16)            2112      \n",
            "_________________________________________________________________\n",
            "lstm_78 (LSTM)               (None, 30, 16)            2112      \n",
            "_________________________________________________________________\n",
            "lstm_79 (LSTM)               (None, 16)                2112      \n",
            "_________________________________________________________________\n",
            "dropout_117 (Dropout)        (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_155 (Dense)            (None, 6)                 102       \n",
            "=================================================================\n",
            "Total params: 8,166\n",
            "Trainable params: 8,166\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/200\n",
            "18/18 [==============================] - 1s 70ms/step - loss: 1.0027 - nd: 0.9326 - val_loss: 0.8003 - val_nd: 0.9044\n",
            "Epoch 2/200\n",
            "18/18 [==============================] - 0s 14ms/step - loss: 0.9385 - nd: 0.8104 - val_loss: 0.6899 - val_nd: 0.6668\n",
            "Epoch 3/200\n",
            "18/18 [==============================] - 0s 14ms/step - loss: 0.8502 - nd: 0.6922 - val_loss: 0.5928 - val_nd: 0.5877\n",
            "Epoch 4/200\n",
            "18/18 [==============================] - 0s 15ms/step - loss: 0.7617 - nd: 0.6134 - val_loss: 0.5480 - val_nd: 0.6366\n",
            "Epoch 5/200\n",
            "18/18 [==============================] - 0s 14ms/step - loss: 0.6918 - nd: 0.5480 - val_loss: 0.5250 - val_nd: 0.6230\n",
            "Epoch 6/200\n",
            "18/18 [==============================] - 0s 14ms/step - loss: 0.6445 - nd: 0.4986 - val_loss: 0.5105 - val_nd: 0.6354\n",
            "Epoch 7/200\n",
            "18/18 [==============================] - 0s 15ms/step - loss: 0.5979 - nd: 0.4500 - val_loss: 0.4333 - val_nd: 0.5351\n",
            "Epoch 8/200\n",
            "18/18 [==============================] - 0s 14ms/step - loss: 0.5796 - nd: 0.4217 - val_loss: 0.3709 - val_nd: 0.4666\n",
            "Epoch 9/200\n",
            "18/18 [==============================] - 0s 14ms/step - loss: 0.5565 - nd: 0.4018 - val_loss: 0.3023 - val_nd: 0.3710\n",
            "Epoch 10/200\n",
            "18/18 [==============================] - 0s 14ms/step - loss: 0.5339 - nd: 0.3814 - val_loss: 0.2702 - val_nd: 0.3211\n",
            "Epoch 11/200\n",
            "18/18 [==============================] - 0s 14ms/step - loss: 0.5202 - nd: 0.3686 - val_loss: 0.2544 - val_nd: 0.2965\n",
            "Epoch 12/200\n",
            "18/18 [==============================] - 0s 15ms/step - loss: 0.5102 - nd: 0.3536 - val_loss: 0.2395 - val_nd: 0.2626\n",
            "Epoch 13/200\n",
            "18/18 [==============================] - 0s 14ms/step - loss: 0.4940 - nd: 0.3427 - val_loss: 0.2390 - val_nd: 0.2536\n",
            "Epoch 14/200\n",
            "18/18 [==============================] - 0s 10ms/step - loss: 0.5054 - nd: 0.3378 - val_loss: 0.2817 - val_nd: 0.3271\n",
            "Epoch 15/200\n",
            "18/18 [==============================] - 0s 12ms/step - loss: 0.4809 - nd: 0.3306 - val_loss: 0.2423 - val_nd: 0.2658\n",
            "Epoch 16/200\n",
            "18/18 [==============================] - 0s 12ms/step - loss: 0.4744 - nd: 0.3235 - val_loss: 0.2660 - val_nd: 0.2873\n",
            "Epoch 17/200\n",
            "18/18 [==============================] - 0s 14ms/step - loss: 0.4722 - nd: 0.3168 - val_loss: 0.2155 - val_nd: 0.2171\n",
            "Epoch 18/200\n",
            "18/18 [==============================] - 0s 13ms/step - loss: 0.4756 - nd: 0.3155 - val_loss: 0.2331 - val_nd: 0.2594\n",
            "Epoch 19/200\n",
            "18/18 [==============================] - 0s 13ms/step - loss: 0.4679 - nd: 0.3248 - val_loss: 0.2487 - val_nd: 0.2667\n",
            "Epoch 20/200\n",
            "18/18 [==============================] - 0s 15ms/step - loss: 0.4657 - nd: 0.3074 - val_loss: 0.2124 - val_nd: 0.2059\n",
            "Epoch 21/200\n",
            "18/18 [==============================] - 0s 12ms/step - loss: 0.4500 - nd: 0.3063 - val_loss: 0.2349 - val_nd: 0.2412\n",
            "Epoch 22/200\n",
            "18/18 [==============================] - 0s 12ms/step - loss: 0.4683 - nd: 0.3033 - val_loss: 0.2137 - val_nd: 0.2112\n",
            "Epoch 23/200\n",
            "18/18 [==============================] - 0s 14ms/step - loss: 0.4583 - nd: 0.2993 - val_loss: 0.2119 - val_nd: 0.2015\n",
            "Epoch 24/200\n",
            "18/18 [==============================] - 0s 12ms/step - loss: 0.4503 - nd: 0.2969 - val_loss: 0.2139 - val_nd: 0.2088\n",
            "Epoch 25/200\n",
            "18/18 [==============================] - 0s 12ms/step - loss: 0.4488 - nd: 0.2984 - val_loss: 0.2480 - val_nd: 0.2407\n",
            "Epoch 26/200\n",
            "18/18 [==============================] - 0s 12ms/step - loss: 0.4597 - nd: 0.3062 - val_loss: 0.2190 - val_nd: 0.2013\n",
            "Epoch 27/200\n",
            "18/18 [==============================] - 0s 13ms/step - loss: 0.4580 - nd: 0.2964 - val_loss: 0.2249 - val_nd: 0.2076\n",
            "Epoch 28/200\n",
            "18/18 [==============================] - 0s 12ms/step - loss: 0.4546 - nd: 0.2916 - val_loss: 0.2194 - val_nd: 0.2046\n",
            "Epoch 29/200\n",
            "18/18 [==============================] - 0s 11ms/step - loss: 0.4528 - nd: 0.2939 - val_loss: 0.2343 - val_nd: 0.2268\n",
            "Epoch 30/200\n",
            "18/18 [==============================] - 0s 12ms/step - loss: 0.4457 - nd: 0.2940 - val_loss: 0.2262 - val_nd: 0.1973\n",
            "Epoch 31/200\n",
            "18/18 [==============================] - 0s 11ms/step - loss: 0.4532 - nd: 0.2876 - val_loss: 0.2183 - val_nd: 0.1929\n",
            "Epoch 32/200\n",
            "18/18 [==============================] - 0s 12ms/step - loss: 0.4530 - nd: 0.2910 - val_loss: 0.2204 - val_nd: 0.1942\n",
            "Epoch 33/200\n",
            "18/18 [==============================] - 0s 11ms/step - loss: 0.4586 - nd: 0.2901 - val_loss: 0.2294 - val_nd: 0.2103\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 62702\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                   _step 33\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                    loss 0.45859789848327637\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                   epoch 32\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                _runtime 6614.5643355846405\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              _timestamp 1650248990.791918\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                  val_nd 0.21033348143100739\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                      nd 0.2900700569152832\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                val_loss 0.22935687005519867\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           best_val_loss 0.2119424194097519\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              best_epoch 22\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              best-nd_95 0.002420758468140747\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       best-nrmse_maxmin 0.03619714236211863\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    best-nrmse_maxmin_95 0.0008225245583334565\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 best-nd 0.24407341051736775\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   best-nrmse_maxmin_std 0.013987813453696136\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     best-nrmse_mean_std 0.1340478745938461\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      best-nrmse_mean_95 0.007882409156429477\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         best-nrmse_mean 0.34688409422065464\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             best-nd_std 0.041167303183524497\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 7 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced atomic-yogurt-626: https://app.wandb.ai/matintavakoli/Interpreting_TS/runs/6kcasexy\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.9.7\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Wandb version 0.12.14 is available!  To upgrade, please run:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in wandb/run-20220418_022955-3etiilt5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33munique-night-627\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://app.wandb.ai/matintavakoli/Interpreting_TS\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://app.wandb.ai/matintavakoli/Interpreting_TS/runs/3etiilt5\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb off` to turn off syncing.\n",
            "\n",
            "/usr/local/envs/eval-metric/lib/python3.7/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  return f(**kwargs)\n",
            "/usr/local/envs/eval-metric/lib/python3.7/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  return f(**kwargs)\n",
            "  4%|█▍                                     | 101/2660 [00:00<00:11, 231.60it/s]\n",
            "(9090, 30, 10) (9090, 6) (1111, 30, 10) (1111, 6)\n",
            "(9090, 30, 10) (9090, 6) (1111, 30, 10) (1111, 6)\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 62856\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              _timestamp 1650249367.06018\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                   _step 2\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                    loss 0.003963266789991376\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                      nd 0.09898032989723443\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                _runtime 6990.832597494125\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                  val_nd 0.09360211996866684\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                val_loss 0.01913442992971974\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              best-nd_95 0.0006967121335456656\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       best-nrmse_maxmin 0.01894819066276385\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    best-nrmse_maxmin_95 0.00018392537229407243\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 best-nd 0.1108517235361908\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   best-nrmse_maxmin_std 0.0031278261189717606\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     best-nrmse_mean_std 0.029974552115326708\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      best-nrmse_mean_95 0.0017625917961743693\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         best-nrmse_mean 0.18158411206658567\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             best-nd_std 0.011848253351497527\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 8 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced unique-night-627: https://app.wandb.ai/matintavakoli/Interpreting_TS/runs/3etiilt5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.9.7\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Wandb version 0.12.14 is available!  To upgrade, please run:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in wandb/run-20220418_023610-16av8dgi\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mradiant-dawn-628\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://app.wandb.ai/matintavakoli/Interpreting_TS\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://app.wandb.ai/matintavakoli/Interpreting_TS/runs/16av8dgi\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb off` to turn off syncing.\n",
            "\n",
            "/usr/local/envs/eval-metric/lib/python3.7/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  return f(**kwargs)\n",
            "/usr/local/envs/eval-metric/lib/python3.7/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  return f(**kwargs)\n",
            "  4%|█▍                                     | 101/2660 [00:00<00:10, 242.90it/s]\n",
            "(9090, 30, 10) (9090, 6) (1111, 30, 10) (1111, 6)\n",
            "(9090, 30, 10) (9090, 6) (1111, 30, 10) (1111, 6)\n",
            "Model: \"sequential_76\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "reshape_38 (Reshape)         (None, 300)               0         \n",
            "_________________________________________________________________\n",
            "dense_156 (Dense)            (None, 16)                4816      \n",
            "_________________________________________________________________\n",
            "dropout_118 (Dropout)        (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_157 (Dense)            (None, 16)                272       \n",
            "_________________________________________________________________\n",
            "dropout_119 (Dropout)        (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_158 (Dense)            (None, 16)                272       \n",
            "_________________________________________________________________\n",
            "dropout_120 (Dropout)        (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_159 (Dense)            (None, 16)                272       \n",
            "_________________________________________________________________\n",
            "dropout_121 (Dropout)        (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_160 (Dense)            (None, 6)                 102       \n",
            "=================================================================\n",
            "Total params: 5,734\n",
            "Trainable params: 5,734\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/200\n",
            "18/18 [==============================] - 0s 11ms/step - loss: 1.3915 - nd: 1.6184 - val_loss: 1.0396 - val_nd: 1.2796\n",
            "Epoch 2/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 1.2801 - nd: 1.4405 - val_loss: 0.9541 - val_nd: 1.1641\n",
            "Epoch 3/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 1.2100 - nd: 1.3128 - val_loss: 0.9034 - val_nd: 1.0884\n",
            "Epoch 4/200\n",
            "18/18 [==============================] - 0s 8ms/step - loss: 1.1579 - nd: 1.2239 - val_loss: 0.8743 - val_nd: 1.0404\n",
            "Epoch 5/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 1.1219 - nd: 1.1571 - val_loss: 0.8564 - val_nd: 1.0058\n",
            "Epoch 6/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 1.0992 - nd: 1.1076 - val_loss: 0.8459 - val_nd: 0.9830\n",
            "Epoch 7/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 1.0732 - nd: 1.0674 - val_loss: 0.8399 - val_nd: 0.9683\n",
            "Epoch 8/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 1.0665 - nd: 1.0388 - val_loss: 0.8363 - val_nd: 0.9586\n",
            "Epoch 9/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 1.0585 - nd: 1.0187 - val_loss: 0.8340 - val_nd: 0.9514\n",
            "Epoch 10/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 1.0536 - nd: 1.0015 - val_loss: 0.8325 - val_nd: 0.9453\n",
            "Epoch 11/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 1.0398 - nd: 0.9818 - val_loss: 0.8314 - val_nd: 0.9411\n",
            "Epoch 12/200\n",
            "18/18 [==============================] - 0s 8ms/step - loss: 1.0423 - nd: 0.9745 - val_loss: 0.8308 - val_nd: 0.9384\n",
            "Epoch 13/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 1.0314 - nd: 0.9603 - val_loss: 0.8302 - val_nd: 0.9356\n",
            "Epoch 14/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 1.0253 - nd: 0.9511 - val_loss: 0.8295 - val_nd: 0.9311\n",
            "Epoch 15/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 1.0255 - nd: 0.9444 - val_loss: 0.8292 - val_nd: 0.9297\n",
            "Epoch 16/200\n",
            "18/18 [==============================] - 0s 8ms/step - loss: 1.0242 - nd: 0.9390 - val_loss: 0.8290 - val_nd: 0.9282\n",
            "Epoch 17/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 1.0209 - nd: 0.9338 - val_loss: 0.8287 - val_nd: 0.9258\n",
            "Epoch 18/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 1.0233 - nd: 0.9296 - val_loss: 0.8286 - val_nd: 0.9253\n",
            "Epoch 19/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 1.0220 - nd: 0.9258 - val_loss: 0.8284 - val_nd: 0.9244\n",
            "Epoch 20/200\n",
            "18/18 [==============================] - 0s 8ms/step - loss: 1.0146 - nd: 0.9230 - val_loss: 0.8283 - val_nd: 0.9232\n",
            "Epoch 21/200\n",
            "18/18 [==============================] - 0s 8ms/step - loss: 1.0127 - nd: 0.9185 - val_loss: 0.8281 - val_nd: 0.9215\n",
            "Epoch 22/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 1.0128 - nd: 0.9163 - val_loss: 0.8279 - val_nd: 0.9201\n",
            "Epoch 23/200\n",
            "18/18 [==============================] - 0s 8ms/step - loss: 1.0096 - nd: 0.9123 - val_loss: 0.8277 - val_nd: 0.9187\n",
            "Epoch 24/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 1.0054 - nd: 0.9087 - val_loss: 0.8276 - val_nd: 0.9172\n",
            "Epoch 25/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 1.0131 - nd: 0.9065 - val_loss: 0.8276 - val_nd: 0.9172\n",
            "Epoch 26/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 1.0123 - nd: 0.9060 - val_loss: 0.8275 - val_nd: 0.9165\n",
            "Epoch 27/200\n",
            "18/18 [==============================] - 0s 8ms/step - loss: 1.0103 - nd: 0.9049 - val_loss: 0.8275 - val_nd: 0.9164\n",
            "Epoch 28/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 1.0049 - nd: 0.9017 - val_loss: 0.8274 - val_nd: 0.9150\n",
            "Epoch 29/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 1.0076 - nd: 0.8997 - val_loss: 0.8273 - val_nd: 0.9142\n",
            "Epoch 30/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 1.0080 - nd: 0.8998 - val_loss: 0.8272 - val_nd: 0.9137\n",
            "Epoch 31/200\n",
            "18/18 [==============================] - 0s 8ms/step - loss: 1.0081 - nd: 0.8972 - val_loss: 0.8272 - val_nd: 0.9134\n",
            "Epoch 32/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 1.0075 - nd: 0.8985 - val_loss: 0.8272 - val_nd: 0.9133\n",
            "Epoch 33/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 1.0112 - nd: 0.8976 - val_loss: 0.8272 - val_nd: 0.9130\n",
            "Epoch 34/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 1.0055 - nd: 0.8957 - val_loss: 0.8271 - val_nd: 0.9124\n",
            "Epoch 35/200\n",
            "18/18 [==============================] - 0s 8ms/step - loss: 1.0048 - nd: 0.8950 - val_loss: 0.8271 - val_nd: 0.9119\n",
            "Epoch 36/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 1.0047 - nd: 0.8935 - val_loss: 0.8271 - val_nd: 0.9116\n",
            "Epoch 37/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 1.0015 - nd: 0.8928 - val_loss: 0.8270 - val_nd: 0.9112\n",
            "Epoch 38/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.9996 - nd: 0.8931 - val_loss: 0.8270 - val_nd: 0.9110\n",
            "Epoch 39/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 1.0073 - nd: 0.8918 - val_loss: 0.8269 - val_nd: 0.9101\n",
            "Epoch 40/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 1.0016 - nd: 0.8910 - val_loss: 0.8269 - val_nd: 0.9102\n",
            "Epoch 41/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 1.0083 - nd: 0.8914 - val_loss: 0.8270 - val_nd: 0.9109\n",
            "Epoch 42/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 1.0036 - nd: 0.8921 - val_loss: 0.8270 - val_nd: 0.9106\n",
            "Epoch 43/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.9987 - nd: 0.8906 - val_loss: 0.8269 - val_nd: 0.9101\n",
            "Epoch 44/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 1.0051 - nd: 0.8910 - val_loss: 0.8269 - val_nd: 0.9100\n",
            "Epoch 45/200\n",
            "18/18 [==============================] - 0s 8ms/step - loss: 1.0067 - nd: 0.8889 - val_loss: 0.8269 - val_nd: 0.9098\n",
            "Epoch 46/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 1.0034 - nd: 0.8892 - val_loss: 0.8269 - val_nd: 0.9099\n",
            "Epoch 47/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 1.0081 - nd: 0.8898 - val_loss: 0.8269 - val_nd: 0.9096\n",
            "Epoch 48/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 1.0071 - nd: 0.8894 - val_loss: 0.8269 - val_nd: 0.9098\n",
            "Epoch 49/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 1.0060 - nd: 0.8893 - val_loss: 0.8269 - val_nd: 0.9100\n",
            "Epoch 50/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 1.0074 - nd: 0.8887 - val_loss: 0.8269 - val_nd: 0.9099\n",
            "Epoch 51/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 1.0093 - nd: 0.8890 - val_loss: 0.8270 - val_nd: 0.9105\n",
            "Epoch 52/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 1.0064 - nd: 0.8898 - val_loss: 0.8269 - val_nd: 0.9101\n",
            "Epoch 53/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 1.0013 - nd: 0.8888 - val_loss: 0.8269 - val_nd: 0.9100\n",
            "Epoch 54/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 1.0024 - nd: 0.8882 - val_loss: 0.8269 - val_nd: 0.9098\n",
            "Epoch 55/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 1.0053 - nd: 0.8884 - val_loss: 0.8269 - val_nd: 0.9098\n",
            "Epoch 56/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 1.0056 - nd: 0.8886 - val_loss: 0.8269 - val_nd: 0.9094\n",
            "Epoch 57/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.9996 - nd: 0.8864 - val_loss: 0.8268 - val_nd: 0.9091\n",
            "Epoch 58/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 1.0008 - nd: 0.8880 - val_loss: 0.8269 - val_nd: 0.9096\n",
            "Epoch 59/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 1.0055 - nd: 0.8880 - val_loss: 0.8269 - val_nd: 0.9099\n",
            "Epoch 60/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 1.0066 - nd: 0.8883 - val_loss: 0.8269 - val_nd: 0.9098\n",
            "Epoch 61/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 1.0039 - nd: 0.8879 - val_loss: 0.8269 - val_nd: 0.9098\n",
            "Epoch 62/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 1.0055 - nd: 0.8877 - val_loss: 0.8269 - val_nd: 0.9092\n",
            "Epoch 63/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 1.0036 - nd: 0.8866 - val_loss: 0.8269 - val_nd: 0.9093\n",
            "Epoch 64/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 1.0095 - nd: 0.8882 - val_loss: 0.8269 - val_nd: 0.9102\n",
            "Epoch 65/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 1.0042 - nd: 0.8881 - val_loss: 0.8269 - val_nd: 0.9101\n",
            "Epoch 66/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 1.0030 - nd: 0.8883 - val_loss: 0.8269 - val_nd: 0.9105\n",
            "Epoch 67/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.9993 - nd: 0.8883 - val_loss: 0.8270 - val_nd: 0.9108\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 62900\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                   _step 67\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                    loss 0.9993311166763306\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                   epoch 66\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                _runtime 7004.993824481964\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              _timestamp 1650249381.221407\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                  val_nd 0.9108224511146545\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                      nd 0.8883383274078369\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                val_loss 0.8269702196121216\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           best_val_loss 0.8268401026725769\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              best_epoch 56\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              best-nd_95 0.0019250821384598472\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       best-nrmse_maxmin 0.13911483711656952\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    best-nrmse_maxmin_95 0.0021421721739798977\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 best-nd 0.9839453926313004\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   best-nrmse_maxmin_std 0.036429677936961724\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     best-nrmse_mean_std 0.3491125268258191\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      best-nrmse_mean_95 0.02052884304517276\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         best-nrmse_mean 1.3331639217005484\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             best-nd_std 0.032737855135143726\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 7 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced radiant-dawn-628: https://app.wandb.ai/matintavakoli/Interpreting_TS/runs/16av8dgi\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.9.7\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Wandb version 0.12.14 is available!  To upgrade, please run:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in wandb/run-20220418_023626-128l952u\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33msoft-river-629\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://app.wandb.ai/matintavakoli/Interpreting_TS\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://app.wandb.ai/matintavakoli/Interpreting_TS/runs/128l952u\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb off` to turn off syncing.\n",
            "\n",
            "/usr/local/envs/eval-metric/lib/python3.7/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  return f(**kwargs)\n",
            "/usr/local/envs/eval-metric/lib/python3.7/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  return f(**kwargs)\n",
            "  4%|█▍                                     | 101/2660 [00:00<00:11, 231.99it/s]\n",
            "(9090, 30, 10) (9090, 6) (1111, 30, 10) (1111, 6)\n",
            "(9090, 30, 10) (9090, 6) (1111, 30, 10) (1111, 6)\n",
            "Model: \"sequential_77\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm_80 (LSTM)               (None, 30, 16)            1728      \n",
            "_________________________________________________________________\n",
            "lstm_81 (LSTM)               (None, 30, 16)            2112      \n",
            "_________________________________________________________________\n",
            "lstm_82 (LSTM)               (None, 30, 16)            2112      \n",
            "_________________________________________________________________\n",
            "lstm_83 (LSTM)               (None, 16)                2112      \n",
            "_________________________________________________________________\n",
            "dropout_122 (Dropout)        (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_161 (Dense)            (None, 6)                 102       \n",
            "=================================================================\n",
            "Total params: 8,166\n",
            "Trainable params: 8,166\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/200\n",
            "18/18 [==============================] - 1s 70ms/step - loss: 1.0079 - nd: 0.9743 - val_loss: 0.8275 - val_nd: 0.9503\n",
            "Epoch 2/200\n",
            "18/18 [==============================] - 0s 13ms/step - loss: 0.9975 - nd: 0.9095 - val_loss: 0.8082 - val_nd: 0.8749\n",
            "Epoch 3/200\n",
            "18/18 [==============================] - 0s 15ms/step - loss: 0.9682 - nd: 0.8583 - val_loss: 0.7850 - val_nd: 0.7539\n",
            "Epoch 4/200\n",
            "18/18 [==============================] - 0s 14ms/step - loss: 0.8877 - nd: 0.7484 - val_loss: 0.6101 - val_nd: 0.6585\n",
            "Epoch 5/200\n",
            "18/18 [==============================] - 0s 13ms/step - loss: 0.7863 - nd: 0.6501 - val_loss: 0.6076 - val_nd: 0.6957\n",
            "Epoch 6/200\n",
            "18/18 [==============================] - 0s 11ms/step - loss: 0.7372 - nd: 0.5968 - val_loss: 1.0151 - val_nd: 1.1401\n",
            "Epoch 7/200\n",
            "18/18 [==============================] - 0s 12ms/step - loss: 0.6934 - nd: 0.5698 - val_loss: 1.0379 - val_nd: 1.1391\n",
            "Epoch 8/200\n",
            "18/18 [==============================] - 0s 12ms/step - loss: 0.6952 - nd: 0.5445 - val_loss: 1.1668 - val_nd: 1.2859\n",
            "Epoch 9/200\n",
            "18/18 [==============================] - 0s 12ms/step - loss: 0.6567 - nd: 0.5272 - val_loss: 1.1442 - val_nd: 1.2343\n",
            "Epoch 10/200\n",
            "18/18 [==============================] - 0s 12ms/step - loss: 0.6432 - nd: 0.5169 - val_loss: 0.9283 - val_nd: 0.9738\n",
            "Epoch 11/200\n",
            "18/18 [==============================] - 0s 12ms/step - loss: 0.6171 - nd: 0.4894 - val_loss: 0.8651 - val_nd: 0.8972\n",
            "Epoch 12/200\n",
            "18/18 [==============================] - 0s 11ms/step - loss: 0.6268 - nd: 0.4908 - val_loss: 0.6757 - val_nd: 0.6667\n",
            "Epoch 13/200\n",
            "18/18 [==============================] - 0s 15ms/step - loss: 0.6089 - nd: 0.4695 - val_loss: 0.5728 - val_nd: 0.5242\n",
            "Epoch 14/200\n",
            "18/18 [==============================] - 0s 14ms/step - loss: 0.5817 - nd: 0.4483 - val_loss: 0.5141 - val_nd: 0.4754\n",
            "Epoch 15/200\n",
            "18/18 [==============================] - 0s 15ms/step - loss: 0.5954 - nd: 0.4432 - val_loss: 0.4141 - val_nd: 0.3998\n",
            "Epoch 16/200\n",
            "18/18 [==============================] - 0s 12ms/step - loss: 0.5649 - nd: 0.4287 - val_loss: 0.5140 - val_nd: 0.4361\n",
            "Epoch 17/200\n",
            "18/18 [==============================] - 0s 14ms/step - loss: 0.5654 - nd: 0.4251 - val_loss: 0.2496 - val_nd: 0.2788\n",
            "Epoch 18/200\n",
            "18/18 [==============================] - 0s 12ms/step - loss: 0.5501 - nd: 0.4128 - val_loss: 0.4200 - val_nd: 0.4207\n",
            "Epoch 19/200\n",
            "18/18 [==============================] - 0s 12ms/step - loss: 0.5568 - nd: 0.4208 - val_loss: 0.3147 - val_nd: 0.3208\n",
            "Epoch 20/200\n",
            "18/18 [==============================] - 0s 11ms/step - loss: 0.5389 - nd: 0.4035 - val_loss: 0.3290 - val_nd: 0.3172\n",
            "Epoch 21/200\n",
            "18/18 [==============================] - 0s 12ms/step - loss: 0.5330 - nd: 0.4053 - val_loss: 0.3682 - val_nd: 0.3495\n",
            "Epoch 22/200\n",
            "18/18 [==============================] - 0s 11ms/step - loss: 0.5326 - nd: 0.4024 - val_loss: 0.2948 - val_nd: 0.2896\n",
            "Epoch 23/200\n",
            "18/18 [==============================] - 0s 11ms/step - loss: 0.5356 - nd: 0.4012 - val_loss: 0.2953 - val_nd: 0.2976\n",
            "Epoch 24/200\n",
            "18/18 [==============================] - 0s 13ms/step - loss: 0.5323 - nd: 0.3933 - val_loss: 0.3025 - val_nd: 0.2751\n",
            "Epoch 25/200\n",
            "18/18 [==============================] - 0s 12ms/step - loss: 0.5368 - nd: 0.3936 - val_loss: 0.2722 - val_nd: 0.2518\n",
            "Epoch 26/200\n",
            "18/18 [==============================] - 0s 11ms/step - loss: 0.5178 - nd: 0.3860 - val_loss: 0.3804 - val_nd: 0.3469\n",
            "Epoch 27/200\n",
            "18/18 [==============================] - 0s 12ms/step - loss: 0.5366 - nd: 0.3882 - val_loss: 0.2696 - val_nd: 0.2393\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 63156\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                   _step 27\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                    loss 0.5365976691246033\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                   epoch 26\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                _runtime 7025.590732097626\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              _timestamp 1650249401.8183146\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                  val_nd 0.23925769329071045\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                      nd 0.38824617862701416\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                val_loss 0.2695530652999878\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           best_val_loss 0.24959903955459595\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              best_epoch 16\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              best-nd_95 0.004660000918150408\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       best-nrmse_maxmin 0.04476974576255362\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    best-nrmse_maxmin_95 0.00024001136001545914\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 best-nd 0.3509267415456706\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   best-nrmse_maxmin_std 0.004081621754207979\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     best-nrmse_mean_std 0.039114956948686726\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      best-nrmse_mean_95 0.0023000744751817755\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         best-nrmse_mean 0.42903698175867366\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             best-nd_std 0.07924775361019061\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 7 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced soft-river-629: https://app.wandb.ai/matintavakoli/Interpreting_TS/runs/128l952u\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.9.7\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Wandb version 0.12.14 is available!  To upgrade, please run:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in wandb/run-20220418_023647-1rql7zbh\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mcharmed-totem-630\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://app.wandb.ai/matintavakoli/Interpreting_TS\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://app.wandb.ai/matintavakoli/Interpreting_TS/runs/1rql7zbh\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb off` to turn off syncing.\n",
            "\n",
            "/usr/local/envs/eval-metric/lib/python3.7/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  return f(**kwargs)\n",
            "/usr/local/envs/eval-metric/lib/python3.7/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  return f(**kwargs)\n",
            "  4%|█▍                                     | 101/2660 [00:00<00:11, 232.15it/s]\n",
            "(9090, 30, 10) (9090, 6) (1111, 30, 10) (1111, 6)\n",
            "(9090, 30, 10) (9090, 6) (1111, 30, 10) (1111, 6)\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 63292\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              _timestamp 1650249781.2844367\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                   _step 2\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                    loss 0.003994813638794581\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                      nd 0.09961624791644837\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                _runtime 7405.056854248047\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                  val_nd 0.0949247767033201\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                val_loss 0.019402988942754346\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              best-nd_95 0.0007159742940933509\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       best-nrmse_maxmin 0.01921242600019734\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    best-nrmse_maxmin_95 0.00018988148968238972\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 best-nd 0.1124716141461086\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   best-nrmse_maxmin_std 0.003229115567526222\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     best-nrmse_mean_std 0.03094522815003752\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      best-nrmse_mean_95 0.0018196704010169588\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         best-nrmse_mean 0.18411632952092893\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             best-nd_std 0.012175824736116217\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 8 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced charmed-totem-630: https://app.wandb.ai/matintavakoli/Interpreting_TS/runs/1rql7zbh\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.9.7\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Wandb version 0.12.14 is available!  To upgrade, please run:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in wandb/run-20220418_024305-3gzq71zs\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mgraceful-flower-631\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://app.wandb.ai/matintavakoli/Interpreting_TS\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://app.wandb.ai/matintavakoli/Interpreting_TS/runs/3gzq71zs\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb off` to turn off syncing.\n",
            "\n",
            "/usr/local/envs/eval-metric/lib/python3.7/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  return f(**kwargs)\n",
            "/usr/local/envs/eval-metric/lib/python3.7/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  return f(**kwargs)\n",
            "  4%|█▍                                     | 101/2660 [00:00<00:11, 232.45it/s]\n",
            "(9090, 30, 10) (9090, 6) (1111, 30, 10) (1111, 6)\n",
            "(9090, 30, 10) (9090, 6) (1111, 30, 10) (1111, 6)\n",
            "Model: \"sequential_78\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "reshape_39 (Reshape)         (None, 300)               0         \n",
            "_________________________________________________________________\n",
            "dense_162 (Dense)            (None, 32)                9632      \n",
            "_________________________________________________________________\n",
            "dropout_123 (Dropout)        (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense_163 (Dense)            (None, 32)                1056      \n",
            "_________________________________________________________________\n",
            "dropout_124 (Dropout)        (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense_164 (Dense)            (None, 32)                1056      \n",
            "_________________________________________________________________\n",
            "dropout_125 (Dropout)        (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense_165 (Dense)            (None, 32)                1056      \n",
            "_________________________________________________________________\n",
            "dropout_126 (Dropout)        (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense_166 (Dense)            (None, 6)                 198       \n",
            "=================================================================\n",
            "Total params: 12,998\n",
            "Trainable params: 12,998\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/200\n",
            "18/18 [==============================] - 0s 11ms/step - loss: 1.2355 - nd: 1.3684 - val_loss: 0.9668 - val_nd: 1.1624\n",
            "Epoch 2/200\n",
            "18/18 [==============================] - 0s 8ms/step - loss: 1.0590 - nd: 1.0753 - val_loss: 0.8471 - val_nd: 0.9706\n",
            "Epoch 3/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 1.0053 - nd: 0.9130 - val_loss: 0.8266 - val_nd: 0.9094\n",
            "Epoch 4/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 1.0027 - nd: 0.8806 - val_loss: 0.8259 - val_nd: 0.9020\n",
            "Epoch 5/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.9985 - nd: 0.8799 - val_loss: 0.8254 - val_nd: 0.9056\n",
            "Epoch 6/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 1.0023 - nd: 0.8792 - val_loss: 0.8250 - val_nd: 0.9095\n",
            "Epoch 7/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.9993 - nd: 0.8807 - val_loss: 0.8239 - val_nd: 0.9074\n",
            "Epoch 8/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 1.0007 - nd: 0.8860 - val_loss: 0.8227 - val_nd: 0.9096\n",
            "Epoch 9/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.9960 - nd: 0.8751 - val_loss: 0.8198 - val_nd: 0.9002\n",
            "Epoch 10/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.9927 - nd: 0.8759 - val_loss: 0.8158 - val_nd: 0.8961\n",
            "Epoch 11/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.9909 - nd: 0.8670 - val_loss: 0.8085 - val_nd: 0.8877\n",
            "Epoch 12/200\n",
            "18/18 [==============================] - 0s 8ms/step - loss: 0.9783 - nd: 0.8533 - val_loss: 0.7927 - val_nd: 0.8595\n",
            "Epoch 13/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.9658 - nd: 0.8279 - val_loss: 0.7607 - val_nd: 0.8050\n",
            "Epoch 14/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.9215 - nd: 0.7616 - val_loss: 0.7017 - val_nd: 0.7108\n",
            "Epoch 15/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.8556 - nd: 0.6597 - val_loss: 0.6076 - val_nd: 0.5771\n",
            "Epoch 16/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.7723 - nd: 0.5431 - val_loss: 0.4965 - val_nd: 0.4518\n",
            "Epoch 17/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.6913 - nd: 0.4476 - val_loss: 0.4078 - val_nd: 0.3863\n",
            "Epoch 18/200\n",
            "18/18 [==============================] - 0s 8ms/step - loss: 0.6257 - nd: 0.3892 - val_loss: 0.3442 - val_nd: 0.3220\n",
            "Epoch 19/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.5979 - nd: 0.3595 - val_loss: 0.2942 - val_nd: 0.2808\n",
            "Epoch 20/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.5659 - nd: 0.3362 - val_loss: 0.2683 - val_nd: 0.2636\n",
            "Epoch 21/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.5488 - nd: 0.3190 - val_loss: 0.2638 - val_nd: 0.2700\n",
            "Epoch 22/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.5535 - nd: 0.3137 - val_loss: 0.2434 - val_nd: 0.2699\n",
            "Epoch 23/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.5132 - nd: 0.3003 - val_loss: 0.2309 - val_nd: 0.2444\n",
            "Epoch 24/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.5027 - nd: 0.2934 - val_loss: 0.2228 - val_nd: 0.2302\n",
            "Epoch 25/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4868 - nd: 0.2937 - val_loss: 0.2390 - val_nd: 0.2619\n",
            "Epoch 26/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4955 - nd: 0.2943 - val_loss: 0.2243 - val_nd: 0.2353\n",
            "Epoch 27/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.4713 - nd: 0.2680 - val_loss: 0.1991 - val_nd: 0.2102\n",
            "Epoch 28/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.4637 - nd: 0.2661 - val_loss: 0.1857 - val_nd: 0.1968\n",
            "Epoch 29/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4579 - nd: 0.2562 - val_loss: 0.1879 - val_nd: 0.1970\n",
            "Epoch 30/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.4650 - nd: 0.2584 - val_loss: 0.1854 - val_nd: 0.2007\n",
            "Epoch 31/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.4379 - nd: 0.2568 - val_loss: 0.1753 - val_nd: 0.1965\n",
            "Epoch 32/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.4497 - nd: 0.2473 - val_loss: 0.1719 - val_nd: 0.1908\n",
            "Epoch 33/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4293 - nd: 0.2420 - val_loss: 0.1807 - val_nd: 0.1918\n",
            "Epoch 34/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4273 - nd: 0.2382 - val_loss: 0.1726 - val_nd: 0.1814\n",
            "Epoch 35/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4310 - nd: 0.2414 - val_loss: 0.1968 - val_nd: 0.2028\n",
            "Epoch 36/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4277 - nd: 0.2440 - val_loss: 0.1867 - val_nd: 0.2050\n",
            "Epoch 37/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4194 - nd: 0.2399 - val_loss: 0.1748 - val_nd: 0.1812\n",
            "Epoch 38/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4160 - nd: 0.2312 - val_loss: 0.1816 - val_nd: 0.1836\n",
            "Epoch 39/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4271 - nd: 0.2362 - val_loss: 0.1898 - val_nd: 0.1891\n",
            "Epoch 40/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4287 - nd: 0.2278 - val_loss: 0.1762 - val_nd: 0.1780\n",
            "Epoch 41/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4275 - nd: 0.2293 - val_loss: 0.1812 - val_nd: 0.1756\n",
            "Epoch 42/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4160 - nd: 0.2291 - val_loss: 0.1984 - val_nd: 0.1938\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 63336\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                   _step 42\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                    loss 0.4160107970237732\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                   epoch 41\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                _runtime 7416.438029289246\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              _timestamp 1650249792.6656117\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                  val_nd 0.19377286732196808\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                      nd 0.22909535467624664\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                val_loss 0.19840538501739502\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           best_val_loss 0.17192620038986206\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              best_epoch 31\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              best-nd_95 0.0015423836206893081\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       best-nrmse_maxmin 0.030627681113849326\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    best-nrmse_maxmin_95 0.0001585546259485295\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 best-nd 0.2330237264145749\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   best-nrmse_maxmin_std 0.002696372415289607\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     best-nrmse_mean_std 0.02583984927876914\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      best-nrmse_mean_95 0.001519459112446667\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         best-nrmse_mean 0.2935108886489136\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             best-nd_std 0.026229702373812887\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 7 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced graceful-flower-631: https://app.wandb.ai/matintavakoli/Interpreting_TS/runs/3gzq71zs\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.9.7\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Wandb version 0.12.14 is available!  To upgrade, please run:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in wandb/run-20220418_024317-11srp7qa\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33msmart-glade-632\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://app.wandb.ai/matintavakoli/Interpreting_TS\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://app.wandb.ai/matintavakoli/Interpreting_TS/runs/11srp7qa\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb off` to turn off syncing.\n",
            "\n",
            "/usr/local/envs/eval-metric/lib/python3.7/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  return f(**kwargs)\n",
            "/usr/local/envs/eval-metric/lib/python3.7/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  return f(**kwargs)\n",
            "  4%|█▍                                     | 101/2660 [00:00<00:11, 217.42it/s]\n",
            "(9090, 30, 10) (9090, 6) (1111, 30, 10) (1111, 6)\n",
            "(9090, 30, 10) (9090, 6) (1111, 30, 10) (1111, 6)\n",
            "Model: \"sequential_79\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm_84 (LSTM)               (None, 30, 32)            5504      \n",
            "_________________________________________________________________\n",
            "lstm_85 (LSTM)               (None, 30, 32)            8320      \n",
            "_________________________________________________________________\n",
            "lstm_86 (LSTM)               (None, 30, 32)            8320      \n",
            "_________________________________________________________________\n",
            "lstm_87 (LSTM)               (None, 32)                8320      \n",
            "_________________________________________________________________\n",
            "dropout_127 (Dropout)        (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense_167 (Dense)            (None, 6)                 198       \n",
            "=================================================================\n",
            "Total params: 30,662\n",
            "Trainable params: 30,662\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/200\n",
            "18/18 [==============================] - 1s 70ms/step - loss: 0.9736 - nd: 0.8440 - val_loss: 0.7493 - val_nd: 0.7994\n",
            "Epoch 2/200\n",
            "18/18 [==============================] - 0s 17ms/step - loss: 0.8485 - nd: 0.6716 - val_loss: 0.4739 - val_nd: 0.4673\n",
            "Epoch 3/200\n",
            "18/18 [==============================] - 0s 15ms/step - loss: 0.6339 - nd: 0.4650 - val_loss: 0.3656 - val_nd: 0.3874\n",
            "Epoch 4/200\n",
            "18/18 [==============================] - 0s 15ms/step - loss: 0.5615 - nd: 0.4133 - val_loss: 0.3406 - val_nd: 0.3701\n",
            "Epoch 5/200\n",
            "18/18 [==============================] - 0s 14ms/step - loss: 0.5227 - nd: 0.3850 - val_loss: 0.2770 - val_nd: 0.3128\n",
            "Epoch 6/200\n",
            "18/18 [==============================] - 0s 11ms/step - loss: 0.4739 - nd: 0.3228 - val_loss: 0.3261 - val_nd: 0.3578\n",
            "Epoch 7/200\n",
            "18/18 [==============================] - 0s 13ms/step - loss: 0.4644 - nd: 0.3263 - val_loss: 0.2401 - val_nd: 0.2697\n",
            "Epoch 8/200\n",
            "18/18 [==============================] - 0s 14ms/step - loss: 0.4342 - nd: 0.2936 - val_loss: 0.2287 - val_nd: 0.2471\n",
            "Epoch 9/200\n",
            "18/18 [==============================] - 0s 14ms/step - loss: 0.4290 - nd: 0.2757 - val_loss: 0.2240 - val_nd: 0.2302\n",
            "Epoch 10/200\n",
            "18/18 [==============================] - 0s 11ms/step - loss: 0.4264 - nd: 0.2599 - val_loss: 0.2377 - val_nd: 0.2454\n",
            "Epoch 11/200\n",
            "18/18 [==============================] - 0s 11ms/step - loss: 0.4325 - nd: 0.2610 - val_loss: 0.2262 - val_nd: 0.2253\n",
            "Epoch 12/200\n",
            "18/18 [==============================] - 0s 14ms/step - loss: 0.4185 - nd: 0.2465 - val_loss: 0.2179 - val_nd: 0.2049\n",
            "Epoch 13/200\n",
            "18/18 [==============================] - 0s 11ms/step - loss: 0.4223 - nd: 0.2406 - val_loss: 0.2294 - val_nd: 0.2205\n",
            "Epoch 14/200\n",
            "18/18 [==============================] - 0s 11ms/step - loss: 0.4052 - nd: 0.2339 - val_loss: 0.2280 - val_nd: 0.2225\n",
            "Epoch 15/200\n",
            "18/18 [==============================] - 0s 13ms/step - loss: 0.4097 - nd: 0.2367 - val_loss: 0.2171 - val_nd: 0.1966\n",
            "Epoch 16/200\n",
            "18/18 [==============================] - 0s 14ms/step - loss: 0.4117 - nd: 0.2361 - val_loss: 0.2156 - val_nd: 0.1991\n",
            "Epoch 17/200\n",
            "18/18 [==============================] - 0s 14ms/step - loss: 0.4084 - nd: 0.2230 - val_loss: 0.2114 - val_nd: 0.1883\n",
            "Epoch 18/200\n",
            "18/18 [==============================] - 0s 14ms/step - loss: 0.4114 - nd: 0.2168 - val_loss: 0.2105 - val_nd: 0.1803\n",
            "Epoch 19/200\n",
            "18/18 [==============================] - 0s 12ms/step - loss: 0.4120 - nd: 0.2119 - val_loss: 0.2050 - val_nd: 0.1753\n",
            "Epoch 20/200\n",
            "18/18 [==============================] - 0s 11ms/step - loss: 0.4136 - nd: 0.2109 - val_loss: 0.2128 - val_nd: 0.1817\n",
            "Epoch 21/200\n",
            "18/18 [==============================] - 0s 12ms/step - loss: 0.3986 - nd: 0.2185 - val_loss: 0.2083 - val_nd: 0.1816\n",
            "Epoch 22/200\n",
            "18/18 [==============================] - 0s 11ms/step - loss: 0.4009 - nd: 0.2044 - val_loss: 0.2081 - val_nd: 0.1807\n",
            "Epoch 23/200\n",
            "18/18 [==============================] - 0s 11ms/step - loss: 0.4073 - nd: 0.2061 - val_loss: 0.2170 - val_nd: 0.2010\n",
            "Epoch 24/200\n",
            "18/18 [==============================] - 0s 11ms/step - loss: 0.4300 - nd: 0.2598 - val_loss: 0.2238 - val_nd: 0.2177\n",
            "Epoch 25/200\n",
            "18/18 [==============================] - 0s 11ms/step - loss: 0.4004 - nd: 0.2345 - val_loss: 0.2299 - val_nd: 0.2026\n",
            "Epoch 26/200\n",
            "18/18 [==============================] - 0s 11ms/step - loss: 0.4005 - nd: 0.2304 - val_loss: 0.2114 - val_nd: 0.1745\n",
            "Epoch 27/200\n",
            "18/18 [==============================] - 0s 12ms/step - loss: 0.3954 - nd: 0.2040 - val_loss: 0.1962 - val_nd: 0.1663\n",
            "Epoch 28/200\n",
            "18/18 [==============================] - 0s 14ms/step - loss: 0.3926 - nd: 0.2001 - val_loss: 0.1929 - val_nd: 0.1583\n",
            "Epoch 29/200\n",
            "18/18 [==============================] - 0s 13ms/step - loss: 0.4018 - nd: 0.1985 - val_loss: 0.1979 - val_nd: 0.1604\n",
            "Epoch 30/200\n",
            "18/18 [==============================] - 0s 12ms/step - loss: 0.3836 - nd: 0.1966 - val_loss: 0.1985 - val_nd: 0.1593\n",
            "Epoch 31/200\n",
            "18/18 [==============================] - 0s 12ms/step - loss: 0.3938 - nd: 0.1947 - val_loss: 0.1976 - val_nd: 0.1652\n",
            "Epoch 32/200\n",
            "18/18 [==============================] - 0s 12ms/step - loss: 0.3870 - nd: 0.1974 - val_loss: 0.2118 - val_nd: 0.1904\n",
            "Epoch 33/200\n",
            "18/18 [==============================] - 0s 14ms/step - loss: 0.3858 - nd: 0.2060 - val_loss: 0.1904 - val_nd: 0.1541\n",
            "Epoch 34/200\n",
            "18/18 [==============================] - 0s 13ms/step - loss: 0.3871 - nd: 0.1912 - val_loss: 0.1957 - val_nd: 0.1634\n",
            "Epoch 35/200\n",
            "18/18 [==============================] - 0s 13ms/step - loss: 0.3798 - nd: 0.1909 - val_loss: 0.1881 - val_nd: 0.1521\n",
            "Epoch 36/200\n",
            "18/18 [==============================] - 0s 11ms/step - loss: 0.3868 - nd: 0.1919 - val_loss: 0.1885 - val_nd: 0.1494\n",
            "Epoch 37/200\n",
            "18/18 [==============================] - 0s 13ms/step - loss: 0.3941 - nd: 0.1836 - val_loss: 0.1855 - val_nd: 0.1420\n",
            "Epoch 38/200\n",
            "18/18 [==============================] - 0s 12ms/step - loss: 0.3792 - nd: 0.1826 - val_loss: 0.1921 - val_nd: 0.1519\n",
            "Epoch 39/200\n",
            "18/18 [==============================] - 0s 11ms/step - loss: 0.3940 - nd: 0.1919 - val_loss: 0.1886 - val_nd: 0.1479\n",
            "Epoch 40/200\n",
            "18/18 [==============================] - 0s 11ms/step - loss: 0.3909 - nd: 0.1842 - val_loss: 0.1924 - val_nd: 0.1573\n",
            "Epoch 41/200\n",
            "18/18 [==============================] - 0s 11ms/step - loss: 0.3917 - nd: 0.1939 - val_loss: 0.2032 - val_nd: 0.1669\n",
            "Epoch 42/200\n",
            "18/18 [==============================] - 0s 11ms/step - loss: 0.3987 - nd: 0.1951 - val_loss: 0.1935 - val_nd: 0.1565\n",
            "Epoch 43/200\n",
            "18/18 [==============================] - 0s 12ms/step - loss: 0.3897 - nd: 0.1867 - val_loss: 0.2037 - val_nd: 0.1750\n",
            "Epoch 44/200\n",
            "18/18 [==============================] - 0s 11ms/step - loss: 0.3908 - nd: 0.1892 - val_loss: 0.2194 - val_nd: 0.2103\n",
            "Epoch 45/200\n",
            "18/18 [==============================] - 0s 12ms/step - loss: 0.3862 - nd: 0.1976 - val_loss: 0.2853 - val_nd: 0.3131\n",
            "Epoch 46/200\n",
            "18/18 [==============================] - 0s 11ms/step - loss: 0.4091 - nd: 0.2419 - val_loss: 0.1886 - val_nd: 0.1587\n",
            "Epoch 47/200\n",
            "18/18 [==============================] - 0s 12ms/step - loss: 0.4018 - nd: 0.2140 - val_loss: 0.1906 - val_nd: 0.1447\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 63516\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                   _step 47\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                    loss 0.40179598331451416\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                   epoch 46\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                _runtime 7441.334000349045\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              _timestamp 1650249817.5615828\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                  val_nd 0.14473672211170197\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                      nd 0.21401071548461914\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                val_loss 0.19059252738952637\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           best_val_loss 0.18546804785728455\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              best_epoch 36\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              best-nd_95 0.002237248087368649\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       best-nrmse_maxmin 0.032376659503954096\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    best-nrmse_maxmin_95 0.0009396080428093059\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 best-nd 0.1898400390400446\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   best-nrmse_maxmin_std 0.015978929612799262\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     best-nrmse_mean_std 0.15312911914152122\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      best-nrmse_mean_95 0.00900444243889951\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         best-nrmse_mean 0.31027168094001756\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             best-nd_std 0.03804653439060512\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 7 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced smart-glade-632: https://app.wandb.ai/matintavakoli/Interpreting_TS/runs/11srp7qa\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.9.7\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Wandb version 0.12.14 is available!  To upgrade, please run:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in wandb/run-20220418_024342-1iwt17sy\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mdecent-water-633\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://app.wandb.ai/matintavakoli/Interpreting_TS\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://app.wandb.ai/matintavakoli/Interpreting_TS/runs/1iwt17sy\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb off` to turn off syncing.\n",
            "\n",
            "/usr/local/envs/eval-metric/lib/python3.7/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  return f(**kwargs)\n",
            "/usr/local/envs/eval-metric/lib/python3.7/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  return f(**kwargs)\n",
            "  4%|█▍                                     | 101/2660 [00:00<00:10, 235.27it/s]\n",
            "(9090, 30, 10) (9090, 6) (1111, 30, 10) (1111, 6)\n",
            "(9090, 30, 10) (9090, 6) (1111, 30, 10) (1111, 6)\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 63712\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              _timestamp 1650250194.8031304\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                   _step 2\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                    loss 0.004023915671601779\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                      nd 0.10006272627297372\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                _runtime 7818.5755479335785\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                  val_nd 0.09500427430813049\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                val_loss 0.019541464008951806\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              best-nd_95 0.0006327117706109935\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       best-nrmse_maxmin 0.01929601917260211\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    best-nrmse_maxmin_95 0.00020470443464235848\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 best-nd 0.11235463073828741\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   best-nrmse_maxmin_std 0.0034811938633458\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     best-nrmse_mean_std 0.03336094236425048\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      best-nrmse_mean_95 0.00196172149954518\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         best-nrmse_mean 0.18491741877826784\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             best-nd_std 0.010759866228427444\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 8 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced decent-water-633: https://app.wandb.ai/matintavakoli/Interpreting_TS/runs/1iwt17sy\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.9.7\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Wandb version 0.12.14 is available!  To upgrade, please run:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in wandb/run-20220418_025000-1yek8cez\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mdutiful-glade-634\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://app.wandb.ai/matintavakoli/Interpreting_TS\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://app.wandb.ai/matintavakoli/Interpreting_TS/runs/1yek8cez\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb off` to turn off syncing.\n",
            "\n",
            "/usr/local/envs/eval-metric/lib/python3.7/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  return f(**kwargs)\n",
            "/usr/local/envs/eval-metric/lib/python3.7/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  return f(**kwargs)\n",
            "  4%|█▍                                     | 101/2660 [00:00<00:10, 242.39it/s]\n",
            "(9090, 30, 10) (9090, 6) (1111, 30, 10) (1111, 6)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Error uploading \"config.yaml\": CommError, /tmp/tmpjgk3xcw6wandb/7kgopvyd-config.yaml is an empty file\n",
            "(9090, 30, 10) (9090, 6) (1111, 30, 10) (1111, 6)\n",
            "Model: \"sequential_80\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "reshape_40 (Reshape)         (None, 300)               0         \n",
            "_________________________________________________________________\n",
            "dense_168 (Dense)            (None, 32)                9632      \n",
            "_________________________________________________________________\n",
            "dropout_128 (Dropout)        (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense_169 (Dense)            (None, 32)                1056      \n",
            "_________________________________________________________________\n",
            "dropout_129 (Dropout)        (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense_170 (Dense)            (None, 32)                1056      \n",
            "_________________________________________________________________\n",
            "dropout_130 (Dropout)        (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense_171 (Dense)            (None, 32)                1056      \n",
            "_________________________________________________________________\n",
            "dropout_131 (Dropout)        (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense_172 (Dense)            (None, 6)                 198       \n",
            "=================================================================\n",
            "Total params: 12,998\n",
            "Trainable params: 12,998\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/200\n",
            "18/18 [==============================] - 0s 11ms/step - loss: 1.1442 - nd: 1.2086 - val_loss: 0.8601 - val_nd: 1.0027\n",
            "Epoch 2/200\n",
            "18/18 [==============================] - 0s 8ms/step - loss: 1.0632 - nd: 1.0154 - val_loss: 0.8274 - val_nd: 0.9172\n",
            "Epoch 3/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 1.0397 - nd: 0.9555 - val_loss: 0.8262 - val_nd: 0.9060\n",
            "Epoch 4/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 1.0375 - nd: 0.9404 - val_loss: 0.8259 - val_nd: 0.9115\n",
            "Epoch 5/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 1.0360 - nd: 0.9401 - val_loss: 0.8259 - val_nd: 0.9187\n",
            "Epoch 6/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 1.0235 - nd: 0.9356 - val_loss: 0.8242 - val_nd: 0.9133\n",
            "Epoch 7/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 1.0212 - nd: 0.9150 - val_loss: 0.8218 - val_nd: 0.9048\n",
            "Epoch 8/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 1.0174 - nd: 0.9126 - val_loss: 0.8196 - val_nd: 0.9055\n",
            "Epoch 9/200\n",
            "18/18 [==============================] - 0s 8ms/step - loss: 1.0150 - nd: 0.9111 - val_loss: 0.8161 - val_nd: 0.9013\n",
            "Epoch 10/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 1.0087 - nd: 0.8946 - val_loss: 0.8100 - val_nd: 0.8881\n",
            "Epoch 11/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.9990 - nd: 0.8815 - val_loss: 0.7997 - val_nd: 0.8708\n",
            "Epoch 12/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.9921 - nd: 0.8651 - val_loss: 0.7798 - val_nd: 0.8345\n",
            "Epoch 13/200\n",
            "18/18 [==============================] - 0s 8ms/step - loss: 0.9692 - nd: 0.8251 - val_loss: 0.7467 - val_nd: 0.7794\n",
            "Epoch 14/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.9434 - nd: 0.7861 - val_loss: 0.6913 - val_nd: 0.6842\n",
            "Epoch 15/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.8961 - nd: 0.7182 - val_loss: 0.6085 - val_nd: 0.5640\n",
            "Epoch 16/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.8327 - nd: 0.6616 - val_loss: 0.5156 - val_nd: 0.4563\n",
            "Epoch 17/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.7923 - nd: 0.6172 - val_loss: 0.4320 - val_nd: 0.3666\n",
            "Epoch 18/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.7401 - nd: 0.5828 - val_loss: 0.3743 - val_nd: 0.3352\n",
            "Epoch 19/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.7047 - nd: 0.5503 - val_loss: 0.3324 - val_nd: 0.3136\n",
            "Epoch 20/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.6793 - nd: 0.5265 - val_loss: 0.3116 - val_nd: 0.3082\n",
            "Epoch 21/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.6654 - nd: 0.5116 - val_loss: 0.2953 - val_nd: 0.2849\n",
            "Epoch 22/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.6481 - nd: 0.4918 - val_loss: 0.2645 - val_nd: 0.2544\n",
            "Epoch 23/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.6411 - nd: 0.4830 - val_loss: 0.2628 - val_nd: 0.2573\n",
            "Epoch 24/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.6175 - nd: 0.4663 - val_loss: 0.2547 - val_nd: 0.2485\n",
            "Epoch 25/200\n",
            "18/18 [==============================] - 0s 8ms/step - loss: 0.6137 - nd: 0.4593 - val_loss: 0.2320 - val_nd: 0.2301\n",
            "Epoch 26/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.5983 - nd: 0.4593 - val_loss: 0.2630 - val_nd: 0.2824\n",
            "Epoch 27/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.6065 - nd: 0.4496 - val_loss: 0.2294 - val_nd: 0.2316\n",
            "Epoch 28/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.5896 - nd: 0.4343 - val_loss: 0.2278 - val_nd: 0.2312\n",
            "Epoch 29/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.5805 - nd: 0.4336 - val_loss: 0.2131 - val_nd: 0.2251\n",
            "Epoch 30/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.5896 - nd: 0.4363 - val_loss: 0.2046 - val_nd: 0.2187\n",
            "Epoch 31/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5853 - nd: 0.4264 - val_loss: 0.2144 - val_nd: 0.2325\n",
            "Epoch 32/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.5682 - nd: 0.4202 - val_loss: 0.2061 - val_nd: 0.2340\n",
            "Epoch 33/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.5746 - nd: 0.4185 - val_loss: 0.1975 - val_nd: 0.2101\n",
            "Epoch 34/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5499 - nd: 0.4051 - val_loss: 0.2013 - val_nd: 0.2193\n",
            "Epoch 35/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.5601 - nd: 0.4060 - val_loss: 0.1932 - val_nd: 0.2090\n",
            "Epoch 36/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.5481 - nd: 0.4056 - val_loss: 0.1785 - val_nd: 0.1977\n",
            "Epoch 37/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.5524 - nd: 0.4108 - val_loss: 0.2001 - val_nd: 0.2347\n",
            "Epoch 38/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5625 - nd: 0.4204 - val_loss: 0.2043 - val_nd: 0.2284\n",
            "Epoch 39/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5531 - nd: 0.3978 - val_loss: 0.2056 - val_nd: 0.2396\n",
            "Epoch 40/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.5450 - nd: 0.3986 - val_loss: 0.1990 - val_nd: 0.2365\n",
            "Epoch 41/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5463 - nd: 0.3986 - val_loss: 0.1885 - val_nd: 0.2279\n",
            "Epoch 42/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.5355 - nd: 0.3896 - val_loss: 0.1803 - val_nd: 0.2143\n",
            "Epoch 43/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5466 - nd: 0.4005 - val_loss: 0.1867 - val_nd: 0.2258\n",
            "Epoch 44/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.5149 - nd: 0.3895 - val_loss: 0.1851 - val_nd: 0.2159\n",
            "Epoch 45/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5279 - nd: 0.3886 - val_loss: 0.2122 - val_nd: 0.2421\n",
            "Epoch 46/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5252 - nd: 0.3936 - val_loss: 0.1895 - val_nd: 0.2166\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 63756\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                   _step 46\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                    loss 0.5251926779747009\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                   epoch 45\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                _runtime 7831.745730161667\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              _timestamp 1650250207.9733126\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                  val_nd 0.21663321554660797\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                      nd 0.39363768696784973\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                val_loss 0.18947653472423553\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           best_val_loss 0.1785101592540741\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              best_epoch 35\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              best-nd_95 0.0011121093053058967\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       best-nrmse_maxmin 0.03181946506149152\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    best-nrmse_maxmin_95 0.0003637614994325103\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 best-nd 0.25023557657729223\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   best-nrmse_maxmin_std 0.006186110729639695\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     best-nrmse_mean_std 0.05928267474079521\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      best-nrmse_mean_95 0.0034859955788954155\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         best-nrmse_mean 0.3049319807077504\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             best-nd_std 0.01891247786480315\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 7 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced dutiful-glade-634: https://app.wandb.ai/matintavakoli/Interpreting_TS/runs/1yek8cez\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.9.7\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Wandb version 0.12.14 is available!  To upgrade, please run:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in wandb/run-20220418_025012-2vg2rs69\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mfeasible-meadow-635\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://app.wandb.ai/matintavakoli/Interpreting_TS\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://app.wandb.ai/matintavakoli/Interpreting_TS/runs/2vg2rs69\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb off` to turn off syncing.\n",
            "\n",
            "/usr/local/envs/eval-metric/lib/python3.7/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  return f(**kwargs)\n",
            "/usr/local/envs/eval-metric/lib/python3.7/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  return f(**kwargs)\n",
            "  4%|█▍                                     | 101/2660 [00:00<00:11, 231.26it/s]\n",
            "(9090, 30, 10) (9090, 6) (1111, 30, 10) (1111, 6)\n",
            "(9090, 30, 10) (9090, 6) (1111, 30, 10) (1111, 6)\n",
            "Model: \"sequential_81\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm_88 (LSTM)               (None, 30, 32)            5504      \n",
            "_________________________________________________________________\n",
            "lstm_89 (LSTM)               (None, 30, 32)            8320      \n",
            "_________________________________________________________________\n",
            "lstm_90 (LSTM)               (None, 30, 32)            8320      \n",
            "_________________________________________________________________\n",
            "lstm_91 (LSTM)               (None, 32)                8320      \n",
            "_________________________________________________________________\n",
            "dropout_132 (Dropout)        (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense_173 (Dense)            (None, 6)                 198       \n",
            "=================================================================\n",
            "Total params: 30,662\n",
            "Trainable params: 30,662\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/200\n",
            "18/18 [==============================] - 1s 73ms/step - loss: 0.9893 - nd: 0.8907 - val_loss: 0.7658 - val_nd: 0.8169\n",
            "Epoch 2/200\n",
            "18/18 [==============================] - 0s 15ms/step - loss: 0.8703 - nd: 0.7182 - val_loss: 0.6849 - val_nd: 0.7464\n",
            "Epoch 3/200\n",
            "18/18 [==============================] - 0s 14ms/step - loss: 0.6992 - nd: 0.5556 - val_loss: 0.4112 - val_nd: 0.4642\n",
            "Epoch 4/200\n",
            "18/18 [==============================] - 0s 14ms/step - loss: 0.6288 - nd: 0.5039 - val_loss: 0.3204 - val_nd: 0.3518\n",
            "Epoch 5/200\n",
            "18/18 [==============================] - 0s 14ms/step - loss: 0.5974 - nd: 0.4628 - val_loss: 0.2728 - val_nd: 0.3071\n",
            "Epoch 6/200\n",
            "18/18 [==============================] - 0s 14ms/step - loss: 0.5669 - nd: 0.4364 - val_loss: 0.2721 - val_nd: 0.3389\n",
            "Epoch 7/200\n",
            "18/18 [==============================] - 0s 15ms/step - loss: 0.5349 - nd: 0.4075 - val_loss: 0.2305 - val_nd: 0.2837\n",
            "Epoch 8/200\n",
            "18/18 [==============================] - 0s 14ms/step - loss: 0.5123 - nd: 0.3928 - val_loss: 0.2262 - val_nd: 0.2772\n",
            "Epoch 9/200\n",
            "18/18 [==============================] - 0s 10ms/step - loss: 0.4918 - nd: 0.3760 - val_loss: 0.2307 - val_nd: 0.2506\n",
            "Epoch 10/200\n",
            "18/18 [==============================] - 0s 14ms/step - loss: 0.4777 - nd: 0.3551 - val_loss: 0.2125 - val_nd: 0.2314\n",
            "Epoch 11/200\n",
            "18/18 [==============================] - 0s 13ms/step - loss: 0.4827 - nd: 0.3534 - val_loss: 0.2726 - val_nd: 0.2964\n",
            "Epoch 12/200\n",
            "18/18 [==============================] - 0s 12ms/step - loss: 0.4813 - nd: 0.3506 - val_loss: 0.2218 - val_nd: 0.2388\n",
            "Epoch 13/200\n",
            "18/18 [==============================] - 0s 11ms/step - loss: 0.4649 - nd: 0.3236 - val_loss: 0.2180 - val_nd: 0.2144\n",
            "Epoch 14/200\n",
            "18/18 [==============================] - 0s 12ms/step - loss: 0.4475 - nd: 0.3202 - val_loss: 0.2307 - val_nd: 0.2367\n",
            "Epoch 15/200\n",
            "18/18 [==============================] - 0s 12ms/step - loss: 0.4504 - nd: 0.3119 - val_loss: 0.2434 - val_nd: 0.2322\n",
            "Epoch 16/200\n",
            "18/18 [==============================] - 0s 12ms/step - loss: 0.4550 - nd: 0.3073 - val_loss: 0.2399 - val_nd: 0.2187\n",
            "Epoch 17/200\n",
            "18/18 [==============================] - 0s 14ms/step - loss: 0.4602 - nd: 0.3070 - val_loss: 0.2070 - val_nd: 0.2119\n",
            "Epoch 18/200\n",
            "18/18 [==============================] - 0s 14ms/step - loss: 0.4393 - nd: 0.3027 - val_loss: 0.1653 - val_nd: 0.1847\n",
            "Epoch 19/200\n",
            "18/18 [==============================] - 0s 12ms/step - loss: 0.4456 - nd: 0.3079 - val_loss: 0.2419 - val_nd: 0.2963\n",
            "Epoch 20/200\n",
            "18/18 [==============================] - 0s 12ms/step - loss: 0.4502 - nd: 0.3152 - val_loss: 0.1709 - val_nd: 0.1874\n",
            "Epoch 21/200\n",
            "18/18 [==============================] - 0s 12ms/step - loss: 0.4478 - nd: 0.3048 - val_loss: 0.2131 - val_nd: 0.2038\n",
            "Epoch 22/200\n",
            "18/18 [==============================] - 0s 12ms/step - loss: 0.4351 - nd: 0.2995 - val_loss: 0.1676 - val_nd: 0.1857\n",
            "Epoch 23/200\n",
            "18/18 [==============================] - 0s 12ms/step - loss: 0.4369 - nd: 0.2850 - val_loss: 0.1724 - val_nd: 0.1888\n",
            "Epoch 24/200\n",
            "18/18 [==============================] - 0s 12ms/step - loss: 0.4208 - nd: 0.2816 - val_loss: 0.2149 - val_nd: 0.2285\n",
            "Epoch 25/200\n",
            "18/18 [==============================] - 0s 11ms/step - loss: 0.4319 - nd: 0.2839 - val_loss: 0.1848 - val_nd: 0.1962\n",
            "Epoch 26/200\n",
            "18/18 [==============================] - 0s 12ms/step - loss: 0.4340 - nd: 0.2820 - val_loss: 0.1740 - val_nd: 0.1914\n",
            "Epoch 27/200\n",
            "18/18 [==============================] - 0s 11ms/step - loss: 0.4338 - nd: 0.2834 - val_loss: 0.1947 - val_nd: 0.2051\n",
            "Epoch 28/200\n",
            "18/18 [==============================] - 0s 12ms/step - loss: 0.4300 - nd: 0.2795 - val_loss: 0.1675 - val_nd: 0.1816\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 63950\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                   _step 28\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                    loss 0.4299604594707489\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                   epoch 27\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                _runtime 7852.077401638031\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              _timestamp 1650250228.304984\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                  val_nd 0.18157726526260376\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                      nd 0.2794802486896515\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                val_loss 0.16751918196678162\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           best_val_loss 0.1653137505054474\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              best_epoch 17\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              best-nd_95 0.001372221024530733\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       best-nrmse_maxmin 0.029578211680093922\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    best-nrmse_maxmin_95 0.00020151802261097928\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 best-nd 0.2382903182846065\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   best-nrmse_maxmin_std 0.003427005892141813\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     best-nrmse_mean_std 0.03284164873823151\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      best-nrmse_mean_95 0.0019311855074974902\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         best-nrmse_mean 0.28345362362233817\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             best-nd_std 0.02333592537013851\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 7 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced feasible-meadow-635: https://app.wandb.ai/matintavakoli/Interpreting_TS/runs/2vg2rs69\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.9.7\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Wandb version 0.12.14 is available!  To upgrade, please run:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in wandb/run-20220418_025032-1mxx8mpo\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mfast-bird-636\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://app.wandb.ai/matintavakoli/Interpreting_TS\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://app.wandb.ai/matintavakoli/Interpreting_TS/runs/1mxx8mpo\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb off` to turn off syncing.\n",
            "\n",
            "/usr/local/envs/eval-metric/lib/python3.7/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  return f(**kwargs)\n",
            "/usr/local/envs/eval-metric/lib/python3.7/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  return f(**kwargs)\n",
            "  4%|█▍                                     | 101/2660 [00:00<00:10, 250.47it/s]\n",
            "(9090, 30, 10) (9090, 6) (1111, 30, 10) (1111, 6)\n",
            "(9090, 30, 10) (9090, 6) (1111, 30, 10) (1111, 6)\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 64089\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              _timestamp 1650250723.8158305\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                   _step 2\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                    loss 0.0028490875371646684\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                      nd 0.07835256356691052\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                _runtime 8347.58824801445\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                  val_nd 0.09239674566173289\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                val_loss 0.01935590873189652\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              best-nd_95 0.0005446779558987218\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       best-nrmse_maxmin 0.019054239995858497\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    best-nrmse_maxmin_95 0.00019317976628655957\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 best-nd 0.10844893379951855\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   best-nrmse_maxmin_std 0.0032852059023258237\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     best-nrmse_mean_std 0.031482752487921574\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      best-nrmse_mean_95 0.0018512784125246284\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         best-nrmse_mean 0.18260040297942137\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             best-nd_std 0.009262767369388525\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 8 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced fast-bird-636: https://app.wandb.ai/matintavakoli/Interpreting_TS/runs/1mxx8mpo\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.9.7\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Wandb version 0.12.14 is available!  To upgrade, please run:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in wandb/run-20220418_025847-mygv3dxl\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mhelpful-sun-637\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://app.wandb.ai/matintavakoli/Interpreting_TS\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://app.wandb.ai/matintavakoli/Interpreting_TS/runs/mygv3dxl\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb off` to turn off syncing.\n",
            "\n",
            "/usr/local/envs/eval-metric/lib/python3.7/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  return f(**kwargs)\n",
            "/usr/local/envs/eval-metric/lib/python3.7/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  return f(**kwargs)\n",
            "  4%|█▍                                     | 101/2660 [00:00<00:11, 230.98it/s]\n",
            "(9090, 30, 10) (9090, 6) (1111, 30, 10) (1111, 6)\n",
            "(9090, 30, 10) (9090, 6) (1111, 30, 10) (1111, 6)\n",
            "Model: \"sequential_82\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "reshape_41 (Reshape)         (None, 300)               0         \n",
            "_________________________________________________________________\n",
            "dense_174 (Dense)            (None, 32)                9632      \n",
            "_________________________________________________________________\n",
            "dropout_133 (Dropout)        (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense_175 (Dense)            (None, 32)                1056      \n",
            "_________________________________________________________________\n",
            "dropout_134 (Dropout)        (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense_176 (Dense)            (None, 32)                1056      \n",
            "_________________________________________________________________\n",
            "dropout_135 (Dropout)        (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense_177 (Dense)            (None, 32)                1056      \n",
            "_________________________________________________________________\n",
            "dropout_136 (Dropout)        (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense_178 (Dense)            (None, 6)                 198       \n",
            "=================================================================\n",
            "Total params: 12,998\n",
            "Trainable params: 12,998\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/200\n",
            "18/18 [==============================] - 0s 11ms/step - loss: 1.2503 - nd: 1.3322 - val_loss: 0.8455 - val_nd: 0.9681\n",
            "Epoch 2/200\n",
            "18/18 [==============================] - 0s 8ms/step - loss: 1.1583 - nd: 1.1725 - val_loss: 0.8288 - val_nd: 0.9270\n",
            "Epoch 3/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 1.1115 - nd: 1.0914 - val_loss: 0.8277 - val_nd: 0.9202\n",
            "Epoch 4/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 1.0904 - nd: 1.0344 - val_loss: 0.8278 - val_nd: 0.9208\n",
            "Epoch 5/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 1.0647 - nd: 0.9998 - val_loss: 0.8279 - val_nd: 0.9229\n",
            "Epoch 6/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 1.0496 - nd: 0.9756 - val_loss: 0.8279 - val_nd: 0.9226\n",
            "Epoch 7/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 1.0412 - nd: 0.9564 - val_loss: 0.8285 - val_nd: 0.9266\n",
            "Epoch 8/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 1.0357 - nd: 0.9481 - val_loss: 0.8287 - val_nd: 0.9279\n",
            "Epoch 9/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 1.0215 - nd: 0.9350 - val_loss: 0.8284 - val_nd: 0.9261\n",
            "Epoch 10/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 1.0178 - nd: 0.9297 - val_loss: 0.8285 - val_nd: 0.9263\n",
            "Epoch 11/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 1.0093 - nd: 0.9215 - val_loss: 0.8283 - val_nd: 0.9249\n",
            "Epoch 12/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 1.0113 - nd: 0.9188 - val_loss: 0.8287 - val_nd: 0.9275\n",
            "Epoch 13/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 1.0074 - nd: 0.9145 - val_loss: 0.8284 - val_nd: 0.9256\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 64135\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                   _step 13\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                    loss 1.0073628425598145\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                   epoch 12\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                _runtime 8355.053977966309\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              _timestamp 1650250731.2815604\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                  val_nd 0.9256318211555481\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                      nd 0.9145024418830872\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                val_loss 0.8283876776695251\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           best_val_loss 0.8276993036270142\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              best_epoch 2\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              best-nd_95 0.0021264815784951346\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       best-nrmse_maxmin 0.13955361309844672\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    best-nrmse_maxmin_95 0.002109748015874498\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 best-nd 1.003631155267584\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   best-nrmse_maxmin_std 0.03587827424891816\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     best-nrmse_mean_std 0.3438283204936339\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      best-nrmse_mean_95 0.02021811617610839\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         best-nrmse_mean 1.3373687953205868\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             best-nd_std 0.036162844417652634\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 7 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced helpful-sun-637: https://app.wandb.ai/matintavakoli/Interpreting_TS/runs/mygv3dxl\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.9.7\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Wandb version 0.12.14 is available!  To upgrade, please run:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in wandb/run-20220418_025856-1fe0xgdh\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mgraceful-firebrand-638\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://app.wandb.ai/matintavakoli/Interpreting_TS\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://app.wandb.ai/matintavakoli/Interpreting_TS/runs/1fe0xgdh\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb off` to turn off syncing.\n",
            "\n",
            "/usr/local/envs/eval-metric/lib/python3.7/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  return f(**kwargs)\n",
            "/usr/local/envs/eval-metric/lib/python3.7/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  return f(**kwargs)\n",
            "  4%|█▍                                     | 101/2660 [00:00<00:10, 234.95it/s]\n",
            "(9090, 30, 10) (9090, 6) (1111, 30, 10) (1111, 6)\n",
            "(9090, 30, 10) (9090, 6) (1111, 30, 10) (1111, 6)\n",
            "Model: \"sequential_83\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm_92 (LSTM)               (None, 30, 32)            5504      \n",
            "_________________________________________________________________\n",
            "lstm_93 (LSTM)               (None, 30, 32)            8320      \n",
            "_________________________________________________________________\n",
            "lstm_94 (LSTM)               (None, 30, 32)            8320      \n",
            "_________________________________________________________________\n",
            "lstm_95 (LSTM)               (None, 32)                8320      \n",
            "_________________________________________________________________\n",
            "dropout_137 (Dropout)        (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense_179 (Dense)            (None, 6)                 198       \n",
            "=================================================================\n",
            "Total params: 30,662\n",
            "Trainable params: 30,662\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/200\n",
            "18/18 [==============================] - 1s 72ms/step - loss: 1.0047 - nd: 0.9152 - val_loss: 0.8021 - val_nd: 0.8442\n",
            "Epoch 2/200\n",
            "18/18 [==============================] - 0s 15ms/step - loss: 0.9431 - nd: 0.8373 - val_loss: 0.7990 - val_nd: 0.7609\n",
            "Epoch 3/200\n",
            "18/18 [==============================] - 0s 14ms/step - loss: 0.8024 - nd: 0.6756 - val_loss: 0.7244 - val_nd: 0.7284\n",
            "Epoch 4/200\n",
            "18/18 [==============================] - 0s 14ms/step - loss: 0.6984 - nd: 0.5773 - val_loss: 0.7094 - val_nd: 0.7172\n",
            "Epoch 5/200\n",
            "18/18 [==============================] - 0s 14ms/step - loss: 0.6579 - nd: 0.5392 - val_loss: 0.6920 - val_nd: 0.6544\n",
            "Epoch 6/200\n",
            "18/18 [==============================] - 0s 13ms/step - loss: 0.6372 - nd: 0.5028 - val_loss: 0.6411 - val_nd: 0.6106\n",
            "Epoch 7/200\n",
            "18/18 [==============================] - 0s 15ms/step - loss: 0.6116 - nd: 0.4783 - val_loss: 0.4256 - val_nd: 0.4518\n",
            "Epoch 8/200\n",
            "18/18 [==============================] - 0s 14ms/step - loss: 0.5992 - nd: 0.4713 - val_loss: 0.3951 - val_nd: 0.4381\n",
            "Epoch 9/200\n",
            "18/18 [==============================] - 0s 12ms/step - loss: 0.5757 - nd: 0.4475 - val_loss: 0.4551 - val_nd: 0.5158\n",
            "Epoch 10/200\n",
            "18/18 [==============================] - 0s 12ms/step - loss: 0.5676 - nd: 0.4289 - val_loss: 0.4918 - val_nd: 0.5566\n",
            "Epoch 11/200\n",
            "18/18 [==============================] - 0s 11ms/step - loss: 0.5515 - nd: 0.4119 - val_loss: 0.5425 - val_nd: 0.6192\n",
            "Epoch 12/200\n",
            "18/18 [==============================] - 0s 11ms/step - loss: 0.5364 - nd: 0.4040 - val_loss: 0.5393 - val_nd: 0.6229\n",
            "Epoch 13/200\n",
            "18/18 [==============================] - 0s 12ms/step - loss: 0.5263 - nd: 0.3916 - val_loss: 0.4969 - val_nd: 0.5816\n",
            "Epoch 14/200\n",
            "18/18 [==============================] - 0s 12ms/step - loss: 0.5337 - nd: 0.3948 - val_loss: 0.4452 - val_nd: 0.5211\n",
            "Epoch 15/200\n",
            "18/18 [==============================] - 0s 12ms/step - loss: 0.5102 - nd: 0.3825 - val_loss: 0.4739 - val_nd: 0.5391\n",
            "Epoch 16/200\n",
            "18/18 [==============================] - 0s 11ms/step - loss: 0.5104 - nd: 0.3782 - val_loss: 0.4600 - val_nd: 0.5343\n",
            "Epoch 17/200\n",
            "18/18 [==============================] - 0s 13ms/step - loss: 0.5119 - nd: 0.3721 - val_loss: 0.3834 - val_nd: 0.4424\n",
            "Epoch 18/200\n",
            "18/18 [==============================] - 0s 12ms/step - loss: 0.5244 - nd: 0.3758 - val_loss: 0.3927 - val_nd: 0.4565\n",
            "Epoch 19/200\n",
            "18/18 [==============================] - 0s 12ms/step - loss: 0.5076 - nd: 0.3700 - val_loss: 0.4223 - val_nd: 0.4720\n",
            "Epoch 20/200\n",
            "18/18 [==============================] - 0s 14ms/step - loss: 0.5170 - nd: 0.3687 - val_loss: 0.3449 - val_nd: 0.4085\n",
            "Epoch 21/200\n",
            "18/18 [==============================] - 0s 13ms/step - loss: 0.5078 - nd: 0.3615 - val_loss: 0.3974 - val_nd: 0.4737\n",
            "Epoch 22/200\n",
            "18/18 [==============================] - 0s 12ms/step - loss: 0.4879 - nd: 0.3569 - val_loss: 0.3766 - val_nd: 0.4365\n",
            "Epoch 23/200\n",
            "18/18 [==============================] - 0s 12ms/step - loss: 0.4979 - nd: 0.3560 - val_loss: 0.4325 - val_nd: 0.5110\n",
            "Epoch 24/200\n",
            "18/18 [==============================] - 0s 12ms/step - loss: 0.4906 - nd: 0.3448 - val_loss: 0.4026 - val_nd: 0.4590\n",
            "Epoch 25/200\n",
            "18/18 [==============================] - 0s 12ms/step - loss: 0.4993 - nd: 0.3457 - val_loss: 0.3838 - val_nd: 0.4395\n",
            "Epoch 26/200\n",
            "18/18 [==============================] - 0s 14ms/step - loss: 0.4916 - nd: 0.3446 - val_loss: 0.3443 - val_nd: 0.4135\n",
            "Epoch 27/200\n",
            "18/18 [==============================] - 0s 15ms/step - loss: 0.4749 - nd: 0.3370 - val_loss: 0.3073 - val_nd: 0.3632\n",
            "Epoch 28/200\n",
            "18/18 [==============================] - 0s 14ms/step - loss: 0.4950 - nd: 0.3470 - val_loss: 0.3063 - val_nd: 0.3456\n",
            "Epoch 29/200\n",
            "18/18 [==============================] - 0s 12ms/step - loss: 0.4784 - nd: 0.3421 - val_loss: 0.3096 - val_nd: 0.3551\n",
            "Epoch 30/200\n",
            "18/18 [==============================] - 0s 12ms/step - loss: 0.4712 - nd: 0.3289 - val_loss: 0.3560 - val_nd: 0.4015\n",
            "Epoch 31/200\n",
            "18/18 [==============================] - 0s 14ms/step - loss: 0.4841 - nd: 0.3414 - val_loss: 0.2755 - val_nd: 0.2839\n",
            "Epoch 32/200\n",
            "18/18 [==============================] - 0s 13ms/step - loss: 0.4859 - nd: 0.3394 - val_loss: 0.3366 - val_nd: 0.3795\n",
            "Epoch 33/200\n",
            "18/18 [==============================] - 0s 10ms/step - loss: 0.4872 - nd: 0.3368 - val_loss: 0.3113 - val_nd: 0.3308\n",
            "Epoch 34/200\n",
            "18/18 [==============================] - 0s 12ms/step - loss: 0.4948 - nd: 0.3363 - val_loss: 0.3555 - val_nd: 0.3968\n",
            "Epoch 35/200\n",
            "18/18 [==============================] - 0s 12ms/step - loss: 0.4847 - nd: 0.3409 - val_loss: 0.3409 - val_nd: 0.3695\n",
            "Epoch 36/200\n",
            "18/18 [==============================] - 0s 15ms/step - loss: 0.4820 - nd: 0.3313 - val_loss: 0.2705 - val_nd: 0.2977\n",
            "Epoch 37/200\n",
            "18/18 [==============================] - 0s 11ms/step - loss: 0.4827 - nd: 0.3324 - val_loss: 0.2817 - val_nd: 0.2838\n",
            "Epoch 38/200\n",
            "18/18 [==============================] - 0s 14ms/step - loss: 0.4828 - nd: 0.3353 - val_loss: 0.2266 - val_nd: 0.2431\n",
            "Epoch 39/200\n",
            "18/18 [==============================] - 0s 12ms/step - loss: 0.4797 - nd: 0.3409 - val_loss: 0.3168 - val_nd: 0.2862\n",
            "Epoch 40/200\n",
            "18/18 [==============================] - 0s 12ms/step - loss: 0.4831 - nd: 0.3319 - val_loss: 0.2626 - val_nd: 0.2710\n",
            "Epoch 41/200\n",
            "18/18 [==============================] - 0s 12ms/step - loss: 0.4787 - nd: 0.3230 - val_loss: 0.3193 - val_nd: 0.3233\n",
            "Epoch 42/200\n",
            "18/18 [==============================] - 0s 12ms/step - loss: 0.4702 - nd: 0.3196 - val_loss: 0.2881 - val_nd: 0.2815\n",
            "Epoch 43/200\n",
            "18/18 [==============================] - 0s 12ms/step - loss: 0.4689 - nd: 0.3162 - val_loss: 0.3155 - val_nd: 0.3259\n",
            "Epoch 44/200\n",
            "18/18 [==============================] - 0s 12ms/step - loss: 0.4873 - nd: 0.3272 - val_loss: 0.3231 - val_nd: 0.3504\n",
            "Epoch 45/200\n",
            "18/18 [==============================] - 0s 12ms/step - loss: 0.4762 - nd: 0.3217 - val_loss: 0.3406 - val_nd: 0.3626\n",
            "Epoch 46/200\n",
            "18/18 [==============================] - 0s 12ms/step - loss: 0.4717 - nd: 0.3178 - val_loss: 0.2980 - val_nd: 0.2938\n",
            "Epoch 47/200\n",
            "18/18 [==============================] - 0s 12ms/step - loss: 0.4708 - nd: 0.3207 - val_loss: 0.3276 - val_nd: 0.3574\n",
            "Epoch 48/200\n",
            "18/18 [==============================] - 0s 12ms/step - loss: 0.4747 - nd: 0.3175 - val_loss: 0.3064 - val_nd: 0.2946\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 64229\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                   _step 48\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                    loss 0.4746878147125244\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                   epoch 47\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                _runtime 8380.852266073227\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              _timestamp 1650250757.0798485\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                  val_nd 0.2945704162120819\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                      nd 0.3174540102481842\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                val_loss 0.30638211965560913\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           best_val_loss 0.22664690017700195\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              best_epoch 37\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              best-nd_95 0.004462723060399873\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       best-nrmse_maxmin 0.040085851442830875\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    best-nrmse_maxmin_95 0.0005134186854942899\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 best-nd 0.29128090230033926\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   best-nrmse_maxmin_std 0.008731173706092001\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     best-nrmse_mean_std 0.08367249691209182\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      best-nrmse_mean_95 0.0049201888340232435\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         best-nrmse_mean 0.38415033235779006\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             best-nd_std 0.07589285575966274\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 7 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced graceful-firebrand-638: https://app.wandb.ai/matintavakoli/Interpreting_TS/runs/1fe0xgdh\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.9.7\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Wandb version 0.12.14 is available!  To upgrade, please run:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in wandb/run-20220418_025921-2a6nli1b\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mfine-feather-639\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://app.wandb.ai/matintavakoli/Interpreting_TS\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://app.wandb.ai/matintavakoli/Interpreting_TS/runs/2a6nli1b\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb off` to turn off syncing.\n",
            "\n",
            "/usr/local/envs/eval-metric/lib/python3.7/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  return f(**kwargs)\n",
            "/usr/local/envs/eval-metric/lib/python3.7/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  return f(**kwargs)\n",
            "  4%|█▍                                     | 101/2660 [00:00<00:10, 238.27it/s]\n",
            "(9090, 30, 10) (9090, 6) (1111, 30, 10) (1111, 6)\n",
            "(9090, 30, 10) (9090, 6) (1111, 30, 10) (1111, 6)\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 64428\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              _timestamp 1650251253.9407248\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                   _step 2\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                    loss 0.002867504860071669\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                      nd 0.07834269408913505\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                _runtime 8877.71314239502\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                  val_nd 0.09318353622399057\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                val_loss 0.01957530451281142\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              best-nd_95 0.0005175553166663754\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       best-nrmse_maxmin 0.01923428112369521\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    best-nrmse_maxmin_95 0.00021490526371660217\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 best-nd 0.10923910046526276\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   best-nrmse_maxmin_std 0.0036546686766117573\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     best-nrmse_mean_std 0.03502338446721699\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      best-nrmse_mean_95 0.0020594779831459927\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         best-nrmse_mean 0.18432577132279265\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             best-nd_std 0.008801521058734108\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 8 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced fine-feather-639: https://app.wandb.ai/matintavakoli/Interpreting_TS/runs/2a6nli1b\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.9.7\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Wandb version 0.12.14 is available!  To upgrade, please run:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in wandb/run-20220418_030738-35fn58i6\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33meternal-plant-640\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://app.wandb.ai/matintavakoli/Interpreting_TS\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://app.wandb.ai/matintavakoli/Interpreting_TS/runs/35fn58i6\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb off` to turn off syncing.\n",
            "\n",
            "/usr/local/envs/eval-metric/lib/python3.7/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  return f(**kwargs)\n",
            "/usr/local/envs/eval-metric/lib/python3.7/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  return f(**kwargs)\n",
            "  4%|█▍                                     | 101/2660 [00:00<00:10, 243.48it/s]\n",
            "(9090, 30, 10) (9090, 6) (1111, 30, 10) (1111, 6)\n",
            "(9090, 30, 10) (9090, 6) (1111, 30, 10) (1111, 6)\n",
            "Model: \"sequential_84\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "reshape_42 (Reshape)         (None, 300)               0         \n",
            "_________________________________________________________________\n",
            "dense_180 (Dense)            (None, 64)                19264     \n",
            "_________________________________________________________________\n",
            "dropout_138 (Dropout)        (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_181 (Dense)            (None, 64)                4160      \n",
            "_________________________________________________________________\n",
            "dropout_139 (Dropout)        (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_182 (Dense)            (None, 64)                4160      \n",
            "_________________________________________________________________\n",
            "dropout_140 (Dropout)        (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_183 (Dense)            (None, 64)                4160      \n",
            "_________________________________________________________________\n",
            "dropout_141 (Dropout)        (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_184 (Dense)            (None, 6)                 390       \n",
            "=================================================================\n",
            "Total params: 32,134\n",
            "Trainable params: 32,134\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/200\n",
            "18/18 [==============================] - 0s 11ms/step - loss: 1.1580 - nd: 1.1113 - val_loss: 0.8329 - val_nd: 0.9390\n",
            "Epoch 2/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 1.0105 - nd: 0.9234 - val_loss: 0.8343 - val_nd: 0.9160\n",
            "Epoch 3/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 1.0033 - nd: 0.8818 - val_loss: 0.8282 - val_nd: 0.9194\n",
            "Epoch 4/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.9966 - nd: 0.8932 - val_loss: 0.8249 - val_nd: 0.8955\n",
            "Epoch 5/200\n",
            "18/18 [==============================] - 0s 8ms/step - loss: 0.9969 - nd: 0.8751 - val_loss: 0.8239 - val_nd: 0.9043\n",
            "Epoch 6/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.9973 - nd: 0.8805 - val_loss: 0.8230 - val_nd: 0.9126\n",
            "Epoch 7/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 1.0032 - nd: 0.8828 - val_loss: 0.8191 - val_nd: 0.9061\n",
            "Epoch 8/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.9913 - nd: 0.8658 - val_loss: 0.8132 - val_nd: 0.9068\n",
            "Epoch 9/200\n",
            "18/18 [==============================] - 0s 8ms/step - loss: 0.9824 - nd: 0.8593 - val_loss: 0.7888 - val_nd: 0.8345\n",
            "Epoch 10/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.9544 - nd: 0.8079 - val_loss: 0.7368 - val_nd: 0.7497\n",
            "Epoch 11/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.8830 - nd: 0.6950 - val_loss: 0.6109 - val_nd: 0.6006\n",
            "Epoch 12/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.7552 - nd: 0.5217 - val_loss: 0.4273 - val_nd: 0.3989\n",
            "Epoch 13/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.6492 - nd: 0.4087 - val_loss: 0.3123 - val_nd: 0.3409\n",
            "Epoch 14/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.5624 - nd: 0.3432 - val_loss: 0.2741 - val_nd: 0.2724\n",
            "Epoch 15/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.5381 - nd: 0.3106 - val_loss: 0.2360 - val_nd: 0.2155\n",
            "Epoch 16/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.5100 - nd: 0.3027 - val_loss: 0.2316 - val_nd: 0.2431\n",
            "Epoch 17/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.5086 - nd: 0.3038 - val_loss: 0.2189 - val_nd: 0.2255\n",
            "Epoch 18/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.4831 - nd: 0.2700 - val_loss: 0.1961 - val_nd: 0.1966\n",
            "Epoch 19/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.4656 - nd: 0.2623 - val_loss: 0.1742 - val_nd: 0.1835\n",
            "Epoch 20/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4634 - nd: 0.2702 - val_loss: 0.1778 - val_nd: 0.2000\n",
            "Epoch 21/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.4330 - nd: 0.2473 - val_loss: 0.1692 - val_nd: 0.1844\n",
            "Epoch 22/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.4339 - nd: 0.2322 - val_loss: 0.1750 - val_nd: 0.1787\n",
            "Epoch 23/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.4266 - nd: 0.2214 - val_loss: 0.1659 - val_nd: 0.1772\n",
            "Epoch 24/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.4214 - nd: 0.2217 - val_loss: 0.1762 - val_nd: 0.1735\n",
            "Epoch 25/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4160 - nd: 0.2172 - val_loss: 0.1716 - val_nd: 0.1652\n",
            "Epoch 26/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.3984 - nd: 0.2143 - val_loss: 0.1863 - val_nd: 0.1677\n",
            "Epoch 27/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.4280 - nd: 0.2226 - val_loss: 0.1709 - val_nd: 0.1528\n",
            "Epoch 28/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4176 - nd: 0.2107 - val_loss: 0.1711 - val_nd: 0.1467\n",
            "Epoch 29/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4026 - nd: 0.2182 - val_loss: 0.2171 - val_nd: 0.2185\n",
            "Epoch 30/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4159 - nd: 0.2198 - val_loss: 0.1853 - val_nd: 0.1663\n",
            "Epoch 31/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4073 - nd: 0.2081 - val_loss: 0.1843 - val_nd: 0.1517\n",
            "Epoch 32/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.3978 - nd: 0.2042 - val_loss: 0.1820 - val_nd: 0.1508\n",
            "Epoch 33/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.3991 - nd: 0.2005 - val_loss: 0.1854 - val_nd: 0.1541\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 64558\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                   _step 33\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                    loss 0.3990853726863861\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                   epoch 32\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                _runtime 8888.43400645256\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              _timestamp 1650251264.661589\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                  val_nd 0.1540648490190506\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                      nd 0.20052658021450043\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                val_loss 0.18540121614933014\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           best_val_loss 0.1659170389175415\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              best_epoch 22\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              best-nd_95 0.001302920427206317\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       best-nrmse_maxmin 0.028334813647930668\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    best-nrmse_maxmin_95 0.00019328203234517298\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 best-nd 0.1878385905465501\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   best-nrmse_maxmin_std 0.003286945034046617\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     best-nrmse_mean_std 0.03149941891770939\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      best-nrmse_mean_95 0.0018522584475991308\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         best-nrmse_mean 0.27153790398271055\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             best-nd_std 0.022157402713541233\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 7 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced eternal-plant-640: https://app.wandb.ai/matintavakoli/Interpreting_TS/runs/35fn58i6\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.9.7\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Wandb version 0.12.14 is available!  To upgrade, please run:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in wandb/run-20220418_030749-3g6y6mc7\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mgiddy-lion-641\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://app.wandb.ai/matintavakoli/Interpreting_TS\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://app.wandb.ai/matintavakoli/Interpreting_TS/runs/3g6y6mc7\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb off` to turn off syncing.\n",
            "\n",
            "/usr/local/envs/eval-metric/lib/python3.7/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  return f(**kwargs)\n",
            "/usr/local/envs/eval-metric/lib/python3.7/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  return f(**kwargs)\n",
            "  4%|█▍                                     | 101/2660 [00:00<00:11, 228.09it/s]\n",
            "(9090, 30, 10) (9090, 6) (1111, 30, 10) (1111, 6)\n",
            "(9090, 30, 10) (9090, 6) (1111, 30, 10) (1111, 6)\n",
            "Model: \"sequential_85\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm_96 (LSTM)               (None, 30, 64)            19200     \n",
            "_________________________________________________________________\n",
            "lstm_97 (LSTM)               (None, 30, 64)            33024     \n",
            "_________________________________________________________________\n",
            "lstm_98 (LSTM)               (None, 30, 64)            33024     \n",
            "_________________________________________________________________\n",
            "lstm_99 (LSTM)               (None, 64)                33024     \n",
            "_________________________________________________________________\n",
            "dropout_142 (Dropout)        (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_185 (Dense)            (None, 6)                 390       \n",
            "=================================================================\n",
            "Total params: 118,662\n",
            "Trainable params: 118,662\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/200\n",
            "18/18 [==============================] - 1s 76ms/step - loss: 0.9255 - nd: 0.7964 - val_loss: 0.5478 - val_nd: 0.5427\n",
            "Epoch 2/200\n",
            "18/18 [==============================] - 0s 18ms/step - loss: 0.6495 - nd: 0.5123 - val_loss: 0.3639 - val_nd: 0.3998\n",
            "Epoch 3/200\n",
            "18/18 [==============================] - 0s 16ms/step - loss: 0.5382 - nd: 0.3990 - val_loss: 0.2480 - val_nd: 0.2658\n",
            "Epoch 4/200\n",
            "18/18 [==============================] - 0s 14ms/step - loss: 0.4613 - nd: 0.3336 - val_loss: 0.2750 - val_nd: 0.2869\n",
            "Epoch 5/200\n",
            "18/18 [==============================] - 0s 14ms/step - loss: 0.4378 - nd: 0.3074 - val_loss: 0.2738 - val_nd: 0.2766\n",
            "Epoch 6/200\n",
            "18/18 [==============================] - 0s 15ms/step - loss: 0.4218 - nd: 0.2761 - val_loss: 0.2403 - val_nd: 0.2270\n",
            "Epoch 7/200\n",
            "18/18 [==============================] - 0s 15ms/step - loss: 0.3973 - nd: 0.2435 - val_loss: 0.2179 - val_nd: 0.2008\n",
            "Epoch 8/200\n",
            "18/18 [==============================] - 0s 15ms/step - loss: 0.4014 - nd: 0.2264 - val_loss: 0.2106 - val_nd: 0.1863\n",
            "Epoch 9/200\n",
            "18/18 [==============================] - 0s 14ms/step - loss: 0.4036 - nd: 0.2290 - val_loss: 0.2492 - val_nd: 0.2478\n",
            "Epoch 10/200\n",
            "18/18 [==============================] - 0s 13ms/step - loss: 0.4089 - nd: 0.2477 - val_loss: 0.2350 - val_nd: 0.2273\n",
            "Epoch 11/200\n",
            "18/18 [==============================] - 0s 15ms/step - loss: 0.4134 - nd: 0.2577 - val_loss: 0.2065 - val_nd: 0.2070\n",
            "Epoch 12/200\n",
            "18/18 [==============================] - 0s 16ms/step - loss: 0.4211 - nd: 0.2687 - val_loss: 0.1569 - val_nd: 0.1736\n",
            "Epoch 13/200\n",
            "18/18 [==============================] - 0s 14ms/step - loss: 0.3921 - nd: 0.2173 - val_loss: 0.2223 - val_nd: 0.2206\n",
            "Epoch 14/200\n",
            "18/18 [==============================] - 0s 13ms/step - loss: 0.3973 - nd: 0.2341 - val_loss: 0.1708 - val_nd: 0.1884\n",
            "Epoch 15/200\n",
            "18/18 [==============================] - 0s 13ms/step - loss: 0.3904 - nd: 0.2225 - val_loss: 0.1676 - val_nd: 0.1884\n",
            "Epoch 16/200\n",
            "18/18 [==============================] - 0s 13ms/step - loss: 0.3859 - nd: 0.2005 - val_loss: 0.2376 - val_nd: 0.2558\n",
            "Epoch 17/200\n",
            "18/18 [==============================] - 0s 13ms/step - loss: 0.3847 - nd: 0.2113 - val_loss: 0.2649 - val_nd: 0.2278\n",
            "Epoch 18/200\n",
            "18/18 [==============================] - 0s 16ms/step - loss: 0.3881 - nd: 0.2145 - val_loss: 0.1390 - val_nd: 0.1469\n",
            "Epoch 19/200\n",
            "18/18 [==============================] - 0s 13ms/step - loss: 0.3809 - nd: 0.1989 - val_loss: 0.1512 - val_nd: 0.1642\n",
            "Epoch 20/200\n",
            "18/18 [==============================] - 0s 14ms/step - loss: 0.3629 - nd: 0.1839 - val_loss: 0.1743 - val_nd: 0.1675\n",
            "Epoch 21/200\n",
            "18/18 [==============================] - 0s 13ms/step - loss: 0.3832 - nd: 0.2265 - val_loss: 0.1716 - val_nd: 0.1653\n",
            "Epoch 22/200\n",
            "18/18 [==============================] - 0s 12ms/step - loss: 0.3765 - nd: 0.1959 - val_loss: 0.2042 - val_nd: 0.1843\n",
            "Epoch 23/200\n",
            "18/18 [==============================] - 0s 14ms/step - loss: 0.3459 - nd: 0.1879 - val_loss: 0.1434 - val_nd: 0.1488\n",
            "Epoch 24/200\n",
            "18/18 [==============================] - 0s 14ms/step - loss: 0.3696 - nd: 0.1764 - val_loss: 0.1470 - val_nd: 0.1432\n",
            "Epoch 25/200\n",
            "18/18 [==============================] - 0s 13ms/step - loss: 0.3790 - nd: 0.1751 - val_loss: 0.1926 - val_nd: 0.1735\n",
            "Epoch 26/200\n",
            "18/18 [==============================] - 0s 14ms/step - loss: 0.3887 - nd: 0.1770 - val_loss: 0.1451 - val_nd: 0.1615\n",
            "Epoch 27/200\n",
            "18/18 [==============================] - 0s 15ms/step - loss: 0.3438 - nd: 0.1727 - val_loss: 0.1339 - val_nd: 0.1419\n",
            "Epoch 28/200\n",
            "18/18 [==============================] - 0s 17ms/step - loss: 0.3558 - nd: 0.1659 - val_loss: 0.1336 - val_nd: 0.1337\n",
            "Epoch 29/200\n",
            "18/18 [==============================] - 0s 13ms/step - loss: 0.3420 - nd: 0.1660 - val_loss: 0.1728 - val_nd: 0.1862\n",
            "Epoch 30/200\n",
            "18/18 [==============================] - 0s 13ms/step - loss: 0.3527 - nd: 0.1852 - val_loss: 0.1665 - val_nd: 0.1681\n",
            "Epoch 31/200\n",
            "18/18 [==============================] - 0s 17ms/step - loss: 0.3493 - nd: 0.1745 - val_loss: 0.1231 - val_nd: 0.1260\n",
            "Epoch 32/200\n",
            "18/18 [==============================] - 0s 13ms/step - loss: 0.3645 - nd: 0.1911 - val_loss: 0.1257 - val_nd: 0.1239\n",
            "Epoch 33/200\n",
            "18/18 [==============================] - 0s 15ms/step - loss: 0.3590 - nd: 0.1946 - val_loss: 0.1761 - val_nd: 0.2172\n",
            "Epoch 34/200\n",
            "18/18 [==============================] - 0s 13ms/step - loss: 0.3509 - nd: 0.1928 - val_loss: 0.1532 - val_nd: 0.1727\n",
            "Epoch 35/200\n",
            "18/18 [==============================] - 0s 13ms/step - loss: 0.3571 - nd: 0.2031 - val_loss: 0.2312 - val_nd: 0.2755\n",
            "Epoch 36/200\n",
            "18/18 [==============================] - 0s 13ms/step - loss: 0.3684 - nd: 0.2024 - val_loss: 0.1600 - val_nd: 0.1496\n",
            "Epoch 37/200\n",
            "18/18 [==============================] - 0s 14ms/step - loss: 0.3615 - nd: 0.1719 - val_loss: 0.1723 - val_nd: 0.1943\n",
            "Epoch 38/200\n",
            "18/18 [==============================] - 0s 13ms/step - loss: 0.3468 - nd: 0.1669 - val_loss: 0.2870 - val_nd: 0.2083\n",
            "Epoch 39/200\n",
            "18/18 [==============================] - 0s 14ms/step - loss: 0.3617 - nd: 0.1669 - val_loss: 0.1299 - val_nd: 0.1354\n",
            "Epoch 40/200\n",
            "18/18 [==============================] - 0s 13ms/step - loss: 0.3576 - nd: 0.1584 - val_loss: 0.1423 - val_nd: 0.1499\n",
            "Epoch 41/200\n",
            "18/18 [==============================] - 0s 13ms/step - loss: 0.3502 - nd: 0.1579 - val_loss: 0.1297 - val_nd: 0.1283\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 64713\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                   _step 41\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                    loss 0.3501773774623871\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                   epoch 40\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                _runtime 8913.234147787094\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              _timestamp 1650251289.4617302\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                  val_nd 0.1282718926668167\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                      nd 0.15785159170627594\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                val_loss 0.12967365980148315\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           best_val_loss 0.12306182831525803\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              best_epoch 30\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              best-nd_95 0.0010912670357206432\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       best-nrmse_maxmin 0.02196389303292158\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    best-nrmse_maxmin_95 0.00013762976456280194\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 best-nd 0.15955983942940866\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   best-nrmse_maxmin_std 0.0023405252194307483\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     best-nrmse_mean_std 0.022429697975067286\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      best-nrmse_mean_95 0.0013189321891921638\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         best-nrmse_mean 0.21048416098884587\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             best-nd_std 0.018558035221168456\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 7 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced giddy-lion-641: https://app.wandb.ai/matintavakoli/Interpreting_TS/runs/3g6y6mc7\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.9.7\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Wandb version 0.12.14 is available!  To upgrade, please run:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in wandb/run-20220418_030813-342nyzkw\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mhonest-breeze-642\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://app.wandb.ai/matintavakoli/Interpreting_TS\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://app.wandb.ai/matintavakoli/Interpreting_TS/runs/342nyzkw\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb off` to turn off syncing.\n",
            "\n",
            "/usr/local/envs/eval-metric/lib/python3.7/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  return f(**kwargs)\n",
            "/usr/local/envs/eval-metric/lib/python3.7/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  return f(**kwargs)\n",
            "  4%|█▍                                     | 101/2660 [00:00<00:10, 239.42it/s]\n",
            "(9090, 30, 10) (9090, 6) (1111, 30, 10) (1111, 6)\n",
            "(9090, 30, 10) (9090, 6) (1111, 30, 10) (1111, 6)\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 64895\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              _timestamp 1650251787.0006454\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                   _step 2\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                    loss 0.002898480778685172\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                      nd 0.07926563413698892\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                _runtime 9410.773062944412\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                  val_nd 0.09190478509225898\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                val_loss 0.019510198743783187\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              best-nd_95 0.0005363968253915866\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       best-nrmse_maxmin 0.019206251431713953\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    best-nrmse_maxmin_95 0.00021873024963076155\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 best-nd 0.10807951383746524\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   best-nrmse_maxmin_std 0.0037197162048444475\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     best-nrmse_mean_std 0.03564674729201012\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      best-nrmse_mean_95 0.0020961335500679964\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         best-nrmse_mean 0.1840571574577275\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             best-nd_std 0.009121938858499786\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 8 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced honest-breeze-642: https://app.wandb.ai/matintavakoli/Interpreting_TS/runs/342nyzkw\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.9.7\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Wandb version 0.12.14 is available!  To upgrade, please run:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in wandb/run-20220418_031630-31m2pls5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mlemon-frost-643\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://app.wandb.ai/matintavakoli/Interpreting_TS\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://app.wandb.ai/matintavakoli/Interpreting_TS/runs/31m2pls5\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb off` to turn off syncing.\n",
            "\n",
            "/usr/local/envs/eval-metric/lib/python3.7/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  return f(**kwargs)\n",
            "/usr/local/envs/eval-metric/lib/python3.7/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  return f(**kwargs)\n",
            "  4%|█▍                                     | 101/2660 [00:00<00:10, 236.88it/s]\n",
            "(9090, 30, 10) (9090, 6) (1111, 30, 10) (1111, 6)\n",
            "(9090, 30, 10) (9090, 6) (1111, 30, 10) (1111, 6)\n",
            "Model: \"sequential_86\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "reshape_43 (Reshape)         (None, 300)               0         \n",
            "_________________________________________________________________\n",
            "dense_186 (Dense)            (None, 64)                19264     \n",
            "_________________________________________________________________\n",
            "dropout_143 (Dropout)        (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_187 (Dense)            (None, 64)                4160      \n",
            "_________________________________________________________________\n",
            "dropout_144 (Dropout)        (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_188 (Dense)            (None, 64)                4160      \n",
            "_________________________________________________________________\n",
            "dropout_145 (Dropout)        (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_189 (Dense)            (None, 64)                4160      \n",
            "_________________________________________________________________\n",
            "dropout_146 (Dropout)        (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_190 (Dense)            (None, 6)                 390       \n",
            "=================================================================\n",
            "Total params: 32,134\n",
            "Trainable params: 32,134\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/200\n",
            "18/18 [==============================] - 0s 11ms/step - loss: 1.1189 - nd: 1.1019 - val_loss: 0.8420 - val_nd: 0.9051\n",
            "Epoch 2/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 1.0522 - nd: 0.9521 - val_loss: 0.8273 - val_nd: 0.9130\n",
            "Epoch 3/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 1.0423 - nd: 0.9610 - val_loss: 0.8251 - val_nd: 0.9129\n",
            "Epoch 4/200\n",
            "18/18 [==============================] - 0s 8ms/step - loss: 1.0348 - nd: 0.9329 - val_loss: 0.8223 - val_nd: 0.9019\n",
            "Epoch 5/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 1.0215 - nd: 0.9213 - val_loss: 0.8182 - val_nd: 0.8992\n",
            "Epoch 6/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 1.0117 - nd: 0.9087 - val_loss: 0.8086 - val_nd: 0.8928\n",
            "Epoch 7/200\n",
            "18/18 [==============================] - 0s 8ms/step - loss: 1.0021 - nd: 0.8894 - val_loss: 0.7848 - val_nd: 0.8679\n",
            "Epoch 8/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.9646 - nd: 0.8485 - val_loss: 0.7168 - val_nd: 0.7758\n",
            "Epoch 9/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.8933 - nd: 0.7486 - val_loss: 0.5557 - val_nd: 0.5469\n",
            "Epoch 10/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.7754 - nd: 0.6038 - val_loss: 0.3713 - val_nd: 0.3416\n",
            "Epoch 11/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.6717 - nd: 0.5208 - val_loss: 0.2759 - val_nd: 0.2635\n",
            "Epoch 12/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.6369 - nd: 0.4822 - val_loss: 0.2687 - val_nd: 0.2715\n",
            "Epoch 13/200\n",
            "18/18 [==============================] - 0s 8ms/step - loss: 0.6129 - nd: 0.4690 - val_loss: 0.2277 - val_nd: 0.2389\n",
            "Epoch 14/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.5945 - nd: 0.4355 - val_loss: 0.2157 - val_nd: 0.2336\n",
            "Epoch 15/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.5659 - nd: 0.4205 - val_loss: 0.2087 - val_nd: 0.2274\n",
            "Epoch 16/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.5375 - nd: 0.4091 - val_loss: 0.1827 - val_nd: 0.2110\n",
            "Epoch 17/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.5321 - nd: 0.3977 - val_loss: 0.1939 - val_nd: 0.2080\n",
            "Epoch 18/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.5409 - nd: 0.3856 - val_loss: 0.1816 - val_nd: 0.1980\n",
            "Epoch 19/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.5077 - nd: 0.3723 - val_loss: 0.1862 - val_nd: 0.2030\n",
            "Epoch 20/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.5228 - nd: 0.3779 - val_loss: 0.1729 - val_nd: 0.2026\n",
            "Epoch 21/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.4951 - nd: 0.3629 - val_loss: 0.1720 - val_nd: 0.1926\n",
            "Epoch 22/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.5047 - nd: 0.3608 - val_loss: 0.1780 - val_nd: 0.2009\n",
            "Epoch 23/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.5014 - nd: 0.3538 - val_loss: 0.1611 - val_nd: 0.1775\n",
            "Epoch 24/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.4943 - nd: 0.3523 - val_loss: 0.1766 - val_nd: 0.1932\n",
            "Epoch 25/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4838 - nd: 0.3449 - val_loss: 0.1769 - val_nd: 0.1827\n",
            "Epoch 26/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.4792 - nd: 0.3402 - val_loss: 0.1869 - val_nd: 0.2066\n",
            "Epoch 27/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4793 - nd: 0.3366 - val_loss: 0.1917 - val_nd: 0.2072\n",
            "Epoch 28/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4860 - nd: 0.3410 - val_loss: 0.1855 - val_nd: 0.1841\n",
            "Epoch 29/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4820 - nd: 0.3338 - val_loss: 0.1870 - val_nd: 0.1898\n",
            "Epoch 30/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.4760 - nd: 0.3305 - val_loss: 0.1794 - val_nd: 0.1838\n",
            "Epoch 31/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4768 - nd: 0.3294 - val_loss: 0.1924 - val_nd: 0.1849\n",
            "Epoch 32/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4750 - nd: 0.3306 - val_loss: 0.2001 - val_nd: 0.2002\n",
            "Epoch 33/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4778 - nd: 0.3283 - val_loss: 0.1878 - val_nd: 0.1765\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 65037\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                   _step 33\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                    loss 0.4778032898902893\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                   epoch 32\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                _runtime 9421.83094906807\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              _timestamp 1650251798.0585315\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                  val_nd 0.17651128768920898\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                      nd 0.3282996714115143\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                val_loss 0.18782801926136017\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           best_val_loss 0.16106875240802765\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              best_epoch 22\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              best-nd_95 0.0013437090263247599\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       best-nrmse_maxmin 0.027775642719854964\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    best-nrmse_maxmin_95 0.00034713219223806113\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 best-nd 0.20232600211094873\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   best-nrmse_maxmin_std 0.005903313523716196\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     best-nrmse_mean_std 0.05657257537318417\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      best-nrmse_mean_95 0.0033266337677901187\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         best-nrmse_mean 0.2661792627131982\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             best-nd_std 0.022851051686968084\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 7 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced lemon-frost-643: https://app.wandb.ai/matintavakoli/Interpreting_TS/runs/31m2pls5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.9.7\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Wandb version 0.12.14 is available!  To upgrade, please run:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in wandb/run-20220418_031643-1q6cyjgu\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mrosy-voice-644\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://app.wandb.ai/matintavakoli/Interpreting_TS\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://app.wandb.ai/matintavakoli/Interpreting_TS/runs/1q6cyjgu\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb off` to turn off syncing.\n",
            "\n",
            "/usr/local/envs/eval-metric/lib/python3.7/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  return f(**kwargs)\n",
            "/usr/local/envs/eval-metric/lib/python3.7/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  return f(**kwargs)\n",
            "  4%|█▍                                     | 101/2660 [00:00<00:11, 221.33it/s]\n",
            "(9090, 30, 10) (9090, 6) (1111, 30, 10) (1111, 6)\n",
            "(9090, 30, 10) (9090, 6) (1111, 30, 10) (1111, 6)\n",
            "Model: \"sequential_87\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm_100 (LSTM)              (None, 30, 64)            19200     \n",
            "_________________________________________________________________\n",
            "lstm_101 (LSTM)              (None, 30, 64)            33024     \n",
            "_________________________________________________________________\n",
            "lstm_102 (LSTM)              (None, 30, 64)            33024     \n",
            "_________________________________________________________________\n",
            "lstm_103 (LSTM)              (None, 64)                33024     \n",
            "_________________________________________________________________\n",
            "dropout_147 (Dropout)        (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_191 (Dense)            (None, 6)                 390       \n",
            "=================================================================\n",
            "Total params: 118,662\n",
            "Trainable params: 118,662\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/200\n",
            "18/18 [==============================] - 1s 78ms/step - loss: 0.9461 - nd: 0.8158 - val_loss: 0.7805 - val_nd: 0.6359\n",
            "Epoch 2/200\n",
            "18/18 [==============================] - 0s 18ms/step - loss: 0.6852 - nd: 0.5527 - val_loss: 0.4654 - val_nd: 0.5007\n",
            "Epoch 3/200\n",
            "18/18 [==============================] - 0s 16ms/step - loss: 0.5851 - nd: 0.4626 - val_loss: 0.4536 - val_nd: 0.4667\n",
            "Epoch 4/200\n",
            "18/18 [==============================] - 0s 16ms/step - loss: 0.5317 - nd: 0.4102 - val_loss: 0.3358 - val_nd: 0.3775\n",
            "Epoch 5/200\n",
            "18/18 [==============================] - 0s 14ms/step - loss: 0.4960 - nd: 0.3760 - val_loss: 0.3459 - val_nd: 0.3686\n",
            "Epoch 6/200\n",
            "18/18 [==============================] - 0s 17ms/step - loss: 0.4796 - nd: 0.3476 - val_loss: 0.3040 - val_nd: 0.3333\n",
            "Epoch 7/200\n",
            "18/18 [==============================] - 0s 17ms/step - loss: 0.4738 - nd: 0.3300 - val_loss: 0.2529 - val_nd: 0.3018\n",
            "Epoch 8/200\n",
            "18/18 [==============================] - 0s 14ms/step - loss: 0.4642 - nd: 0.3194 - val_loss: 0.2598 - val_nd: 0.3005\n",
            "Epoch 9/200\n",
            "18/18 [==============================] - 0s 16ms/step - loss: 0.4608 - nd: 0.3102 - val_loss: 0.2452 - val_nd: 0.2724\n",
            "Epoch 10/200\n",
            "18/18 [==============================] - 0s 14ms/step - loss: 0.4472 - nd: 0.3034 - val_loss: 0.2631 - val_nd: 0.2901\n",
            "Epoch 11/200\n",
            "18/18 [==============================] - 0s 17ms/step - loss: 0.4351 - nd: 0.2995 - val_loss: 0.2107 - val_nd: 0.2474\n",
            "Epoch 12/200\n",
            "18/18 [==============================] - 0s 17ms/step - loss: 0.4436 - nd: 0.2891 - val_loss: 0.1940 - val_nd: 0.2256\n",
            "Epoch 13/200\n",
            "18/18 [==============================] - 0s 15ms/step - loss: 0.4281 - nd: 0.2838 - val_loss: 0.2237 - val_nd: 0.2676\n",
            "Epoch 14/200\n",
            "18/18 [==============================] - 0s 14ms/step - loss: 0.4439 - nd: 0.3039 - val_loss: 0.2424 - val_nd: 0.2491\n",
            "Epoch 15/200\n",
            "18/18 [==============================] - 0s 14ms/step - loss: 0.4425 - nd: 0.3006 - val_loss: 0.2819 - val_nd: 0.3151\n",
            "Epoch 16/200\n",
            "18/18 [==============================] - 0s 14ms/step - loss: 0.4208 - nd: 0.2820 - val_loss: 0.1952 - val_nd: 0.2185\n",
            "Epoch 17/200\n",
            "18/18 [==============================] - 0s 14ms/step - loss: 0.4155 - nd: 0.2678 - val_loss: 0.1984 - val_nd: 0.2002\n",
            "Epoch 18/200\n",
            "18/18 [==============================] - 0s 14ms/step - loss: 0.4144 - nd: 0.2621 - val_loss: 0.1941 - val_nd: 0.2027\n",
            "Epoch 19/200\n",
            "18/18 [==============================] - 0s 17ms/step - loss: 0.4207 - nd: 0.2647 - val_loss: 0.1811 - val_nd: 0.1945\n",
            "Epoch 20/200\n",
            "18/18 [==============================] - 0s 14ms/step - loss: 0.4154 - nd: 0.2580 - val_loss: 0.1839 - val_nd: 0.1980\n",
            "Epoch 21/200\n",
            "18/18 [==============================] - 0s 15ms/step - loss: 0.4069 - nd: 0.2552 - val_loss: 0.1857 - val_nd: 0.1921\n",
            "Epoch 22/200\n",
            "18/18 [==============================] - 0s 14ms/step - loss: 0.3940 - nd: 0.2485 - val_loss: 0.1867 - val_nd: 0.1931\n",
            "Epoch 23/200\n",
            "18/18 [==============================] - 0s 15ms/step - loss: 0.4110 - nd: 0.2616 - val_loss: 0.1980 - val_nd: 0.1931\n",
            "Epoch 24/200\n",
            "18/18 [==============================] - 0s 14ms/step - loss: 0.3909 - nd: 0.2505 - val_loss: 0.1857 - val_nd: 0.1938\n",
            "Epoch 25/200\n",
            "18/18 [==============================] - 0s 14ms/step - loss: 0.4089 - nd: 0.2486 - val_loss: 0.1967 - val_nd: 0.1883\n",
            "Epoch 26/200\n",
            "18/18 [==============================] - 0s 14ms/step - loss: 0.4044 - nd: 0.2437 - val_loss: 0.1991 - val_nd: 0.1944\n",
            "Epoch 27/200\n",
            "18/18 [==============================] - 0s 14ms/step - loss: 0.4060 - nd: 0.2412 - val_loss: 0.2382 - val_nd: 0.2184\n",
            "Epoch 28/200\n",
            "18/18 [==============================] - 0s 14ms/step - loss: 0.4199 - nd: 0.2498 - val_loss: 0.3080 - val_nd: 0.2882\n",
            "Epoch 29/200\n",
            "18/18 [==============================] - 0s 14ms/step - loss: 0.4236 - nd: 0.2722 - val_loss: 0.2357 - val_nd: 0.2624\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 65193\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                   _step 29\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                    loss 0.4235856533050537\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                   epoch 28\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                _runtime 9444.749633550644\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              _timestamp 1650251820.977216\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                  val_nd 0.2624216377735138\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                      nd 0.2721574008464813\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                val_loss 0.2356633096933365\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           best_val_loss 0.18109141290187836\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              best_epoch 18\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              best-nd_95 0.004175813046857932\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       best-nrmse_maxmin 0.03202221533724694\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    best-nrmse_maxmin_95 0.00024187099572201085\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 best-nd 0.24533564229420757\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   best-nrmse_maxmin_std 0.004113246630439981\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     best-nrmse_mean_std 0.039418024147661476\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      best-nrmse_mean_95 0.0023178957175658877\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         best-nrmse_mean 0.3068749751313105\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             best-nd_std 0.07101367773784968\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 7 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced rosy-voice-644: https://app.wandb.ai/matintavakoli/Interpreting_TS/runs/1q6cyjgu\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.9.7\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Wandb version 0.12.14 is available!  To upgrade, please run:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in wandb/run-20220418_031706-1kdzrbfj\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33methereal-wildflower-645\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://app.wandb.ai/matintavakoli/Interpreting_TS\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://app.wandb.ai/matintavakoli/Interpreting_TS/runs/1kdzrbfj\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb off` to turn off syncing.\n",
            "\n",
            "/usr/local/envs/eval-metric/lib/python3.7/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  return f(**kwargs)\n",
            "/usr/local/envs/eval-metric/lib/python3.7/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  return f(**kwargs)\n",
            "  4%|█▍                                     | 101/2660 [00:00<00:11, 221.63it/s]\n",
            "(9090, 30, 10) (9090, 6) (1111, 30, 10) (1111, 6)\n",
            "(9090, 30, 10) (9090, 6) (1111, 30, 10) (1111, 6)\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 65339\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              _timestamp 1650252319.433991\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                   _step 2\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                    loss 0.0029906789496784627\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                      nd 0.08078431348599559\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                _runtime 9943.206408500671\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                  val_nd 0.09317283043326365\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                val_loss 0.019486815767190645\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              best-nd_95 0.0005960115714613389\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       best-nrmse_maxmin 0.019160514308653537\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    best-nrmse_maxmin_95 0.00020479424700769647\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 best-nd 0.10928543876126146\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   best-nrmse_maxmin_std 0.0034827212081520492\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     best-nrmse_mean_std 0.03337557919978789\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      best-nrmse_mean_95 0.001962582188510304\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         best-nrmse_mean 0.1836188499154814\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             best-nd_std 0.010135744390097179\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 8 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced ethereal-wildflower-645: https://app.wandb.ai/matintavakoli/Interpreting_TS/runs/1kdzrbfj\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.9.7\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Wandb version 0.12.14 is available!  To upgrade, please run:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in wandb/run-20220418_032523-1t06xcxn\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mcrimson-field-646\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://app.wandb.ai/matintavakoli/Interpreting_TS\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://app.wandb.ai/matintavakoli/Interpreting_TS/runs/1t06xcxn\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb off` to turn off syncing.\n",
            "\n",
            "/usr/local/envs/eval-metric/lib/python3.7/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  return f(**kwargs)\n",
            "/usr/local/envs/eval-metric/lib/python3.7/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  return f(**kwargs)\n",
            "  4%|█▍                                     | 101/2660 [00:00<00:10, 237.34it/s]\n",
            "(9090, 30, 10) (9090, 6) (1111, 30, 10) (1111, 6)\n",
            "(9090, 30, 10) (9090, 6) (1111, 30, 10) (1111, 6)\n",
            "Model: \"sequential_88\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "reshape_44 (Reshape)         (None, 300)               0         \n",
            "_________________________________________________________________\n",
            "dense_192 (Dense)            (None, 64)                19264     \n",
            "_________________________________________________________________\n",
            "dropout_148 (Dropout)        (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_193 (Dense)            (None, 64)                4160      \n",
            "_________________________________________________________________\n",
            "dropout_149 (Dropout)        (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_194 (Dense)            (None, 64)                4160      \n",
            "_________________________________________________________________\n",
            "dropout_150 (Dropout)        (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_195 (Dense)            (None, 64)                4160      \n",
            "_________________________________________________________________\n",
            "dropout_151 (Dropout)        (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_196 (Dense)            (None, 6)                 390       \n",
            "=================================================================\n",
            "Total params: 32,134\n",
            "Trainable params: 32,134\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/200\n",
            "18/18 [==============================] - 0s 11ms/step - loss: 1.3111 - nd: 1.4081 - val_loss: 0.8305 - val_nd: 0.9172\n",
            "Epoch 2/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 1.1603 - nd: 1.1518 - val_loss: 0.8300 - val_nd: 0.9162\n",
            "Epoch 3/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 1.1051 - nd: 1.0615 - val_loss: 0.8273 - val_nd: 0.9179\n",
            "Epoch 4/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 1.0724 - nd: 1.0052 - val_loss: 0.8277 - val_nd: 0.9228\n",
            "Epoch 5/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 1.0429 - nd: 0.9665 - val_loss: 0.8277 - val_nd: 0.9241\n",
            "Epoch 6/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 1.0333 - nd: 0.9431 - val_loss: 0.8277 - val_nd: 0.9240\n",
            "Epoch 7/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 1.0178 - nd: 0.9335 - val_loss: 0.8282 - val_nd: 0.9291\n",
            "Epoch 8/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 1.0126 - nd: 0.9234 - val_loss: 0.8274 - val_nd: 0.9255\n",
            "Epoch 9/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 1.0128 - nd: 0.9165 - val_loss: 0.8273 - val_nd: 0.9269\n",
            "Epoch 10/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 1.0103 - nd: 0.9119 - val_loss: 0.8265 - val_nd: 0.9247\n",
            "Epoch 11/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 1.0112 - nd: 0.9107 - val_loss: 0.8256 - val_nd: 0.9242\n",
            "Epoch 12/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 1.0099 - nd: 0.9019 - val_loss: 0.8232 - val_nd: 0.9171\n",
            "Epoch 13/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 1.0099 - nd: 0.8967 - val_loss: 0.8196 - val_nd: 0.9092\n",
            "Epoch 14/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 1.0035 - nd: 0.8858 - val_loss: 0.8128 - val_nd: 0.8943\n",
            "Epoch 15/200\n",
            "18/18 [==============================] - 0s 8ms/step - loss: 0.9939 - nd: 0.8722 - val_loss: 0.7993 - val_nd: 0.8655\n",
            "Epoch 16/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.9849 - nd: 0.8388 - val_loss: 0.7715 - val_nd: 0.8056\n",
            "Epoch 17/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.9626 - nd: 0.8014 - val_loss: 0.7096 - val_nd: 0.7007\n",
            "Epoch 18/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.9117 - nd: 0.7405 - val_loss: 0.5955 - val_nd: 0.5216\n",
            "Epoch 19/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.8515 - nd: 0.6911 - val_loss: 0.5120 - val_nd: 0.4624\n",
            "Epoch 20/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.8131 - nd: 0.6566 - val_loss: 0.4267 - val_nd: 0.3955\n",
            "Epoch 21/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.7596 - nd: 0.6161 - val_loss: 0.3705 - val_nd: 0.3759\n",
            "Epoch 22/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.7266 - nd: 0.5881 - val_loss: 0.3333 - val_nd: 0.3547\n",
            "Epoch 23/200\n",
            "18/18 [==============================] - 0s 8ms/step - loss: 0.7235 - nd: 0.5719 - val_loss: 0.3259 - val_nd: 0.3619\n",
            "Epoch 24/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.7106 - nd: 0.5825 - val_loss: 0.3491 - val_nd: 0.4172\n",
            "Epoch 25/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.6906 - nd: 0.5496 - val_loss: 0.2756 - val_nd: 0.2950\n",
            "Epoch 26/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.6816 - nd: 0.5378 - val_loss: 0.2937 - val_nd: 0.3231\n",
            "Epoch 27/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.6676 - nd: 0.5268 - val_loss: 0.3086 - val_nd: 0.3524\n",
            "Epoch 28/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.6679 - nd: 0.5248 - val_loss: 0.2921 - val_nd: 0.3163\n",
            "Epoch 29/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.6569 - nd: 0.5130 - val_loss: 0.2728 - val_nd: 0.3056\n",
            "Epoch 30/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.6457 - nd: 0.5033 - val_loss: 0.2553 - val_nd: 0.2826\n",
            "Epoch 31/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.6373 - nd: 0.4951 - val_loss: 0.2652 - val_nd: 0.2976\n",
            "Epoch 32/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.6466 - nd: 0.5052 - val_loss: 0.2438 - val_nd: 0.2801\n",
            "Epoch 33/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.6452 - nd: 0.5015 - val_loss: 0.2535 - val_nd: 0.2963\n",
            "Epoch 34/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.6418 - nd: 0.5202 - val_loss: 0.2671 - val_nd: 0.3233\n",
            "Epoch 35/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.6378 - nd: 0.5132 - val_loss: 0.2485 - val_nd: 0.2960\n",
            "Epoch 36/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.6275 - nd: 0.4991 - val_loss: 0.2562 - val_nd: 0.3043\n",
            "Epoch 37/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.6381 - nd: 0.5069 - val_loss: 0.2528 - val_nd: 0.3102\n",
            "Epoch 38/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.6353 - nd: 0.4956 - val_loss: 0.2353 - val_nd: 0.2847\n",
            "Epoch 39/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.6145 - nd: 0.5065 - val_loss: 0.2863 - val_nd: 0.3486\n",
            "Epoch 40/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.6334 - nd: 0.5281 - val_loss: 0.2605 - val_nd: 0.3252\n",
            "Epoch 41/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.6327 - nd: 0.5071 - val_loss: 0.2855 - val_nd: 0.3304\n",
            "Epoch 42/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.6115 - nd: 0.5164 - val_loss: 0.2974 - val_nd: 0.3833\n",
            "Epoch 43/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.6270 - nd: 0.5090 - val_loss: 0.2763 - val_nd: 0.3479\n",
            "Epoch 44/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.6185 - nd: 0.5103 - val_loss: 0.2931 - val_nd: 0.3690\n",
            "Epoch 45/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5937 - nd: 0.4995 - val_loss: 0.2659 - val_nd: 0.3307\n",
            "Epoch 46/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.6035 - nd: 0.4914 - val_loss: 0.2697 - val_nd: 0.3427\n",
            "Epoch 47/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.6115 - nd: 0.4952 - val_loss: 0.2907 - val_nd: 0.3673\n",
            "Epoch 48/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.5948 - nd: 0.5021 - val_loss: 0.2790 - val_nd: 0.3458\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 65481\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                   _step 48\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                    loss 0.594794750213623\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                   epoch 47\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                _runtime 9956.11028265953\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              _timestamp 1650252332.337865\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                  val_nd 0.34581711888313293\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                      nd 0.5020760297775269\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                val_loss 0.27899298071861267\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           best_val_loss 0.2352534383535385\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              best_epoch 37\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              best-nd_95 0.00111325226961289\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       best-nrmse_maxmin 0.041407161898760314\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    best-nrmse_maxmin_95 0.00025359054696306326\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 best-nd 0.33712584570352566\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   best-nrmse_maxmin_std 0.004312548760522296\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     best-nrmse_mean_std 0.04132797433594495\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      best-nrmse_mean_95 0.0024302064043116886\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         best-nrmse_mean 0.3968127015609742\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             best-nd_std 0.01893191506135671\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 7 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced crimson-field-646: https://app.wandb.ai/matintavakoli/Interpreting_TS/runs/1t06xcxn\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.9.7\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Wandb version 0.12.14 is available!  To upgrade, please run:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in wandb/run-20220418_032537-zy9scpqt\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mtrue-sound-647\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://app.wandb.ai/matintavakoli/Interpreting_TS\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://app.wandb.ai/matintavakoli/Interpreting_TS/runs/zy9scpqt\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb off` to turn off syncing.\n",
            "\n",
            "/usr/local/envs/eval-metric/lib/python3.7/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  return f(**kwargs)\n",
            "/usr/local/envs/eval-metric/lib/python3.7/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  return f(**kwargs)\n",
            "  4%|█▍                                     | 101/2660 [00:00<00:10, 237.85it/s]\n",
            "(9090, 30, 10) (9090, 6) (1111, 30, 10) (1111, 6)\n",
            "(9090, 30, 10) (9090, 6) (1111, 30, 10) (1111, 6)\n",
            "Model: \"sequential_89\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm_104 (LSTM)              (None, 30, 64)            19200     \n",
            "_________________________________________________________________\n",
            "lstm_105 (LSTM)              (None, 30, 64)            33024     \n",
            "_________________________________________________________________\n",
            "lstm_106 (LSTM)              (None, 30, 64)            33024     \n",
            "_________________________________________________________________\n",
            "lstm_107 (LSTM)              (None, 64)                33024     \n",
            "_________________________________________________________________\n",
            "dropout_152 (Dropout)        (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_197 (Dense)            (None, 6)                 390       \n",
            "=================================================================\n",
            "Total params: 118,662\n",
            "Trainable params: 118,662\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/200\n",
            "18/18 [==============================] - 1s 78ms/step - loss: 0.9773 - nd: 0.8832 - val_loss: 0.7854 - val_nd: 0.7366\n",
            "Epoch 2/200\n",
            "18/18 [==============================] - 0s 16ms/step - loss: 0.8063 - nd: 0.6794 - val_loss: 0.8044 - val_nd: 0.7912\n",
            "Epoch 3/200\n",
            "18/18 [==============================] - 0s 16ms/step - loss: 0.6840 - nd: 0.5809 - val_loss: 0.7528 - val_nd: 0.6922\n",
            "Epoch 4/200\n",
            "18/18 [==============================] - 0s 17ms/step - loss: 0.6387 - nd: 0.5203 - val_loss: 0.5504 - val_nd: 0.5523\n",
            "Epoch 5/200\n",
            "18/18 [==============================] - 0s 18ms/step - loss: 0.5984 - nd: 0.4791 - val_loss: 0.4723 - val_nd: 0.4710\n",
            "Epoch 6/200\n",
            "18/18 [==============================] - 0s 17ms/step - loss: 0.5758 - nd: 0.4596 - val_loss: 0.4511 - val_nd: 0.5342\n",
            "Epoch 7/200\n",
            "18/18 [==============================] - 0s 15ms/step - loss: 0.5656 - nd: 0.4486 - val_loss: 0.4815 - val_nd: 0.5713\n",
            "Epoch 8/200\n",
            "18/18 [==============================] - 0s 17ms/step - loss: 0.5424 - nd: 0.4277 - val_loss: 0.3836 - val_nd: 0.4540\n",
            "Epoch 9/200\n",
            "18/18 [==============================] - 0s 14ms/step - loss: 0.5261 - nd: 0.4018 - val_loss: 0.4506 - val_nd: 0.5091\n",
            "Epoch 10/200\n",
            "18/18 [==============================] - 0s 14ms/step - loss: 0.5069 - nd: 0.3905 - val_loss: 0.3991 - val_nd: 0.4631\n",
            "Epoch 11/200\n",
            "18/18 [==============================] - 0s 17ms/step - loss: 0.5140 - nd: 0.3805 - val_loss: 0.3647 - val_nd: 0.4295\n",
            "Epoch 12/200\n",
            "18/18 [==============================] - 0s 15ms/step - loss: 0.5044 - nd: 0.3719 - val_loss: 0.4189 - val_nd: 0.5077\n",
            "Epoch 13/200\n",
            "18/18 [==============================] - 0s 14ms/step - loss: 0.5084 - nd: 0.3641 - val_loss: 0.3964 - val_nd: 0.4807\n",
            "Epoch 14/200\n",
            "18/18 [==============================] - 0s 17ms/step - loss: 0.4943 - nd: 0.3575 - val_loss: 0.3459 - val_nd: 0.4125\n",
            "Epoch 15/200\n",
            "18/18 [==============================] - 0s 15ms/step - loss: 0.4908 - nd: 0.3591 - val_loss: 0.4024 - val_nd: 0.4612\n",
            "Epoch 16/200\n",
            "18/18 [==============================] - 0s 14ms/step - loss: 0.4817 - nd: 0.3415 - val_loss: 0.3764 - val_nd: 0.4268\n",
            "Epoch 17/200\n",
            "18/18 [==============================] - 0s 14ms/step - loss: 0.4872 - nd: 0.3440 - val_loss: 0.3695 - val_nd: 0.4403\n",
            "Epoch 18/200\n",
            "18/18 [==============================] - 0s 14ms/step - loss: 0.4945 - nd: 0.3386 - val_loss: 0.3570 - val_nd: 0.3956\n",
            "Epoch 19/200\n",
            "18/18 [==============================] - 0s 14ms/step - loss: 0.4838 - nd: 0.3394 - val_loss: 0.3573 - val_nd: 0.4266\n",
            "Epoch 20/200\n",
            "18/18 [==============================] - 0s 17ms/step - loss: 0.4752 - nd: 0.3298 - val_loss: 0.3383 - val_nd: 0.3661\n",
            "Epoch 21/200\n",
            "18/18 [==============================] - 0s 17ms/step - loss: 0.4733 - nd: 0.3225 - val_loss: 0.3197 - val_nd: 0.3571\n",
            "Epoch 22/200\n",
            "18/18 [==============================] - 0s 15ms/step - loss: 0.4763 - nd: 0.3258 - val_loss: 0.3443 - val_nd: 0.3738\n",
            "Epoch 23/200\n",
            "18/18 [==============================] - 0s 15ms/step - loss: 0.4782 - nd: 0.3257 - val_loss: 0.3217 - val_nd: 0.3640\n",
            "Epoch 24/200\n",
            "18/18 [==============================] - 0s 14ms/step - loss: 0.4655 - nd: 0.3196 - val_loss: 0.3315 - val_nd: 0.3696\n",
            "Epoch 25/200\n",
            "18/18 [==============================] - 0s 17ms/step - loss: 0.4663 - nd: 0.3180 - val_loss: 0.3039 - val_nd: 0.3275\n",
            "Epoch 26/200\n",
            "18/18 [==============================] - 0s 17ms/step - loss: 0.4643 - nd: 0.3139 - val_loss: 0.2892 - val_nd: 0.3229\n",
            "Epoch 27/200\n",
            "18/18 [==============================] - 0s 15ms/step - loss: 0.4711 - nd: 0.3154 - val_loss: 0.3205 - val_nd: 0.3625\n",
            "Epoch 28/200\n",
            "18/18 [==============================] - 0s 15ms/step - loss: 0.4702 - nd: 0.3084 - val_loss: 0.3061 - val_nd: 0.3401\n",
            "Epoch 29/200\n",
            "18/18 [==============================] - 0s 14ms/step - loss: 0.4613 - nd: 0.3063 - val_loss: 0.2991 - val_nd: 0.3306\n",
            "Epoch 30/200\n",
            "18/18 [==============================] - 0s 14ms/step - loss: 0.4560 - nd: 0.3040 - val_loss: 0.3169 - val_nd: 0.3662\n",
            "Epoch 31/200\n",
            "18/18 [==============================] - 0s 14ms/step - loss: 0.4524 - nd: 0.3041 - val_loss: 0.3137 - val_nd: 0.3630\n",
            "Epoch 32/200\n",
            "18/18 [==============================] - 0s 14ms/step - loss: 0.4584 - nd: 0.3045 - val_loss: 0.3193 - val_nd: 0.3888\n",
            "Epoch 33/200\n",
            "18/18 [==============================] - 0s 14ms/step - loss: 0.4565 - nd: 0.2996 - val_loss: 0.2943 - val_nd: 0.3349\n",
            "Epoch 34/200\n",
            "18/18 [==============================] - 0s 16ms/step - loss: 0.4527 - nd: 0.2948 - val_loss: 0.2735 - val_nd: 0.2969\n",
            "Epoch 35/200\n",
            "18/18 [==============================] - 0s 16ms/step - loss: 0.4552 - nd: 0.3029 - val_loss: 0.2589 - val_nd: 0.2800\n",
            "Epoch 36/200\n",
            "18/18 [==============================] - 0s 16ms/step - loss: 0.4501 - nd: 0.2972 - val_loss: 0.2594 - val_nd: 0.2855\n",
            "Epoch 37/200\n",
            "18/18 [==============================] - 0s 14ms/step - loss: 0.4481 - nd: 0.2971 - val_loss: 0.2768 - val_nd: 0.3152\n",
            "Epoch 38/200\n",
            "18/18 [==============================] - 0s 14ms/step - loss: 0.4473 - nd: 0.3030 - val_loss: 0.2898 - val_nd: 0.3289\n",
            "Epoch 39/200\n",
            "18/18 [==============================] - 0s 14ms/step - loss: 0.4477 - nd: 0.2904 - val_loss: 0.2740 - val_nd: 0.3082\n",
            "Epoch 40/200\n",
            "18/18 [==============================] - 0s 14ms/step - loss: 0.4463 - nd: 0.2955 - val_loss: 0.2622 - val_nd: 0.3077\n",
            "Epoch 41/200\n",
            "18/18 [==============================] - 0s 14ms/step - loss: 0.4527 - nd: 0.2898 - val_loss: 0.2889 - val_nd: 0.3492\n",
            "Epoch 42/200\n",
            "18/18 [==============================] - 0s 14ms/step - loss: 0.4418 - nd: 0.2870 - val_loss: 0.2761 - val_nd: 0.2985\n",
            "Epoch 43/200\n",
            "18/18 [==============================] - 0s 14ms/step - loss: 0.4409 - nd: 0.2887 - val_loss: 0.2926 - val_nd: 0.3219\n",
            "Epoch 44/200\n",
            "18/18 [==============================] - 0s 15ms/step - loss: 0.4413 - nd: 0.2846 - val_loss: 0.2784 - val_nd: 0.3153\n",
            "Epoch 45/200\n",
            "18/18 [==============================] - 0s 14ms/step - loss: 0.4332 - nd: 0.2851 - val_loss: 0.2961 - val_nd: 0.3282\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 65682\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                   _step 45\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                    loss 0.4331951141357422\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                   epoch 44\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                _runtime 9983.236670970917\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              _timestamp 1650252359.4642534\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                  val_nd 0.32820925116539\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                      nd 0.28505462408065796\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                val_loss 0.2960517108440399\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           best_val_loss 0.25888094305992126\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              best_epoch 34\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              best-nd_95 0.003993727287640323\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       best-nrmse_maxmin 0.046798741243578655\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    best-nrmse_maxmin_95 0.00029039000429440507\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 best-nd 0.3718526011397739\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   best-nrmse_maxmin_std 0.004938358578761644\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     best-nrmse_mean_std 0.047325228754059805\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      best-nrmse_mean_95 0.002782862597347336\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         best-nrmse_mean 0.4484812310469752\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             best-nd_std 0.06791713599121654\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 7 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced true-sound-647: https://app.wandb.ai/matintavakoli/Interpreting_TS/runs/zy9scpqt\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.9.7\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Wandb version 0.12.14 is available!  To upgrade, please run:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in wandb/run-20220418_032604-18qv45mo\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mstilted-armadillo-648\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://app.wandb.ai/matintavakoli/Interpreting_TS\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://app.wandb.ai/matintavakoli/Interpreting_TS/runs/18qv45mo\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb off` to turn off syncing.\n",
            "\n",
            "/usr/local/envs/eval-metric/lib/python3.7/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  return f(**kwargs)\n",
            "/usr/local/envs/eval-metric/lib/python3.7/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  return f(**kwargs)\n",
            "  4%|█▍                                     | 101/2660 [00:00<00:10, 235.86it/s]\n",
            "(9090, 30, 10) (9090, 6) (1111, 30, 10) (1111, 6)\n",
            "(9090, 30, 10) (9090, 6) (1111, 30, 10) (1111, 6)\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 65877\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              _timestamp 1650252974.1411467\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                   _step 2\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                    loss 0.0020290591610504353\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                      nd 0.060117089815114204\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                _runtime 10597.91356420517\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                  val_nd 0.09184958040023568\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                val_loss 0.019963845654319336\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              best-nd_95 0.0005208157807196811\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       best-nrmse_maxmin 0.01927804535824194\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    best-nrmse_maxmin_95 0.000242629357125296\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 best-nd 0.10735371242849699\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   best-nrmse_maxmin_std 0.004126143288335678\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     best-nrmse_mean_std 0.039541615271178034\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      best-nrmse_mean_95 0.0023251632390138186\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         best-nrmse_mean 0.18474517229948106\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             best-nd_std 0.008856968355095116\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 8 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced stilted-armadillo-648: https://app.wandb.ai/matintavakoli/Interpreting_TS/runs/18qv45mo\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.9.7\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Wandb version 0.12.14 is available!  To upgrade, please run:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in wandb/run-20220418_033618-2vcr7gyg\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mtrue-dew-649\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://app.wandb.ai/matintavakoli/Interpreting_TS\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://app.wandb.ai/matintavakoli/Interpreting_TS/runs/2vcr7gyg\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb off` to turn off syncing.\n",
            "\n",
            "/usr/local/envs/eval-metric/lib/python3.7/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  return f(**kwargs)\n",
            "/usr/local/envs/eval-metric/lib/python3.7/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  return f(**kwargs)\n",
            "  4%|█▍                                     | 101/2660 [00:00<00:10, 246.05it/s]\n",
            "(9090, 30, 10) (9090, 6) (1111, 30, 10) (1111, 6)\n",
            "(9090, 30, 10) (9090, 6) (1111, 30, 10) (1111, 6)\n",
            "Model: \"sequential_90\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "reshape_45 (Reshape)         (None, 300)               0         \n",
            "_________________________________________________________________\n",
            "dense_198 (Dense)            (None, 128)               38528     \n",
            "_________________________________________________________________\n",
            "dropout_153 (Dropout)        (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_199 (Dense)            (None, 128)               16512     \n",
            "_________________________________________________________________\n",
            "dropout_154 (Dropout)        (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_200 (Dense)            (None, 128)               16512     \n",
            "_________________________________________________________________\n",
            "dropout_155 (Dropout)        (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_201 (Dense)            (None, 128)               16512     \n",
            "_________________________________________________________________\n",
            "dropout_156 (Dropout)        (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_202 (Dense)            (None, 6)                 774       \n",
            "=================================================================\n",
            "Total params: 88,838\n",
            "Trainable params: 88,838\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/200\n",
            "18/18 [==============================] - 0s 11ms/step - loss: 1.0309 - nd: 0.9242 - val_loss: 0.8322 - val_nd: 0.9536\n",
            "Epoch 2/200\n",
            "18/18 [==============================] - 0s 8ms/step - loss: 0.9998 - nd: 0.9049 - val_loss: 0.8183 - val_nd: 0.8835\n",
            "Epoch 3/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.9914 - nd: 0.8577 - val_loss: 0.7873 - val_nd: 0.8840\n",
            "Epoch 4/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.9195 - nd: 0.7746 - val_loss: 0.6060 - val_nd: 0.6709\n",
            "Epoch 5/200\n",
            "18/18 [==============================] - 0s 8ms/step - loss: 0.6790 - nd: 0.4914 - val_loss: 0.3096 - val_nd: 0.3442\n",
            "Epoch 6/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.5235 - nd: 0.3433 - val_loss: 0.2114 - val_nd: 0.2234\n",
            "Epoch 7/200\n",
            "18/18 [==============================] - 0s 8ms/step - loss: 0.4845 - nd: 0.2858 - val_loss: 0.1694 - val_nd: 0.1845\n",
            "Epoch 8/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4318 - nd: 0.2351 - val_loss: 0.1842 - val_nd: 0.2268\n",
            "Epoch 9/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.4421 - nd: 0.2846 - val_loss: 0.2031 - val_nd: 0.2104\n",
            "Epoch 10/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.4026 - nd: 0.2353 - val_loss: 0.1675 - val_nd: 0.1562\n",
            "Epoch 11/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4164 - nd: 0.2087 - val_loss: 0.1692 - val_nd: 0.1516\n",
            "Epoch 12/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4128 - nd: 0.2238 - val_loss: 0.1712 - val_nd: 0.1470\n",
            "Epoch 13/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4215 - nd: 0.2108 - val_loss: 0.1783 - val_nd: 0.1661\n",
            "Epoch 14/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.3987 - nd: 0.2028 - val_loss: 0.1756 - val_nd: 0.1475\n",
            "Epoch 15/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.3885 - nd: 0.1931 - val_loss: 0.1766 - val_nd: 0.1508\n",
            "Epoch 16/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.3957 - nd: 0.2047 - val_loss: 0.2089 - val_nd: 0.1999\n",
            "Epoch 17/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4104 - nd: 0.2218 - val_loss: 0.1753 - val_nd: 0.1458\n",
            "Epoch 18/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.4122 - nd: 0.2156 - val_loss: 0.2177 - val_nd: 0.2329\n",
            "Epoch 19/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.4067 - nd: 0.2063 - val_loss: 0.1649 - val_nd: 0.1438\n",
            "Epoch 20/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.3868 - nd: 0.1868 - val_loss: 0.1614 - val_nd: 0.1385\n",
            "Epoch 21/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.3940 - nd: 0.1843 - val_loss: 0.1660 - val_nd: 0.1380\n",
            "Epoch 22/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.3806 - nd: 0.1805 - val_loss: 0.1608 - val_nd: 0.1325\n",
            "Epoch 23/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.3891 - nd: 0.2061 - val_loss: 0.2389 - val_nd: 0.2647\n",
            "Epoch 24/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4102 - nd: 0.2293 - val_loss: 0.1691 - val_nd: 0.1651\n",
            "Epoch 25/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.3793 - nd: 0.1885 - val_loss: 0.1624 - val_nd: 0.1341\n",
            "Epoch 26/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.3713 - nd: 0.1821 - val_loss: 0.1513 - val_nd: 0.1347\n",
            "Epoch 27/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.3959 - nd: 0.1796 - val_loss: 0.1522 - val_nd: 0.1287\n",
            "Epoch 28/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.3829 - nd: 0.1773 - val_loss: 0.1434 - val_nd: 0.1353\n",
            "Epoch 29/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.3827 - nd: 0.1796 - val_loss: 0.1507 - val_nd: 0.1523\n",
            "Epoch 30/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.4012 - nd: 0.2129 - val_loss: 0.1305 - val_nd: 0.1258\n",
            "Epoch 31/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.3999 - nd: 0.2394 - val_loss: 0.1722 - val_nd: 0.2039\n",
            "Epoch 32/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.3946 - nd: 0.2185 - val_loss: 0.1321 - val_nd: 0.1278\n",
            "Epoch 33/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.3689 - nd: 0.1821 - val_loss: 0.1389 - val_nd: 0.1457\n",
            "Epoch 34/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.3658 - nd: 0.1825 - val_loss: 0.1408 - val_nd: 0.1356\n",
            "Epoch 35/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.3622 - nd: 0.1757 - val_loss: 0.1497 - val_nd: 0.1636\n",
            "Epoch 36/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.3659 - nd: 0.1953 - val_loss: 0.1549 - val_nd: 0.1699\n",
            "Epoch 37/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.3638 - nd: 0.1831 - val_loss: 0.1249 - val_nd: 0.1220\n",
            "Epoch 38/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.3718 - nd: 0.1766 - val_loss: 0.1333 - val_nd: 0.1248\n",
            "Epoch 39/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.3615 - nd: 0.1915 - val_loss: 0.1375 - val_nd: 0.1458\n",
            "Epoch 40/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.3644 - nd: 0.2106 - val_loss: 0.1531 - val_nd: 0.1721\n",
            "Epoch 41/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.3681 - nd: 0.1952 - val_loss: 0.1290 - val_nd: 0.1360\n",
            "Epoch 42/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.3610 - nd: 0.1747 - val_loss: 0.1217 - val_nd: 0.1197\n",
            "Epoch 43/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.3698 - nd: 0.1745 - val_loss: 0.1248 - val_nd: 0.1222\n",
            "Epoch 44/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.3585 - nd: 0.1667 - val_loss: 0.1314 - val_nd: 0.1358\n",
            "Epoch 45/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.3485 - nd: 0.1804 - val_loss: 0.1306 - val_nd: 0.1315\n",
            "Epoch 46/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.3501 - nd: 0.1665 - val_loss: 0.1209 - val_nd: 0.1172\n",
            "Epoch 47/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.3572 - nd: 0.1735 - val_loss: 0.1260 - val_nd: 0.1282\n",
            "Epoch 48/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.3650 - nd: 0.1705 - val_loss: 0.1249 - val_nd: 0.1196\n",
            "Epoch 49/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.3577 - nd: 0.1883 - val_loss: 0.1463 - val_nd: 0.1571\n",
            "Epoch 50/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.3708 - nd: 0.1928 - val_loss: 0.1439 - val_nd: 0.1300\n",
            "Epoch 51/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.3765 - nd: 0.1855 - val_loss: 0.1439 - val_nd: 0.1433\n",
            "Epoch 52/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.3585 - nd: 0.1933 - val_loss: 0.1581 - val_nd: 0.1713\n",
            "Epoch 53/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.3635 - nd: 0.1971 - val_loss: 0.1310 - val_nd: 0.1217\n",
            "Epoch 54/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.3621 - nd: 0.2027 - val_loss: 0.1310 - val_nd: 0.1196\n",
            "Epoch 55/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.3578 - nd: 0.1950 - val_loss: 0.1361 - val_nd: 0.1342\n",
            "Epoch 56/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.3466 - nd: 0.1783 - val_loss: 0.1316 - val_nd: 0.1232\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 66038\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                   _step 56\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                    loss 0.3466481864452362\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                   epoch 55\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                _runtime 10610.119901657104\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              _timestamp 1650252986.347484\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                  val_nd 0.12317561358213425\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                      nd 0.1783287227153778\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                val_loss 0.13163122534751892\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           best_val_loss 0.12091287970542908\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              best_epoch 45\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              best-nd_95 0.001263942391427656\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       best-nrmse_maxmin 0.021722387252127724\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    best-nrmse_maxmin_95 0.00014769860583167154\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 best-nd 0.1507959369190843\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   best-nrmse_maxmin_std 0.0025117554543664534\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     best-nrmse_mean_std 0.024070629857331625\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      best-nrmse_mean_95 0.0014154238085708965\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         best-nrmse_mean 0.20816976519534255\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             best-nd_std 0.021494544094014925\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 7 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced true-dew-649: https://app.wandb.ai/matintavakoli/Interpreting_TS/runs/2vcr7gyg\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.9.7\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Wandb version 0.12.14 is available!  To upgrade, please run:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in wandb/run-20220418_033632-2mczf64w\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mefficient-wind-650\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://app.wandb.ai/matintavakoli/Interpreting_TS\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://app.wandb.ai/matintavakoli/Interpreting_TS/runs/2mczf64w\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb off` to turn off syncing.\n",
            "\n",
            "/usr/local/envs/eval-metric/lib/python3.7/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  return f(**kwargs)\n",
            "/usr/local/envs/eval-metric/lib/python3.7/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  return f(**kwargs)\n",
            "  4%|█▍                                     | 101/2660 [00:00<00:13, 195.22it/s]\n",
            "(9090, 30, 10) (9090, 6) (1111, 30, 10) (1111, 6)\n",
            "(9090, 30, 10) (9090, 6) (1111, 30, 10) (1111, 6)\n",
            "Model: \"sequential_91\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm_108 (LSTM)              (None, 30, 128)           71168     \n",
            "_________________________________________________________________\n",
            "lstm_109 (LSTM)              (None, 30, 128)           131584    \n",
            "_________________________________________________________________\n",
            "lstm_110 (LSTM)              (None, 30, 128)           131584    \n",
            "_________________________________________________________________\n",
            "lstm_111 (LSTM)              (None, 128)               131584    \n",
            "_________________________________________________________________\n",
            "dropout_157 (Dropout)        (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_203 (Dense)            (None, 6)                 774       \n",
            "=================================================================\n",
            "Total params: 466,694\n",
            "Trainable params: 466,694\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/200\n",
            "18/18 [==============================] - 2s 89ms/step - loss: 0.8587 - nd: 0.7345 - val_loss: 0.4347 - val_nd: 0.3993\n",
            "Epoch 2/200\n",
            "18/18 [==============================] - 0s 27ms/step - loss: 0.5673 - nd: 0.4533 - val_loss: 0.3251 - val_nd: 0.3905\n",
            "Epoch 3/200\n",
            "18/18 [==============================] - 0s 27ms/step - loss: 0.4742 - nd: 0.3480 - val_loss: 0.2372 - val_nd: 0.2762\n",
            "Epoch 4/200\n",
            "18/18 [==============================] - 0s 27ms/step - loss: 0.4304 - nd: 0.2770 - val_loss: 0.2112 - val_nd: 0.2482\n",
            "Epoch 5/200\n",
            "18/18 [==============================] - 0s 27ms/step - loss: 0.4222 - nd: 0.2604 - val_loss: 0.1911 - val_nd: 0.2059\n",
            "Epoch 6/200\n",
            "18/18 [==============================] - 0s 24ms/step - loss: 0.4167 - nd: 0.2551 - val_loss: 0.2050 - val_nd: 0.2356\n",
            "Epoch 7/200\n",
            "18/18 [==============================] - 0s 28ms/step - loss: 0.3971 - nd: 0.2352 - val_loss: 0.1735 - val_nd: 0.1891\n",
            "Epoch 8/200\n",
            "18/18 [==============================] - 0s 24ms/step - loss: 0.4093 - nd: 0.2710 - val_loss: 0.2561 - val_nd: 0.2955\n",
            "Epoch 9/200\n",
            "18/18 [==============================] - 0s 25ms/step - loss: 0.4032 - nd: 0.2634 - val_loss: 0.1946 - val_nd: 0.2314\n",
            "Epoch 10/200\n",
            "18/18 [==============================] - 0s 24ms/step - loss: 0.4134 - nd: 0.2557 - val_loss: 0.2589 - val_nd: 0.2156\n",
            "Epoch 11/200\n",
            "18/18 [==============================] - 0s 26ms/step - loss: 0.3832 - nd: 0.2240 - val_loss: 0.1662 - val_nd: 0.1763\n",
            "Epoch 12/200\n",
            "18/18 [==============================] - 0s 24ms/step - loss: 0.3750 - nd: 0.2041 - val_loss: 0.1833 - val_nd: 0.2027\n",
            "Epoch 13/200\n",
            "18/18 [==============================] - 0s 27ms/step - loss: 0.3607 - nd: 0.1950 - val_loss: 0.1444 - val_nd: 0.1413\n",
            "Epoch 14/200\n",
            "18/18 [==============================] - 0s 27ms/step - loss: 0.3728 - nd: 0.1941 - val_loss: 0.1389 - val_nd: 0.1430\n",
            "Epoch 15/200\n",
            "18/18 [==============================] - 0s 24ms/step - loss: 0.3689 - nd: 0.1914 - val_loss: 0.1671 - val_nd: 0.1624\n",
            "Epoch 16/200\n",
            "18/18 [==============================] - 0s 23ms/step - loss: 0.3685 - nd: 0.1893 - val_loss: 0.1407 - val_nd: 0.1402\n",
            "Epoch 17/200\n",
            "18/18 [==============================] - 0s 24ms/step - loss: 0.3519 - nd: 0.1830 - val_loss: 0.1481 - val_nd: 0.1530\n",
            "Epoch 18/200\n",
            "18/18 [==============================] - 0s 24ms/step - loss: 0.3798 - nd: 0.2085 - val_loss: 0.1483 - val_nd: 0.1591\n",
            "Epoch 19/200\n",
            "18/18 [==============================] - 0s 25ms/step - loss: 0.3619 - nd: 0.1951 - val_loss: 0.1735 - val_nd: 0.1699\n",
            "Epoch 20/200\n",
            "18/18 [==============================] - 0s 24ms/step - loss: 0.3626 - nd: 0.2271 - val_loss: 0.1814 - val_nd: 0.2061\n",
            "Epoch 21/200\n",
            "18/18 [==============================] - 0s 25ms/step - loss: 0.3727 - nd: 0.2107 - val_loss: 0.2734 - val_nd: 0.1931\n",
            "Epoch 22/200\n",
            "18/18 [==============================] - 0s 24ms/step - loss: 0.3724 - nd: 0.1845 - val_loss: 0.2184 - val_nd: 0.1733\n",
            "Epoch 23/200\n",
            "18/18 [==============================] - 0s 24ms/step - loss: 0.3599 - nd: 0.1940 - val_loss: 0.1680 - val_nd: 0.1428\n",
            "Epoch 24/200\n",
            "18/18 [==============================] - 0s 24ms/step - loss: 0.3743 - nd: 0.1861 - val_loss: 0.1519 - val_nd: 0.1341\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 66263\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                   _step 24\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                    loss 0.374324768781662\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                   epoch 23\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                _runtime 10636.741288661957\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              _timestamp 1650253012.968871\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                  val_nd 0.13405096530914307\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                      nd 0.18611644208431244\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                val_loss 0.15194353461265564\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           best_val_loss 0.13894250988960266\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              best_epoch 13\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              best-nd_95 0.0019058596669980557\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       best-nrmse_maxmin 0.02507813610119151\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    best-nrmse_maxmin_95 0.00013800302066374777\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 best-nd 0.18528869643779017\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   best-nrmse_maxmin_std 0.0023468727949013976\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     best-nrmse_mean_std 0.022490527997106192\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      best-nrmse_mean_95 0.0013225091733417302\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         best-nrmse_mean 0.24032854414795163\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             best-nd_std 0.03241095870122895\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 7 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced efficient-wind-650: https://app.wandb.ai/matintavakoli/Interpreting_TS/runs/2mczf64w\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.9.7\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Wandb version 0.12.14 is available!  To upgrade, please run:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in wandb/run-20220418_033657-3j54j4ly\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mvibrant-wood-651\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://app.wandb.ai/matintavakoli/Interpreting_TS\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://app.wandb.ai/matintavakoli/Interpreting_TS/runs/3j54j4ly\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb off` to turn off syncing.\n",
            "\n",
            "/usr/local/envs/eval-metric/lib/python3.7/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  return f(**kwargs)\n",
            "/usr/local/envs/eval-metric/lib/python3.7/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  return f(**kwargs)\n",
            "  4%|█▍                                     | 101/2660 [00:00<00:10, 240.73it/s]\n",
            "(9090, 30, 10) (9090, 6) (1111, 30, 10) (1111, 6)\n",
            "(9090, 30, 10) (9090, 6) (1111, 30, 10) (1111, 6)\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 66396\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              _timestamp 1650253627.7660131\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                   _step 2\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                    loss 0.002067637637513244\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                      nd 0.060834989102446085\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                _runtime 11251.538430690765\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                  val_nd 0.09368279452671743\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                val_loss 0.02077412685322595\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              best-nd_95 0.0004974152351957014\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       best-nrmse_maxmin 0.020021893020316255\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    best-nrmse_maxmin_95 0.0002941596046250616\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 best-nd 0.1093310068786386\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   best-nrmse_maxmin_std 0.005002464222399872\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     best-nrmse_mean_std 0.0479395653197869\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      best-nrmse_mean_95 0.002818987393697065\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         best-nrmse_mean 0.19187360580717241\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             best-nd_std 0.008459019792723494\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 8 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced vibrant-wood-651: https://app.wandb.ai/matintavakoli/Interpreting_TS/runs/3j54j4ly\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.9.7\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Wandb version 0.12.14 is available!  To upgrade, please run:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in wandb/run-20220418_034711-2wv49x99\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mdecent-dust-652\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://app.wandb.ai/matintavakoli/Interpreting_TS\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://app.wandb.ai/matintavakoli/Interpreting_TS/runs/2wv49x99\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb off` to turn off syncing.\n",
            "\n",
            "/usr/local/envs/eval-metric/lib/python3.7/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  return f(**kwargs)\n",
            "/usr/local/envs/eval-metric/lib/python3.7/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  return f(**kwargs)\n",
            "  4%|█▍                                     | 101/2660 [00:00<00:11, 230.50it/s]\n",
            "(9090, 30, 10) (9090, 6) (1111, 30, 10) (1111, 6)\n",
            "(9090, 30, 10) (9090, 6) (1111, 30, 10) (1111, 6)\n",
            "Model: \"sequential_92\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "reshape_46 (Reshape)         (None, 300)               0         \n",
            "_________________________________________________________________\n",
            "dense_204 (Dense)            (None, 128)               38528     \n",
            "_________________________________________________________________\n",
            "dropout_158 (Dropout)        (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_205 (Dense)            (None, 128)               16512     \n",
            "_________________________________________________________________\n",
            "dropout_159 (Dropout)        (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_206 (Dense)            (None, 128)               16512     \n",
            "_________________________________________________________________\n",
            "dropout_160 (Dropout)        (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_207 (Dense)            (None, 128)               16512     \n",
            "_________________________________________________________________\n",
            "dropout_161 (Dropout)        (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_208 (Dense)            (None, 6)                 774       \n",
            "=================================================================\n",
            "Total params: 88,838\n",
            "Trainable params: 88,838\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/200\n",
            "18/18 [==============================] - 0s 11ms/step - loss: 1.0704 - nd: 1.0046 - val_loss: 0.8244 - val_nd: 0.9285\n",
            "Epoch 2/200\n",
            "18/18 [==============================] - 0s 8ms/step - loss: 1.0372 - nd: 0.9421 - val_loss: 0.7996 - val_nd: 0.9033\n",
            "Epoch 3/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.9793 - nd: 0.8810 - val_loss: 0.6783 - val_nd: 0.7428\n",
            "Epoch 4/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.8025 - nd: 0.6837 - val_loss: 0.3542 - val_nd: 0.3797\n",
            "Epoch 5/200\n",
            "18/18 [==============================] - 0s 8ms/step - loss: 0.6353 - nd: 0.5191 - val_loss: 0.2630 - val_nd: 0.2907\n",
            "Epoch 6/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.5714 - nd: 0.4509 - val_loss: 0.2335 - val_nd: 0.2588\n",
            "Epoch 7/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.5475 - nd: 0.4292 - val_loss: 0.2683 - val_nd: 0.3040\n",
            "Epoch 8/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.5214 - nd: 0.3929 - val_loss: 0.1903 - val_nd: 0.2073\n",
            "Epoch 9/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.5079 - nd: 0.3691 - val_loss: 0.2232 - val_nd: 0.2393\n",
            "Epoch 10/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4951 - nd: 0.3704 - val_loss: 0.1966 - val_nd: 0.2096\n",
            "Epoch 11/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.4772 - nd: 0.3443 - val_loss: 0.1626 - val_nd: 0.1795\n",
            "Epoch 12/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4798 - nd: 0.3369 - val_loss: 0.1954 - val_nd: 0.2009\n",
            "Epoch 13/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.4646 - nd: 0.3229 - val_loss: 0.1931 - val_nd: 0.1998\n",
            "Epoch 14/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4688 - nd: 0.3268 - val_loss: 0.1858 - val_nd: 0.1771\n",
            "Epoch 15/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4634 - nd: 0.3156 - val_loss: 0.1892 - val_nd: 0.1786\n",
            "Epoch 16/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4583 - nd: 0.3049 - val_loss: 0.1848 - val_nd: 0.1637\n",
            "Epoch 17/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4547 - nd: 0.3022 - val_loss: 0.1918 - val_nd: 0.1797\n",
            "Epoch 18/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.4584 - nd: 0.2965 - val_loss: 0.1772 - val_nd: 0.1750\n",
            "Epoch 19/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4408 - nd: 0.2933 - val_loss: 0.1731 - val_nd: 0.1555\n",
            "Epoch 20/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4556 - nd: 0.2900 - val_loss: 0.1813 - val_nd: 0.1610\n",
            "Epoch 21/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4518 - nd: 0.2996 - val_loss: 0.1802 - val_nd: 0.1605\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 66560\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                   _step 21\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                    loss 0.45176491141319275\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                   epoch 20\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                _runtime 11260.136677026749\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              _timestamp 1650253636.3642595\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                  val_nd 0.1604861617088318\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                      nd 0.29957258701324463\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                val_loss 0.18022580444812775\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           best_val_loss 0.16257424652576447\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              best_epoch 10\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              best-nd_95 0.0017201771519184067\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       best-nrmse_maxmin 0.02860351260820542\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    best-nrmse_maxmin_95 0.00027229841455775507\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 best-nd 0.2164836905743263\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   best-nrmse_maxmin_std 0.0046306938655890665\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     best-nrmse_mean_std 0.04437681933861885\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      best-nrmse_mean_95 0.0026094874547455483\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         best-nrmse_mean 0.27411289718301596\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             best-nd_std 0.029253250695756496\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 7 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced decent-dust-652: https://app.wandb.ai/matintavakoli/Interpreting_TS/runs/2wv49x99\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.9.7\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Wandb version 0.12.14 is available!  To upgrade, please run:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in wandb/run-20220418_034720-2rhx1t05\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mrevived-field-653\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://app.wandb.ai/matintavakoli/Interpreting_TS\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://app.wandb.ai/matintavakoli/Interpreting_TS/runs/2rhx1t05\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb off` to turn off syncing.\n",
            "\n",
            "/usr/local/envs/eval-metric/lib/python3.7/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  return f(**kwargs)\n",
            "/usr/local/envs/eval-metric/lib/python3.7/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  return f(**kwargs)\n",
            "  4%|█▍                                     | 101/2660 [00:00<00:10, 239.31it/s]\n",
            "(9090, 30, 10) (9090, 6) (1111, 30, 10) (1111, 6)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Error uploading \"config.yaml\": CommError, /tmp/tmpcuglc6z4wandb/9i4xu1rk-config.yaml is an empty file\n",
            "(9090, 30, 10) (9090, 6) (1111, 30, 10) (1111, 6)\n",
            "Model: \"sequential_93\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm_112 (LSTM)              (None, 30, 128)           71168     \n",
            "_________________________________________________________________\n",
            "lstm_113 (LSTM)              (None, 30, 128)           131584    \n",
            "_________________________________________________________________\n",
            "lstm_114 (LSTM)              (None, 30, 128)           131584    \n",
            "_________________________________________________________________\n",
            "lstm_115 (LSTM)              (None, 128)               131584    \n",
            "_________________________________________________________________\n",
            "dropout_162 (Dropout)        (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_209 (Dense)            (None, 6)                 774       \n",
            "=================================================================\n",
            "Total params: 466,694\n",
            "Trainable params: 466,694\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/200\n",
            "18/18 [==============================] - 2s 90ms/step - loss: 0.8775 - nd: 0.7637 - val_loss: 0.4327 - val_nd: 0.5373\n",
            "Epoch 2/200\n",
            "18/18 [==============================] - 0s 26ms/step - loss: 0.6366 - nd: 0.5313 - val_loss: 0.4582 - val_nd: 0.4981\n",
            "Epoch 3/200\n",
            "18/18 [==============================] - 1s 29ms/step - loss: 0.5346 - nd: 0.4229 - val_loss: 0.3648 - val_nd: 0.3868\n",
            "Epoch 4/200\n",
            "18/18 [==============================] - 1s 29ms/step - loss: 0.4954 - nd: 0.3769 - val_loss: 0.3457 - val_nd: 0.3627\n",
            "Epoch 5/200\n",
            "18/18 [==============================] - 1s 29ms/step - loss: 0.4518 - nd: 0.3365 - val_loss: 0.2656 - val_nd: 0.2730\n",
            "Epoch 6/200\n",
            "18/18 [==============================] - 0s 26ms/step - loss: 0.4590 - nd: 0.3227 - val_loss: 0.3109 - val_nd: 0.3259\n",
            "Epoch 7/200\n",
            "18/18 [==============================] - 1s 29ms/step - loss: 0.4484 - nd: 0.2928 - val_loss: 0.2588 - val_nd: 0.2429\n",
            "Epoch 8/200\n",
            "18/18 [==============================] - 1s 29ms/step - loss: 0.4336 - nd: 0.2747 - val_loss: 0.2359 - val_nd: 0.2283\n",
            "Epoch 9/200\n",
            "18/18 [==============================] - 0s 26ms/step - loss: 0.4408 - nd: 0.2746 - val_loss: 0.2629 - val_nd: 0.2947\n",
            "Epoch 10/200\n",
            "18/18 [==============================] - 0s 27ms/step - loss: 0.4466 - nd: 0.2865 - val_loss: 0.3278 - val_nd: 0.3218\n",
            "Epoch 11/200\n",
            "18/18 [==============================] - 0s 26ms/step - loss: 0.4633 - nd: 0.3204 - val_loss: 0.2711 - val_nd: 0.2655\n",
            "Epoch 12/200\n",
            "18/18 [==============================] - 1s 28ms/step - loss: 0.4326 - nd: 0.2816 - val_loss: 0.2190 - val_nd: 0.2120\n",
            "Epoch 13/200\n",
            "18/18 [==============================] - 0s 26ms/step - loss: 0.4317 - nd: 0.2600 - val_loss: 0.2263 - val_nd: 0.2133\n",
            "Epoch 14/200\n",
            "18/18 [==============================] - 1s 29ms/step - loss: 0.4261 - nd: 0.2459 - val_loss: 0.2028 - val_nd: 0.2075\n",
            "Epoch 15/200\n",
            "18/18 [==============================] - 1s 29ms/step - loss: 0.4251 - nd: 0.2446 - val_loss: 0.1871 - val_nd: 0.1934\n",
            "Epoch 16/200\n",
            "18/18 [==============================] - 0s 26ms/step - loss: 0.4371 - nd: 0.2508 - val_loss: 0.2460 - val_nd: 0.2320\n",
            "Epoch 17/200\n",
            "18/18 [==============================] - 1s 28ms/step - loss: 0.4148 - nd: 0.2391 - val_loss: 0.1842 - val_nd: 0.2014\n",
            "Epoch 18/200\n",
            "18/18 [==============================] - 0s 26ms/step - loss: 0.4198 - nd: 0.2466 - val_loss: 0.2216 - val_nd: 0.2360\n",
            "Epoch 19/200\n",
            "18/18 [==============================] - 0s 26ms/step - loss: 0.4211 - nd: 0.2476 - val_loss: 0.2398 - val_nd: 0.2464\n",
            "Epoch 20/200\n",
            "18/18 [==============================] - 1s 31ms/step - loss: 0.3984 - nd: 0.2308 - val_loss: 0.1783 - val_nd: 0.1870\n",
            "Epoch 21/200\n",
            "18/18 [==============================] - 0s 27ms/step - loss: 0.4028 - nd: 0.2285 - val_loss: 0.1794 - val_nd: 0.1867\n",
            "Epoch 22/200\n",
            "18/18 [==============================] - 0s 26ms/step - loss: 0.3787 - nd: 0.2207 - val_loss: 0.1822 - val_nd: 0.2027\n",
            "Epoch 23/200\n",
            "18/18 [==============================] - 0s 25ms/step - loss: 0.4008 - nd: 0.2218 - val_loss: 0.1814 - val_nd: 0.1798\n",
            "Epoch 24/200\n",
            "18/18 [==============================] - 0s 26ms/step - loss: 0.3899 - nd: 0.2280 - val_loss: 0.2407 - val_nd: 0.2101\n",
            "Epoch 25/200\n",
            "18/18 [==============================] - 0s 26ms/step - loss: 0.4180 - nd: 0.2439 - val_loss: 0.2432 - val_nd: 0.2290\n",
            "Epoch 26/200\n",
            "18/18 [==============================] - 0s 25ms/step - loss: 0.4148 - nd: 0.2642 - val_loss: 0.1996 - val_nd: 0.2009\n",
            "Epoch 27/200\n",
            "18/18 [==============================] - 0s 27ms/step - loss: 0.3971 - nd: 0.2294 - val_loss: 0.1817 - val_nd: 0.1787\n",
            "Epoch 28/200\n",
            "18/18 [==============================] - 0s 26ms/step - loss: 0.3958 - nd: 0.2216 - val_loss: 0.1950 - val_nd: 0.1996\n",
            "Epoch 29/200\n",
            "18/18 [==============================] - 1s 29ms/step - loss: 0.3873 - nd: 0.2281 - val_loss: 0.1757 - val_nd: 0.1767\n",
            "Epoch 30/200\n",
            "18/18 [==============================] - 1s 29ms/step - loss: 0.4054 - nd: 0.2304 - val_loss: 0.1675 - val_nd: 0.1851\n",
            "Epoch 31/200\n",
            "18/18 [==============================] - 0s 25ms/step - loss: 0.3915 - nd: 0.2181 - val_loss: 0.1703 - val_nd: 0.1785\n",
            "Epoch 32/200\n",
            "18/18 [==============================] - 1s 30ms/step - loss: 0.3859 - nd: 0.2081 - val_loss: 0.1504 - val_nd: 0.1435\n",
            "Epoch 33/200\n",
            "18/18 [==============================] - 0s 26ms/step - loss: 0.3787 - nd: 0.2092 - val_loss: 0.1641 - val_nd: 0.1576\n",
            "Epoch 34/200\n",
            "18/18 [==============================] - 0s 25ms/step - loss: 0.3945 - nd: 0.2230 - val_loss: 0.1894 - val_nd: 0.1723\n",
            "Epoch 35/200\n",
            "18/18 [==============================] - 0s 26ms/step - loss: 0.3891 - nd: 0.2291 - val_loss: 0.1865 - val_nd: 0.1683\n",
            "Epoch 36/200\n",
            "18/18 [==============================] - 0s 26ms/step - loss: 0.3702 - nd: 0.2189 - val_loss: 0.1557 - val_nd: 0.1571\n",
            "Epoch 37/200\n",
            "18/18 [==============================] - 0s 26ms/step - loss: 0.3837 - nd: 0.2071 - val_loss: 0.1603 - val_nd: 0.1681\n",
            "Epoch 38/200\n",
            "18/18 [==============================] - 0s 26ms/step - loss: 0.3716 - nd: 0.2013 - val_loss: 0.1772 - val_nd: 0.1684\n",
            "Epoch 39/200\n",
            "18/18 [==============================] - 0s 25ms/step - loss: 0.3699 - nd: 0.1959 - val_loss: 0.1816 - val_nd: 0.1591\n",
            "Epoch 40/200\n",
            "18/18 [==============================] - 0s 26ms/step - loss: 0.3812 - nd: 0.2021 - val_loss: 0.1918 - val_nd: 0.1797\n",
            "Epoch 41/200\n",
            "18/18 [==============================] - 0s 26ms/step - loss: 0.3732 - nd: 0.2009 - val_loss: 0.1523 - val_nd: 0.1541\n",
            "Epoch 42/200\n",
            "18/18 [==============================] - 0s 27ms/step - loss: 0.3756 - nd: 0.2003 - val_loss: 0.1581 - val_nd: 0.1521\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 66680\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                   _step 42\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                    loss 0.3756181299686432\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                   epoch 41\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                _runtime 11296.19108247757\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              _timestamp 1650253672.418665\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                  val_nd 0.15214812755584717\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                      nd 0.2002631574869156\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                val_loss 0.15808694064617157\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           best_val_loss 0.15038856863975525\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              best_epoch 31\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              best-nd_95 0.0009233238399333114\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       best-nrmse_maxmin 0.02658677199465547\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    best-nrmse_maxmin_95 0.00030406993623688426\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 best-nd 0.18141783551685636\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   best-nrmse_maxmin_std 0.005170998849659285\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     best-nrmse_mean_std 0.049554664681411634\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      best-nrmse_mean_95 0.0029139599849088817\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         best-nrmse_mean 0.2547860886186633\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             best-nd_std 0.015702001234474527\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 7 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced revived-field-653: https://app.wandb.ai/matintavakoli/Interpreting_TS/runs/2rhx1t05\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.9.7\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Wandb version 0.12.14 is available!  To upgrade, please run:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in wandb/run-20220418_034757-244two7t\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mgallant-field-654\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://app.wandb.ai/matintavakoli/Interpreting_TS\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://app.wandb.ai/matintavakoli/Interpreting_TS/runs/244two7t\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb off` to turn off syncing.\n",
            "\n",
            "/usr/local/envs/eval-metric/lib/python3.7/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  return f(**kwargs)\n",
            "/usr/local/envs/eval-metric/lib/python3.7/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  return f(**kwargs)\n",
            "  4%|█▍                                     | 101/2660 [00:00<00:12, 213.09it/s]\n",
            "(9090, 30, 10) (9090, 6) (1111, 30, 10) (1111, 6)\n",
            "(9090, 30, 10) (9090, 6) (1111, 30, 10) (1111, 6)\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 66869\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              _timestamp 1650254287.3507557\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                   _step 2\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                    loss 0.0021395566524667255\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                      nd 0.06238906062458591\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                _runtime 11911.123173236847\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                  val_nd 0.09331905643879056\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                val_loss 0.0213298470074412\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              best-nd_95 0.00048626854080667016\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       best-nrmse_maxmin 0.020410729407334804\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    best-nrmse_maxmin_95 0.0003385811781576147\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 best-nd 0.10898910955680383\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   best-nrmse_maxmin_std 0.005757895385637061\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     best-nrmse_mean_std 0.05517900572046967\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      best-nrmse_mean_95 0.00324468777480842\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         best-nrmse_mean 0.19559989879907771\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             best-nd_std 0.008269459638975574\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 8 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced gallant-field-654: https://app.wandb.ai/matintavakoli/Interpreting_TS/runs/244two7t\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.9.7\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Wandb version 0.12.14 is available!  To upgrade, please run:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in wandb/run-20220418_035811-332p0jnl\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mtrue-valley-655\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://app.wandb.ai/matintavakoli/Interpreting_TS\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://app.wandb.ai/matintavakoli/Interpreting_TS/runs/332p0jnl\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb off` to turn off syncing.\n",
            "\n",
            "/usr/local/envs/eval-metric/lib/python3.7/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  return f(**kwargs)\n",
            "/usr/local/envs/eval-metric/lib/python3.7/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  return f(**kwargs)\n",
            "  4%|█▍                                     | 101/2660 [00:00<00:10, 236.38it/s]\n",
            "(9090, 30, 10) (9090, 6) (1111, 30, 10) (1111, 6)\n",
            "(9090, 30, 10) (9090, 6) (1111, 30, 10) (1111, 6)\n",
            "Model: \"sequential_94\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "reshape_47 (Reshape)         (None, 300)               0         \n",
            "_________________________________________________________________\n",
            "dense_210 (Dense)            (None, 128)               38528     \n",
            "_________________________________________________________________\n",
            "dropout_163 (Dropout)        (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_211 (Dense)            (None, 128)               16512     \n",
            "_________________________________________________________________\n",
            "dropout_164 (Dropout)        (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_212 (Dense)            (None, 128)               16512     \n",
            "_________________________________________________________________\n",
            "dropout_165 (Dropout)        (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_213 (Dense)            (None, 128)               16512     \n",
            "_________________________________________________________________\n",
            "dropout_166 (Dropout)        (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_214 (Dense)            (None, 6)                 774       \n",
            "=================================================================\n",
            "Total params: 88,838\n",
            "Trainable params: 88,838\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/200\n",
            "18/18 [==============================] - 0s 11ms/step - loss: 1.2448 - nd: 1.2908 - val_loss: 0.8315 - val_nd: 0.9275\n",
            "Epoch 2/200\n",
            "18/18 [==============================] - 0s 8ms/step - loss: 1.1132 - nd: 1.0846 - val_loss: 0.8259 - val_nd: 0.9109\n",
            "Epoch 3/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 1.0554 - nd: 0.9785 - val_loss: 0.8271 - val_nd: 0.9238\n",
            "Epoch 4/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 1.0334 - nd: 0.9396 - val_loss: 0.8274 - val_nd: 0.9271\n",
            "Epoch 5/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 1.0149 - nd: 0.9166 - val_loss: 0.8273 - val_nd: 0.9284\n",
            "Epoch 6/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 1.0134 - nd: 0.9095 - val_loss: 0.8262 - val_nd: 0.9262\n",
            "Epoch 7/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 1.0021 - nd: 0.9032 - val_loss: 0.8234 - val_nd: 0.9203\n",
            "Epoch 8/200\n",
            "18/18 [==============================] - 0s 8ms/step - loss: 0.9970 - nd: 0.8896 - val_loss: 0.8149 - val_nd: 0.9000\n",
            "Epoch 9/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.9977 - nd: 0.8588 - val_loss: 0.7935 - val_nd: 0.8602\n",
            "Epoch 10/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.9718 - nd: 0.8090 - val_loss: 0.7169 - val_nd: 0.7152\n",
            "Epoch 11/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.8870 - nd: 0.7085 - val_loss: 0.5053 - val_nd: 0.4213\n",
            "Epoch 12/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.7823 - nd: 0.6279 - val_loss: 0.3546 - val_nd: 0.3504\n",
            "Epoch 13/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.6964 - nd: 0.5733 - val_loss: 0.3112 - val_nd: 0.3600\n",
            "Epoch 14/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.6524 - nd: 0.5302 - val_loss: 0.2896 - val_nd: 0.3135\n",
            "Epoch 15/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.6496 - nd: 0.5229 - val_loss: 0.2877 - val_nd: 0.3284\n",
            "Epoch 16/200\n",
            "18/18 [==============================] - 0s 9ms/step - loss: 0.6362 - nd: 0.5086 - val_loss: 0.2558 - val_nd: 0.3069\n",
            "Epoch 17/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.6124 - nd: 0.4835 - val_loss: 0.2385 - val_nd: 0.2621\n",
            "Epoch 18/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.5976 - nd: 0.4673 - val_loss: 0.2392 - val_nd: 0.2755\n",
            "Epoch 19/200\n",
            "18/18 [==============================] - 0s 8ms/step - loss: 0.5852 - nd: 0.4510 - val_loss: 0.2120 - val_nd: 0.2469\n",
            "Epoch 20/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5610 - nd: 0.4458 - val_loss: 0.2274 - val_nd: 0.2656\n",
            "Epoch 21/200\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.5636 - nd: 0.4399 - val_loss: 0.2069 - val_nd: 0.2338\n",
            "Epoch 22/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.5739 - nd: 0.4502 - val_loss: 0.2299 - val_nd: 0.2848\n",
            "Epoch 23/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5609 - nd: 0.4462 - val_loss: 0.2368 - val_nd: 0.2765\n",
            "Epoch 24/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5544 - nd: 0.4344 - val_loss: 0.2321 - val_nd: 0.2421\n",
            "Epoch 25/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.5420 - nd: 0.4269 - val_loss: 0.2116 - val_nd: 0.2519\n",
            "Epoch 26/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.5553 - nd: 0.4319 - val_loss: 0.2202 - val_nd: 0.2591\n",
            "Epoch 27/200\n",
            "18/18 [==============================] - 0s 8ms/step - loss: 0.5433 - nd: 0.4183 - val_loss: 0.1974 - val_nd: 0.2206\n",
            "Epoch 28/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.5411 - nd: 0.4181 - val_loss: 0.2290 - val_nd: 0.2608\n",
            "Epoch 29/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5431 - nd: 0.4169 - val_loss: 0.2241 - val_nd: 0.2644\n",
            "Epoch 30/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.5426 - nd: 0.4165 - val_loss: 0.2180 - val_nd: 0.2438\n",
            "Epoch 31/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5317 - nd: 0.4140 - val_loss: 0.2432 - val_nd: 0.2644\n",
            "Epoch 32/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.5318 - nd: 0.4060 - val_loss: 0.2143 - val_nd: 0.2298\n",
            "Epoch 33/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5198 - nd: 0.4009 - val_loss: 0.2147 - val_nd: 0.2376\n",
            "Epoch 34/200\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.5349 - nd: 0.4089 - val_loss: 0.2131 - val_nd: 0.2476\n",
            "Epoch 35/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5248 - nd: 0.4061 - val_loss: 0.2398 - val_nd: 0.2561\n",
            "Epoch 36/200\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5191 - nd: 0.4043 - val_loss: 0.2312 - val_nd: 0.2469\n",
            "Epoch 37/200\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.5225 - nd: 0.3995 - val_loss: 0.2506 - val_nd: 0.2816\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 67033\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                   _step 37\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                    loss 0.5224775075912476\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                   epoch 36\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                _runtime 11921.790681362152\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              _timestamp 1650254298.0182638\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                  val_nd 0.281598836183548\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                      nd 0.39946630597114563\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                val_loss 0.2505984604358673\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           best_val_loss 0.19740024209022522\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              best_epoch 26\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              best-nd_95 0.0007390874554784083\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       best-nrmse_maxmin 0.034547843429708604\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    best-nrmse_maxmin_95 0.00039355817662109743\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 best-nd 0.2618341503332505\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   best-nrmse_maxmin_std 0.0066928316023859525\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     best-nrmse_mean_std 0.06413867716239072\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      best-nrmse_mean_95 0.0037715427990031893\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         best-nrmse_mean 0.33107854911588747\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             best-nd_std 0.012568886057512394\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 7 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced true-valley-655: https://app.wandb.ai/matintavakoli/Interpreting_TS/runs/332p0jnl\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.9.7\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Wandb version 0.12.14 is available!  To upgrade, please run:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in wandb/run-20220418_035822-2glxhg8o\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mlegendary-sound-656\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://app.wandb.ai/matintavakoli/Interpreting_TS\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://app.wandb.ai/matintavakoli/Interpreting_TS/runs/2glxhg8o\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb off` to turn off syncing.\n",
            "\n",
            "/usr/local/envs/eval-metric/lib/python3.7/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  return f(**kwargs)\n",
            "/usr/local/envs/eval-metric/lib/python3.7/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  return f(**kwargs)\n",
            "  4%|█▍                                     | 101/2660 [00:00<00:10, 239.59it/s]\n",
            "(9090, 30, 10) (9090, 6) (1111, 30, 10) (1111, 6)\n",
            "(9090, 30, 10) (9090, 6) (1111, 30, 10) (1111, 6)\n",
            "Model: \"sequential_95\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm_116 (LSTM)              (None, 30, 128)           71168     \n",
            "_________________________________________________________________\n",
            "lstm_117 (LSTM)              (None, 30, 128)           131584    \n",
            "_________________________________________________________________\n",
            "lstm_118 (LSTM)              (None, 30, 128)           131584    \n",
            "_________________________________________________________________\n",
            "lstm_119 (LSTM)              (None, 128)               131584    \n",
            "_________________________________________________________________\n",
            "dropout_167 (Dropout)        (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_215 (Dense)            (None, 6)                 774       \n",
            "=================================================================\n",
            "Total params: 466,694\n",
            "Trainable params: 466,694\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/200\n",
            "18/18 [==============================] - 2s 91ms/step - loss: 0.8938 - nd: 0.7892 - val_loss: 0.5647 - val_nd: 0.6144\n",
            "Epoch 2/200\n",
            "18/18 [==============================] - 0s 26ms/step - loss: 0.6747 - nd: 0.5815 - val_loss: 0.5862 - val_nd: 0.5956\n",
            "Epoch 3/200\n",
            "18/18 [==============================] - 1s 28ms/step - loss: 0.5992 - nd: 0.4745 - val_loss: 0.5545 - val_nd: 0.5952\n",
            "Epoch 4/200\n",
            "18/18 [==============================] - 0s 26ms/step - loss: 0.5647 - nd: 0.4520 - val_loss: 0.6217 - val_nd: 0.6006\n",
            "Epoch 5/200\n",
            "18/18 [==============================] - 0s 25ms/step - loss: 0.5482 - nd: 0.4245 - val_loss: 0.5754 - val_nd: 0.5817\n",
            "Epoch 6/200\n",
            "18/18 [==============================] - 1s 28ms/step - loss: 0.5396 - nd: 0.4216 - val_loss: 0.4923 - val_nd: 0.5147\n",
            "Epoch 7/200\n",
            "18/18 [==============================] - 0s 26ms/step - loss: 0.5141 - nd: 0.3826 - val_loss: 0.5534 - val_nd: 0.5724\n",
            "Epoch 8/200\n",
            "18/18 [==============================] - 0s 26ms/step - loss: 0.5053 - nd: 0.3614 - val_loss: 0.5305 - val_nd: 0.5396\n",
            "Epoch 9/200\n",
            "18/18 [==============================] - 0s 25ms/step - loss: 0.4870 - nd: 0.3494 - val_loss: 0.5011 - val_nd: 0.5096\n",
            "Epoch 10/200\n",
            "18/18 [==============================] - 1s 28ms/step - loss: 0.4746 - nd: 0.3384 - val_loss: 0.4682 - val_nd: 0.4898\n",
            "Epoch 11/200\n",
            "18/18 [==============================] - 1s 29ms/step - loss: 0.4916 - nd: 0.3395 - val_loss: 0.4670 - val_nd: 0.4718\n",
            "Epoch 12/200\n",
            "18/18 [==============================] - 1s 29ms/step - loss: 0.4723 - nd: 0.3303 - val_loss: 0.4536 - val_nd: 0.4803\n",
            "Epoch 13/200\n",
            "18/18 [==============================] - 1s 29ms/step - loss: 0.4719 - nd: 0.3316 - val_loss: 0.4484 - val_nd: 0.4834\n",
            "Epoch 14/200\n",
            "18/18 [==============================] - 1s 29ms/step - loss: 0.4927 - nd: 0.3463 - val_loss: 0.4408 - val_nd: 0.4779\n",
            "Epoch 15/200\n",
            "18/18 [==============================] - 0s 26ms/step - loss: 0.4769 - nd: 0.3303 - val_loss: 0.4587 - val_nd: 0.5033\n",
            "Epoch 16/200\n",
            "18/18 [==============================] - 1s 29ms/step - loss: 0.4735 - nd: 0.3396 - val_loss: 0.4117 - val_nd: 0.4578\n",
            "Epoch 17/200\n",
            "18/18 [==============================] - 1s 29ms/step - loss: 0.4720 - nd: 0.3092 - val_loss: 0.3735 - val_nd: 0.3979\n",
            "Epoch 18/200\n",
            "18/18 [==============================] - 0s 26ms/step - loss: 0.4658 - nd: 0.3047 - val_loss: 0.3946 - val_nd: 0.4291\n",
            "Epoch 19/200\n",
            "18/18 [==============================] - 1s 29ms/step - loss: 0.4500 - nd: 0.3002 - val_loss: 0.3674 - val_nd: 0.4105\n",
            "Epoch 20/200\n",
            "18/18 [==============================] - 1s 30ms/step - loss: 0.4420 - nd: 0.2996 - val_loss: 0.3462 - val_nd: 0.3998\n",
            "Epoch 21/200\n",
            "18/18 [==============================] - 0s 27ms/step - loss: 0.4627 - nd: 0.3126 - val_loss: 0.3722 - val_nd: 0.3758\n",
            "Epoch 22/200\n",
            "18/18 [==============================] - 0s 25ms/step - loss: 0.4415 - nd: 0.2969 - val_loss: 0.4143 - val_nd: 0.4238\n",
            "Epoch 23/200\n",
            "18/18 [==============================] - 0s 26ms/step - loss: 0.4475 - nd: 0.2870 - val_loss: 0.4026 - val_nd: 0.4203\n",
            "Epoch 24/200\n",
            "18/18 [==============================] - 0s 25ms/step - loss: 0.4469 - nd: 0.2870 - val_loss: 0.4119 - val_nd: 0.4058\n",
            "Epoch 25/200\n",
            "18/18 [==============================] - 0s 26ms/step - loss: 0.4313 - nd: 0.2904 - val_loss: 0.3641 - val_nd: 0.3679\n",
            "Epoch 26/200\n",
            "18/18 [==============================] - 0s 26ms/step - loss: 0.4407 - nd: 0.2901 - val_loss: 0.3611 - val_nd: 0.3578\n",
            "Epoch 27/200\n",
            "18/18 [==============================] - 1s 29ms/step - loss: 0.4290 - nd: 0.2785 - val_loss: 0.3292 - val_nd: 0.3286\n",
            "Epoch 28/200\n",
            "18/18 [==============================] - 0s 26ms/step - loss: 0.4271 - nd: 0.2797 - val_loss: 0.3377 - val_nd: 0.3453\n",
            "Epoch 29/200\n",
            "18/18 [==============================] - 0s 26ms/step - loss: 0.4217 - nd: 0.2741 - val_loss: 0.3600 - val_nd: 0.3450\n",
            "Epoch 30/200\n",
            "18/18 [==============================] - 0s 26ms/step - loss: 0.4202 - nd: 0.2790 - val_loss: 0.3648 - val_nd: 0.3438\n",
            "Epoch 31/200\n",
            "18/18 [==============================] - 1s 28ms/step - loss: 0.4380 - nd: 0.2773 - val_loss: 0.3202 - val_nd: 0.3244\n",
            "Epoch 32/200\n",
            "18/18 [==============================] - 0s 26ms/step - loss: 0.4298 - nd: 0.2751 - val_loss: 0.3268 - val_nd: 0.3446\n",
            "Epoch 33/200\n",
            "18/18 [==============================] - 0s 26ms/step - loss: 0.4267 - nd: 0.2778 - val_loss: 0.3260 - val_nd: 0.3345\n",
            "Epoch 34/200\n",
            "18/18 [==============================] - 0s 26ms/step - loss: 0.4276 - nd: 0.2701 - val_loss: 0.3685 - val_nd: 0.3636\n",
            "Epoch 35/200\n",
            "18/18 [==============================] - 0s 26ms/step - loss: 0.4187 - nd: 0.2760 - val_loss: 0.3470 - val_nd: 0.3429\n",
            "Epoch 36/200\n",
            "18/18 [==============================] - 0s 25ms/step - loss: 0.4220 - nd: 0.2717 - val_loss: 0.3289 - val_nd: 0.3269\n",
            "Epoch 37/200\n",
            "18/18 [==============================] - 0s 25ms/step - loss: 0.4175 - nd: 0.2644 - val_loss: 0.3359 - val_nd: 0.3262\n",
            "Epoch 38/200\n",
            "18/18 [==============================] - 0s 26ms/step - loss: 0.4298 - nd: 0.2643 - val_loss: 0.3519 - val_nd: 0.3401\n",
            "Epoch 39/200\n",
            "18/18 [==============================] - 0s 26ms/step - loss: 0.4486 - nd: 0.2681 - val_loss: 0.3851 - val_nd: 0.3729\n",
            "Epoch 40/200\n",
            "18/18 [==============================] - 0s 25ms/step - loss: 0.4126 - nd: 0.2603 - val_loss: 0.3993 - val_nd: 0.3640\n",
            "Epoch 41/200\n",
            "18/18 [==============================] - 0s 25ms/step - loss: 0.4036 - nd: 0.2618 - val_loss: 0.3862 - val_nd: 0.3546\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 67200\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                   _step 41\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                    loss 0.4036192297935486\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                   epoch 40\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                _runtime 11957.867246389389\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              _timestamp 1650254334.0948288\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                  val_nd 0.35464993119239807\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                      nd 0.26183900237083435\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                val_loss 0.38615185022354126\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           best_val_loss 0.3201538324356079\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              best_epoch 30\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              best-nd_95 0.00471097390860576\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       best-nrmse_maxmin 0.053956804178708705\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    best-nrmse_maxmin_95 0.000910509379490684\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 best-nd 0.3837645543108409\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   best-nrmse_maxmin_std 0.015484079130672037\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     best-nrmse_mean_std 0.14838687292909647\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      best-nrmse_mean_95 0.008725584418359317\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         best-nrmse_mean 0.5170783084843802\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             best-nd_std 0.08011459785750541\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 7 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced legendary-sound-656: https://app.wandb.ai/matintavakoli/Interpreting_TS/runs/2glxhg8o\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.9.7\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Wandb version 0.12.14 is available!  To upgrade, please run:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in wandb/run-20220418_035859-1f3qljhv\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mfast-haze-657\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://app.wandb.ai/matintavakoli/Interpreting_TS\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://app.wandb.ai/matintavakoli/Interpreting_TS/runs/1f3qljhv\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb off` to turn off syncing.\n",
            "\n",
            "/usr/local/envs/eval-metric/lib/python3.7/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  return f(**kwargs)\n",
            "/usr/local/envs/eval-metric/lib/python3.7/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  return f(**kwargs)\n",
            "  4%|█▍                                     | 101/2660 [00:00<00:10, 240.78it/s]\n",
            "(9090, 30, 10) (9090, 6) (1111, 30, 10) (1111, 6)\n",
            "(9090, 30, 10) (9090, 6) (1111, 30, 10) (1111, 6)\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 67387\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              _timestamp 1650254948.998817\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                   _step 2\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                    loss 0.0021875056515033306\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                      nd 0.06356931426806355\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                _runtime 12572.77123451233\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                  val_nd 0.09365716092586235\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                val_loss 0.021446353393103176\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              best-nd_95 0.00048475343741479545\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       best-nrmse_maxmin 0.02054128060533833\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    best-nrmse_maxmin_95 0.00034841214512039214\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 best-nd 0.10956585078465728\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   best-nrmse_maxmin_std 0.005925080341455768\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     best-nrmse_mean_std 0.05678117092418433\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      best-nrmse_mean_95 0.0033388997995058476\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         best-nrmse_mean 0.1968509957397099\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             best-nd_std 0.008243693862873344\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 8 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced fast-haze-657: https://app.wandb.ai/matintavakoli/Interpreting_TS/runs/1f3qljhv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!source activate eval-metric && python train_feature_selection.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4BXYYCeO-R0I",
        "outputId": "9379105b-e3f4-4623-c570-8aa8202af620"
      },
      "execution_count": 144,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  4% 101/2660 [00:00<00:10, 249.36it/s]\n",
            "(9090, 30, 10) (9090, 6) (1111, 30, 10) (1111, 6)\n",
            "Loaded shap features\n",
            "walmart\n",
            "{'f_regression_wrapper': (['Weekly_Sales',\n",
            "                           'Size',\n",
            "                           'Type',\n",
            "                           'Dept',\n",
            "                           'day',\n",
            "                           'Year',\n",
            "                           'weekofmonth',\n",
            "                           'Temperature',\n",
            "                           'month',\n",
            "                           'Store'],\n",
            "                          array([1.91989967e+05, 1.39762249e+03, 7.63976989e+02, 3.82666921e+02,\n",
            "       1.63094936e+02, 1.50902118e+02, 1.44225827e+02, 1.09714972e+02,\n",
            "       6.90097009e+01, 3.45635049e+01])),\n",
            " 'mutual_info_wrapper': (['Weekly_Sales',\n",
            "                          'Dept',\n",
            "                          'Size',\n",
            "                          'Type',\n",
            "                          'Store',\n",
            "                          'weekofmonth',\n",
            "                          'day',\n",
            "                          'Temperature',\n",
            "                          'Year',\n",
            "                          'month'],\n",
            "                         array([254.87444239,  76.76937176,  49.24173439,  34.12716282,\n",
            "        20.27788421,   5.62950907,   4.6482983 ,   3.64896893,\n",
            "         2.44036711,   2.08047719])),\n",
            " 'shap': (['Weekly_Sales',\n",
            "           'month',\n",
            "           'Temperature',\n",
            "           'Dept',\n",
            "           'day',\n",
            "           'weekofmonth',\n",
            "           'Size',\n",
            "           'Store',\n",
            "           'Year',\n",
            "           'Type'],\n",
            "          array([4.04257058e+00, 3.32604796e-01, 3.08039142e-01, 1.78730637e-01,\n",
            "       8.90009303e-02, 2.88346226e-02, 2.75848476e-02, 1.57460047e-02,\n",
            "       1.38059041e-03, 7.40183136e-04])),\n",
            " 'tree_imp': (['Weekly_Sales',\n",
            "               'Temperature',\n",
            "               'month',\n",
            "               'day',\n",
            "               'Dept',\n",
            "               'Store',\n",
            "               'weekofmonth',\n",
            "               'Size',\n",
            "               'Year',\n",
            "               'Type'],\n",
            "              array([5.18939851e+00, 7.37780173e-01, 4.33092040e-02, 1.27094768e-02,\n",
            "       6.88411514e-03, 6.66584595e-03, 2.56340644e-03, 3.18545002e-04,\n",
            "       3.13587826e-04, 5.71389337e-05]))}\n",
            "walmart\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!source activate eval-metric && python eval_metrics_multi.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WBShpCsrhSXM",
        "outputId": "58a63fc9-b929-4335-b9b1-f32df95ad103"
      },
      "execution_count": 145,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "walmart   aopcr\n",
            "RandomTraceback (most recent call last):\n",
            "  File \"/usr/local/envs/eval-metric/lib/python3.7/site-packages/pandas/core/indexes/base.py\", line 3080, in get_loc\n",
            "    return self._engine.get_loc(casted_key)\n",
            "  File \"pandas/_libs/index.pyx\", line 70, in pandas._libs.index.IndexEngine.get_loc\n",
            "  File \"pandas/_libs/index.pyx\", line 101, in pandas._libs.index.IndexEngine.get_loc\n",
            "  File \"pandas/_libs/hashtable_class_helper.pxi\", line 4554, in pandas._libs.hashtable.PyObjectHashTable.get_item\n",
            "  File \"pandas/_libs/hashtable_class_helper.pxi\", line 4562, in pandas._libs.hashtable.PyObjectHashTable.get_item\n",
            "KeyError: 'METHOD_NAME'\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"eval_metrics_multi.py\", line 56, in <module>\n",
            "    exp = get_scores(df, dataset, model, method_name, eval_method_name)\n",
            "  File \"/content/evaluation-local-explanation-time-series/src/reports/wandb_queries.py\", line 62, in get_scores\n",
            "    (df['METHOD_NAME']==method_name) & (df['EVAL_METHOD_NAME']==eval_method_name)\n",
            "  File \"/usr/local/envs/eval-metric/lib/python3.7/site-packages/pandas/core/frame.py\", line 3024, in __getitem__\n",
            "    indexer = self.columns.get_loc(key)\n",
            "  File \"/usr/local/envs/eval-metric/lib/python3.7/site-packages/pandas/core/indexes/base.py\", line 3082, in get_loc\n",
            "    raise KeyError(key) from err\n",
            "KeyError: 'METHOD_NAME'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!source activate eval-metric && python eval_robustness.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZGh-5OaL4Hh3",
        "outputId": "5abe9601-f3e2-4e61-e2c4-e67a524395f1"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: activate: No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!source activate eval-metric && python report_train_results.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xkv0nFCZhisA",
        "outputId": "aa71a78e-9af3-4ffb-d4ec-a4189903b005"
      },
      "execution_count": 146,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "python: can't open file 'report_train.py': [Errno 2] No such file or directory\n"
          ]
        }
      ]
    }
  ]
}